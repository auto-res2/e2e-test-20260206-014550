{
  "research_topic": "Proposing an improved version of the Chain-of-Thought based on human thinking methods",
  "queries": [
    "human-like reasoning",
    "chain-of-thought prompting",
    "cognitive strategies LLM"
  ],
  "research_study_list": [
    {
      "title": "CLeAR: Continual Learning on Algorithmic Reasoning for Human-like Intelligence",
      "full_text": "IEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 1 Towards Neuromorphic Compression based Neural Sensing for Next-Generation Wireless Implantable Brain Machine Interface Vivek Mohan , Member, IEEE, Wee Peng Tay , Senior Member, IEEE, and Arindam Basu , Senior Member, IEEE Abstract—This work introduces a neuromorphic compression based neural sensing architecture with address-event represen- tation inspired readout protocol for massively parallel, next- gen wireless iBMI. The architectural trade-offs and implications of the proposed method are quantitatively analyzed in terms of compression ratio and spike information preservation. For the latter, we use metrics such as root-mean-square error and correlation coefficient between the original and recovered signal to assess the effect of neuromorphic compression on spike shape. Furthermore, we use accuracy, sensitivity, and false detection rate to understand the effect of compression on downstream iBMI tasks, specifically, spike detection. We demonstrate that a data compression ratio of 50 − 100 can be achieved, 5 − 18× more than prior work, by selective transmission of event pulses corresponding to neural spikes. A correlation coefficient of ≈ 0.9 and spike detection accuracy of over 90% for the worst-case analysis involving 10K-channel simulated recording and typical analysis using 100 or 384-channel real neural recordings. We also analyze the collision handling capability and scalability of the proposed pipeline. Index Terms—implantable-brain machine interface (iBMI), neurotechnology, neuromorphic compression, address event rep- resentation (AER). List of Abbreviations- iBMI Implantable Brain Machine Interface ADC Analog to Digital Converter ADM Asynchronous Delta Modulator SPDWOR Wired-OR Readout DVS Dynamic Vision Sensor AER Address event representation APM All Pulse Mode PCM Pulse Count Mode NHP Non-human Primate RMSE Root Mean Square Error CC Correlation Coefficient ThON(OFF) ON/OFF Pulse Generation Threshold Thspd Spike Detection Threshold SPD Spike Detection AT-SPD Absolute-threshold based SPD NEO-SPD Non-linear Energy Operator based SPD TDR Transmission Data Rate CR Compression Ratio I. I NTRODUCTION V . Mohan and W. P. Tay are with the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore. (e-mail: vivek- moh001@e.ntu.edu.sg, wptay@ntu.edu.sg) A. Basu is with the Department of Electrical Engineering, City University of Hong Kong. (e-mail: arinbasu@cityu.edu.hk) Note: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. A DV ANCES in neurotechnology in recent years has al- lowed partial restoration of lost sensory capabilities such as vision [1], speech [2] and touch [3] through stim- ulation, and limited motor capabilities in people suffering from motor impairment or paralysis. At the same time, a variety of neural sensors such as electroencephalography (EEG), electrocorticography (ECoG) and intracortical elec- trode based implantable brain-machine interface (iBMI) have demonstrated promising results for clinical applications [4]. A typical implementation of an iBMI system involves recording neural activity through a microelectrode array followed by amplification, filtering, and spike-detection stages to capture the action potentials which then may be decoded on- or off- chip to operate and control effectors such as prosthetic arm, computer cursor, mobility devices, etc., as shown in Fig. 1. Recent works using iBMI have enabled brain to handwritten text [7], brain-based speech synthesis [8], and therapies for epilepsy via deep brain stimulation and mental disorders [9]. Despite the compelling advances in the direction of futuristic BMI assistive technologies, the efficacy of such systems is limited by the number of recording channels. It, therefore, be- comes necessary for dexterous next generation of implantable brain-machine interfaces (Nx-iBMI) to support parallel record- ing from thousands of electrodes in order to improve iBMI performance and enable more sophisticated control of assistive technologies such as prosthetic arms. It is also necessary to consider implementing Nx-iBMI as wireless transcutaneous implants to reduce the risk of infection, enhance aesthetics, and user mobility, and allow stable recording for longer durations. Increasing the cellular coverage, i.e., the number of simultaneously recorded neurons, allows the study of granular neuronal interactions and cooperation like never before and potentially treating neural disorders or even enhancing sensory perception. With advances in semiconductor technologies, the general direction toward increasing neural signal resolution has resulted in the development of neural interfaces with higher channel counts [10]–[13] and thereby creating a trend of Moore’s law like doubling of the number of simultaneously recorded neurons about every 6.3 years as shown in Fig. 1(a). One of the main issues that come up with increasing the channel count is data bandwidth limitations and power dissi- pation, especially with wireless transmission. To illustrate the challenge with electrode scaling in neural implants, consider an example of Nx-iBMI that would have about 10, 000 chan- nels following Moore’s law like scaling of neural electrodes arXiv:2312.09503v1  [eess.SP]  15 Dec 2023IEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 2 Fig. 1. (a) Studies in [5] observed a Moore’s law like doubling of the number of simultaneously recorded neurons. This trend indicates the need to develop neural processing systems that scale well with electrode count while assuring robust performance within the allowable power, memory, and data rate budget. (adapted from [6]) (b) Block diagram of typical iBMI systems involving different schemes for multiplexing the analog/digital signals followed by different options to transmit the recorded signal, ranging from transmission of all recorded signals to transmitting decoded signals. (c) Trade-off plot, putting into perspective the existing iBMI systems and the proposed pipeline. as shown in Fig. 1(a). A major hurdle for the interface would be digitizing the massive amount of data (10 bits/sample ×30Ks/sec ×10, 000 channels = 3 Gbps of neural data) and transmitting it off-chip (as done in Schemes I-A and II-A in Fig. 1(b)). The power budget is constrained by the maximum allowable thermal power dissipation of ≈ 80 mW/cm2, and the temperature increase in the neural interface is restricted below 0.5◦C to prevent damage to brain tissue [14]. This points to the need for data compression in the implant to satisfy the increased channel count requirements of the future and to keep the transmission power low. Another issue relates to the wiring required to access numerous electrodes in a limited area, necessitating some form of multiplexing. In this work, we analyze a neuromorphic event-driven neural front-end that can potentially address both these issues. The rest of the paper is organized as follows. The following section discusses some of the related works and lists the key contributions of this work. Section III describes the proposed neuromorphic compression pipeline. Section IV presents the trade-off between compression and performance for the neu- romorphic compression pipeline and compares it with relevant conventional and novel methods from previous works. This is followed by a section V that discusses the main results and shows that our approach is scalable and yields expected results with two real datasets. Finally, we summarize our findings and conclude in the last section. II. R ELATED WORK AND CONTRIBUTION Various research works have made progressive improve- ments in the iBMI sensing architecture or processing flow. Following are some popular methods that try to address the issues that arise with increasing electrode count on neural recording arrays: A. Sub-array digitization As the number of recording channels increases, traditionally used analog multiplexing schemes (as shown in Fig. 1(b) Scheme-I) become prone to noise and interference, there- fore multiplexing digital signals becomes a preferred scalable solution. Newer neural recording arrays such as Neuropixel [11], Neuralink [15], Argo [13] and 59K in-vitro electrodes [12] attempt to tackle the power and data-rate limitations by either using programmable switch matrix or by implementing on-chip multiplexing for sub-array digitization, which is aIEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 3 hybrid between the Schemes I and II in Fig. 1(b) involving multiplexing of a group of recording electrodes in the analog domain before digitization, thereby making it feasible for only about 3 − 4% of the electrodes ( 2, 048 channels for Argo and 384 channels for Neuropixel) that can be addressed simultaneously. [10], [12] integrate the ADC under a neuro- pixel while increasing the electrode count. This allows full- frame readout from all electrodes besides ensuring high-SNR, robust digital readout from an arbitrarily selectable subset of electrodes. They however neither address the data deluge post-digitization nor provide compression strategies to realize scalable, low-power, wireless neural implants. B. On-chip compression Analog techniques such as spatial compression before dig- itization and compression via superposition were proposed in [16], [17]. However, they either involve large complexity hardware-intensive algorithms for the front end or are limited by noise summation, which limits the scalability of such neural implants. Compression schemes such as compressive sensing [18] and autoencoder [19] also fall short of the requirements to meet the available wireless data rates with increasing electrode count. On-chip spike sorting [20]–[23] and compression [24] have been proposed to reduce the transmitted data (as shown in Fig. 1(b) Scheme-I/II followed by Scheme-C). However, these do not tackle the issue of multiplexing and digitization in the front end of the neural implant sufficiently. C. Wired-OR readout (SPDWOR) An interesting image sensor inspired technique, presented in [25], exploits the spatio-temporal sparsity of neural signals to simultaneously achieve compression and digital multiplexing with wired-OR interactions. It provides a lossy compression by discarding samples in the baseline region via wired-OR contention among pixels retaining samples potentially corre- sponding to spikes. When multiple pixels compete for access to the limited wires, collision occurs, and more than one row/column decoders are activated, resulting in no unique decoding solution for recovery of quantized voltages. Such collisions may also occur for spike signals depending on the choice of the baseline, especially when neural firing is correlated– leading to loss of spike data. Reducing the loss of valid samples to collision would require a more complex wiring configuration and address decoding schemes. D. Activity dependent processing Data rate and power-sensitive methods propose transmitting a binary train of spike information indicating the presence or absence of spikes in time bins [26] for real-time clinically viable iBMI. While these methods reduce power consumption and data rate by an order of magnitude, they preclude features such as spike shapes for tasks such as spike sorting. However, the performance of iBMI systems employing such compres- sion with increasing electrode count on the neural interface array is still unknown. Some other works such as [27], [28] advocate the integration of decoders in the implant (as shown in Fig. 1(b) Scheme-I/II followed by Scheme-D). While this addresses the problem convincingly, it also limits the implant to specific tasks, reducing its adaptability in the future [29]. E. Neuromorphic compression Limitations imposed by power and bandwidth with electrode scaling on neural interfaces could be addressed effectively with the promising low-power neuromorphic approach as demon- strated previously for sensory applications such as event- based vision [30], audio [31], tactile [32], and olfactory [33] sensors. This approach essentially ‘morphs’ the initial sensory information processing stages of biological sensors/receptors into VLSI chips using a combination of efficient analog and digital circuitry to mimic their asynchronous spatio-temporal activity. A neuromorphic event-driven neural recording ap- proach was first proposed in [34] for iBMI and more recently implemented in [35]–[37]. The neuromorphic approach can also provide benefits of digital multiplexing due to integra- tion with address event representation (AER) circuits and digitizing/communicating data only during spikes by virtue of in-pixel thresholding. The AER-based handling of events could potentially address the shortcomings of data loss due to collisions in SPDWOR configuration. Works in [20], [35], [37] introduce a pipeline involving spike train generation using an analog-to-spike conversion block on-chip decoding using a spiking neural network, which limits its usability to specific tasks. While [35] uses a round-robin arbitration based AER scheme, [20], [37] does not have a collision handling strategy that could scale with the size of the neural electrode. None of the aforementioned neuromorphic/event- based neural recording schemes investigate the gains from using the neuromorphic compression in terms of reduction in data rate, nor assess its impact on iBMI performance. The fidelity of the compressed signal and the effect of collisions in the AER block of a neuromorphic compressive iBMI on task- specific performance when scaling to large Nx-iBMI systems has hitherto been unexplored. In this work, we investigate the architectural trade-offs of neuromorphic compression of neural signals for iBMI inspired by asynchronous event detection image sensors (AEDIS) such as the dynamic vision sensor (DVS) using next-generation large neuromorphic neural recording systems for simulated and realistic neural recordings. We make the following contribu- tions: • We explore the use of address event representation (AER) based readout with a pair of handshaking signals (request and acknowledgment) for elegant collision management to prevent loss of detected pulses by delaying them. • We assess the extent and effect of neuromorphic com- pression on spike shape and spike detection performance for two modes of transmission - ‘All pulse mode’ (APM) and ‘Pulse count mode’ (PCM) quantitatively with a set of standard evaluation metrics and compare them with other popular methods of transmitting neural signals. • We evaluate task-specific fidelity, specifically spike shape preservation and spike detection performance, for APM and PCM pipelines and investigate the performance vari-IEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 4 ation across different synthetic and real datasets (100- channel non-human primate, i.e., NHP and 384-channel Neuropixel recordings). • We assess the collision handling capability and effect of collision on signal recovery. • We discuss the scalability of the proposed method to a higher number of recording channels and the potential to compress the proposed read-out further. An initial version of this work was presented in [38] where the proposed neuromorphic compression based neural sensing pipeline was analyzed using a single-channel synthetic record- ing. This work extends the findings for higher channel count recordings including real neural recordings from non-human primates and mice to understand the scalability of the proposed pipeline, besides investigating its collision handling capability and exploring options for further compression. The neuromor- phic iBMI event dataset obtained from the simulation of the proposed pipeline is made publicly available at: https://sites. google.com/view/brainsyslab/neuromorphic-ibmi-dataset . III. M ETHODOLOGY The firing rate of a biological neuron is ≈ 1 − 200Hz. Combined with an approximate spike duration of 1 − 2 ms, this implies that biological spikes occupy a small fraction of samples in neural recordings. This temporal sparsity of spikes allows the use of an AER-based readout strategy that leads to high compression rates in large iBMI interfaces. A. Overview of the Proposed Pipeline A neuromorphic compression based neural sensing system as shown in Fig. 2(a) is proposed to consist of a DVS-pixel- like ON and OFF threshold crossing detection circuit [30] integrated into each cell, similar to the implementation in [34]. Generally, the front end comprises a capacitive low-noise amplifier with gain A1 followed by a second programmable gain stage with gain A2. The total gain of the amplifier stages is typically ≈ 200 − 1000. This is followed by an asynchronous delta modulator to generate an output Vmod and pulses p (+1/ON and -1/OFF). The AER readout strategy is facilitated by arbitration logic, address decoders, and a pair of handshaking signals for event readout. The readout events are then packetized depending on the mode of operation and transmitted wirelessly for further downstream processing of the neural event stream. The following subsections elaborate on the working principle of some of the main blocks of the proposed pipeline. B. Event Generation Each pixel consists of an asynchronous delta modulator typically composed of an input operational transconductance amplifier (OTA) with a capacitive divider gain stage, a pair of comparators, and inverters that generate the pulse train commonly referred to as ‘events’. ON ( +1) and OFF ( −1) pulses/events, are produced when the change in the amplified version of the input signal ( Vin) increases above the reference voltage, Vref, by ThON or decreases below Vref by ThOFF respectively. The pulse generation process can be described as follows: dVmod dt = A1A2 dVin dt p(t) =    1, Vmod(t +δ) = Vcm, if Vmod(t) > ThON, −1, Vmod(t +δ) = Vcm, if Vmod(t) < ThOFF, 0, Vmod(t +δ) = Vmod(t) +dVmod dt δ, otherwise, (1) where Vcm is the common mode voltage of the ADM. An example waveform with a spike reconstruction is shown in Fig. 2(b). Other implementations [39] have also combined this function in one stage, where the signal reconstruction method was different. An enhanced adaptive version of the asynchronous delta modulator, as introduced in [36], could be used to modulate and minimize the event generation rate by following the amplitude and noise characteristics of the input signal. This could effectively suppress event generation due to noise or abnormalities in the baseline region. For the purpose of simplicity, the event generation block in this work is assumed to contain a basic asynchronous delta modulator, as implemented in [34]. C. AER-based Readout Upon generation of ON/OFF events, the Req signal is generated and, an additional logic asserts the Ack signal once the event is readout, which in turn resets the pixel. The reset state will be held for a ‘refractory period’ determined by the values of the capacitor and the bias voltage in the pixel circuitry. The number of pulses generated by each pixel depends on the amplitude and frequency of the input signal, and also on the parameters of the delta modulator - ON/OFF pulse generate thresholds and refractory period. The pixels across the rows and columns of the electrode array are tied via a wired-OR connection to pass the read request (Req) to the readout block. Row (Req Yi) and column (Req Xi) request lines from each of the neural recording pixels are connected to the row (Y) and column (X) arbiter similar to DVS [30]. For realizing an AER-like readout and managing collision efficiently, we use a toggle tree fair X, Y-arbiter [40] to decide the sequence of readout when pulse read requests are generated by multiple electrodes simultaneously. As in any AER system, time represents itself and events are generated asynchronously. The address (xi, yi) and polarity ( pi) of the generated pulses (ON/ +1 or OFF/ −1) are communicated to the next stage. Acknowledgment signals (Ack Yi and Ack Xi) are then sent from the X, Y-arbiters to the cell (xi, yi) after read out, to reset its amplifier for continuing the delta compression. D. Event Packaging The proposed neuromorphic scheme allows the neural data to be encoded as a train of asynchronous binary ON/OFF pulses, instead of producing an n-bit word for each sample at the output of the ADC. Events from the event stream are packetized depending on the mode of transmission before they are transmitted. In this work, we investigate two modes ofIEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 5 Fig. 2. (a) Neuromorphic compression based neural sensing amplifier inspired from address event representation (AER) protocol used in event-based cameras that output ON/OFF pulses only when the input exceeds the ON/OFF threshold. The circuit implementation can be similar to the implementation in [34]. Generated pulses can be packetized and wirelessly transmitted in ‘All Pulse Mode’ (APM) or ‘Pulse Count Mode’ (PCM) (b) The pulse train generated and the reconstructed waveform, which approximates the original neural spike signal. operation: APM and PCM. In APM, all the generated pulses are transmitted off-chip asynchronously through a wireless link, with the event pulse packetized to contain the address and polarity of the event, (xi, yi, pi). In contrast, PCM involves electrode-wise accumulation of events with fixed-time bins. If the width of the bin is chosen to be ‘n’-sampling intervals long, then it is denoted as PCM‘n’ (e.g. PCM1, PCM2, and PCM4 have bin-widths equal to 1, 2, and 4 times the traditional sampling interval in neural recording systems respectively). The ON and OFF Events generated by an electrode at (xi, yi) location in PCM mode are packetized as the ON and OFF event counts - nON and nOFF as (xi, yi, nON, nOFF). For a bin width of duration tb, in PCM the ON/OFF events are accumulated as follows: nON(OFF) = X tb |p(t)|p(t)=1(−1) (2) The number of bits per APM event is, 1 + log2 Nr + log2 Nc whereas the number of bits per PCM event is, nON + nOFF + log2 Nr + log2 Nc where Nr × Nc is the size of the electrode array. In both modes, nothing is transmitted when no events are generated. At the receiver end, the electrode signals can be recovered by the accumulation of pulses depending on their polarity by stair-step reconstruction or directly processed using spiking neural networks, for further processing downstream processing such as spike detection, spike sorting, decoding, etc. IV. R ESULTS A. Simulation Setup Simulations were performed using the data processing pipeline shown in Fig.3. In order to incorporate realistic arbitration delay when simulating for larger electrode count, we simulated a toggle-tree fair arbiter as introduced in [40] in a 65 nm CMOS process using Cadence Virtuoso to determine the arbitration delay. The arbitration delay (tarb) was estimated to be in the order of a few nanoseconds. This delay was incorporated in the timing of colliding events by delaying transmission of the event by pe × tarb where pe is the event priority decided by the arbiter. Thus, the estimated arbitration delay was fed to the processing pipeline along with the recorded neural signals and, calibrated thresholds for ON/OFF pulse generation and spike detection ( ThON/ ThOFF and ThSPD respectively). To evaluate the correctness of neural signal encoding in the form of neural events using the proposed scheme, we recover the signal by following the steps detailed in Section - IV-C1. The recovered signal which approximates the original channel recording was thus obtained. The drift in the trace of the recovered signal from the baseline owing to the open loop asynchronous behavior of delta modulation was removed using a high-pass filter on the recovered signal. The following subsections elaborate on the neural recording datasets that were used for the simulations in this work, along with the evaluation metrics used and key results.IEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 6 Fig. 3. Block diagram of the data processing pipeline used to generate the simulation results for the proposed neuromorphic compression based neural sensing system. B. Dataset The simulations in this work have been performed for three diverse datasets, ranging from single-channel synthetic data to 100-channel non-human primate (NHP) motor cortex and 384-channel mice visual cortex neural recordings. The details of the datasets used are as follows: • Synthetic dataset provided in [41] with varying noise levels ( σnoise = 0.05, 0.1, 0.15 and 0.2) and sampling frequency of 24 KHz with spike duration of ≈ 1 − 2 ms. The dataset also contains ground truth for the spike detection task. For worst-case simulation of large n- channel electrode arrays, the single-channel signal is replicated to all the n-channels. • Non-human primate (NHP) motor cortex recordings con- taining 100 channels sampled at 30 KHz previously used in [27], [42] for decoding motor intention. Since no ground truth is available for this dataset, spike detection was performed on the original recorded signal using the absolute threshold method in [41]. The resultant spike detections were used as a ground truth of potential spike samples for performance analysis of the different compression methods investigated in this work. • Recordings of mice’s visual cortex to visual stimuli with 384 channels Neuropixel [43] at 30 KHz and < 7 µV RMS noise levels were made available in [44]. Spike detection ground truth containing potential spike times was obtained in the same way as was described for the NHP dataset above. Spike signal duration is < 1 µ S in the Neuropixel recording. C. Evaluation Metrics In order to analyze the effect of neuromorphic compression on iBMI we evaluate the spike information retention i.e., how well the compressed signal preserves spike shape (used in tasks such as spike sorting). This is measured in terms of signal recovery performance. We also show that better tradeoffs may be obtained by using other metrics, such as task-specific performance analysis. To this end, we use spike detection performance to assess the task-specific usability of signal recovered from the compressed data for downstream iBMI tasks, such as spike-based decoding. 1) Signal Recovery: Signal recovery was performed by stair-step reconstruction of the transmitted event pulses. We recover the signal from APM event packets by adding/subtracting the ThON/ThOFF value at the correspond- ing ON ( +1) and OFF ( +1) event times. Signal recovery was done for PCM packets by adding/subtracting ThON/ThOFF multiplied by the event counts ( nON/nOFF) in the event packets. The recovered signal was then resampled to match the sampling rate of the neural recording, as shown in Fig. 4(a). To evaluate the signal recovery performance, we used two common metrics - root-mean-square error (RMSE) and Pearson’s correlation coefficient (CC). RMSE was normal- ized to [0, 1]. These metrics are indicative of spike shape preservation in the event data obtained from the proposed APM/PCM pipeline. A Low RMSE and high CC indicate good spike shape preservation or high recovery, i.e., the recovered signal closely approximates the original signal and may be assumed to yield good performance in tasks such as spike cell classification/clustering that depend on spike shape. These metrics were computed between the original signal and the signal recovered from the event pulse packets. 2) Spike Detection (SPD): Evaluation of spike detection performance is done by determining detection accuracy (A), sensitivity (S), and false detection rate (FDR) as shown in Eq. 3 [45]. S = TP TP + FN; FDR = FP TP + FP; A = TP TP + FP + FN (3) Spike detections occurring within a tolerance window of tspike ± δtolerance (δtolerance here is about half the spike du- ration, i.e., ≈ 0.5 ms) are marked as true positives ( TP), spurious detections that are absent in the ground truth are marked as false positives ( FP) and the missed detections are marked as false negatives ( FN). Accuracy ( A), sensi- tivity ( S), and false detection rate ( FDR) calculated using Eq. (3) are used as metrics for measuring spike detection performance. Accuracy is an overall metric because it takes into account sensitivity and FDR. A high sensitivity is desired for SPD performance evaluation in firing rate based BMI systems, especially considering the sparsity of spikes. Since the synthetic dataset used contains ground truth for SPD, we performed SPD using both - the absolute threshold crossing method (AT-SPD) [41] and non-linear energy operator (NEO- SPD) [46] method involving threshold crossing based detec- tion on the NEO-enhanced signal. In the calibration stage, the spike detection threshold for NEO-SPD was determinedIEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 7 as ThSPD = 8 × median(NEO(neuralSignal)). Spikes are detected for all NEO(recoveredSignal) > ThSPD. For the signal recovered from events of the synthetic dataset, NEO- SPD and AT-SPD yielded similar SPD performance results. Therefore, for simplicity, AT-SPD was used for ‘potential spike times’ ground truth generation from NHP and Neuropixel datasets, and for the evaluation of SPD performance evaluation on the signal recovered from the events stream corresponding to these datasets. D. Choice of threshold For APM and PCM, the ON and OFF thresholds were determined from trade-off curves between threshold and per- formance metrics mentioned in IV-C. During the calibration stage (a few seconds long), the input-referred pulse generation threshold for each recording channel was determined as a factor of the spike amplitudes Vspike−max and is obtained as follows: ThON/OFF = ±k × Vspike−max (4) The factor k was obtained from trade-off curves as shown in Fig. 4(b) by sweeping k in the range (0, 1) in steps of 0.1 and determining the value at which k balances off the performance metrics and pulse generation rate. As shown in the left plot of Fig. 4(b), for the synthetic dataset, an input referred threshold obtained with of k = ±0.3 was found to provide a good tradeoff between data rate and signal recovery. As expected, spike detection required a less stringent threshold, obtained with k = ±0.45. ThON/OFF is a parameter of the asynchronous delta modulator and therefore affects the pulse generation rate. A larger ThON/OFF would result in a lower data rate. However, this results in coarser reconstruction of the signal from the pulse data and suppression of samples whose values are below the threshold, resulting in degraded signal recovery (affecting CC and RMSE) and spike detection performance (affecting Acc and S). Similar trade-off curves were obtained for the NHP and Neuropixel datasets, and a k = ±0.3 yielded better performance without a significant increase in the data rate as shown in Fig. 4(d-e). The more stringent threshold for signal recovery is used to report the results presented in the following subsections. E. Data rates for the different modes The theoretical model for the estimation of transmission data rate (TDR) in terms of the firing rate of the biological neuron, for the architecture studied in this work is: TDR = (Nr × Nc) × Rp × (nb + log2 Nr + log2 Nc) Rp =    fneu × NAP + Rnoise, for APM Rbin, for PCM fneu × Nspike for SPDWOR fs, for [10]–[12] nb =    1, for APM nb−ON + nb−OFF, for PCM bADC, for SPDWOR, [10]–[12] (5) Fig. 4. (a) Snippet of signal recovery for APM and PCM-1, 2, and 4 compared to the original neural signal shown by the blue line. (b) Illustration of choice of threshold from trade-off curves for PCM1 for the tasks of signal reconstruction and (c) spike detection for signal channel synthetic dataset. Optimal input referred thresholds ±k × Vspike−max for k = ±0.3 and ±0.45 for the two cases. Similar trade-off curves are obtained for choosing the threshold for the (d) 100-channel NHP and (e) 384-channel Neuropixel datasets. (f) Percentage error between the theoretically estimated TDR and the TDR obtained from the simulations. where Rp is the sample/pulse generation rate for an Nr × Nc array requiring nb bits to represent the pulse, fneu is the biological spike firing rate, NAP is the average number of pulses generated per spike, nb is the number of bits per pulse/sample, Rnoise is the pulse generation rate corresponding to non-spike samples. Rbin is the rate of non-empty event count bins in PCM, which depends on the number of bins per second (nb) and the probability of non-empty bin ( αb), and is related as follows: Rbin = αb × nb (6)IEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 8 Fig. 5. (a)-(d) Comparison of CR for APM and PCM on datasets with varying noise levels and neural firing rates. (e) Analysis of CR with increasing neural firing rates and comparison with different compression methods (f) Analysis of CR with increasing electrode count. αb in Eq. 6 represents the sparsity of PCM event counts. SPDWOR involves transmitting bADC bits for fneu × Nspike spike samples, where Nspike is the number of samples per spike. The theoretical model was validated by comparing with several seconds of synthetic data and was found to match well with an error of ±5%. F . Compression ratio The extent of compression for the proposed method is evaluated by computing the compression ratio (CR), which is defined as the ratio of the transmission data rate for full sample transmission ( TDRfs) as done in [10]–[12] to the transmission data rate of spike-sample transmission ( TDRspk) as done in SPDWOR [25] or that of the proposed PCM/APM (TDRAPM/PCMn). The CRs for different transmission modes (APM and PCM) were computed using the following equa- tions: CR1 = TDRfs TDRspk ; CR2 = TDRfs TDRAPM ; (7) CR3 = TDRfs TDRPCM1 ; CR4 = TDRfs TDRPCM4 The CR comparison plot shown in Fig 5 (a-d) was obtained for the synthetic dataset by sweeping the firing rate fneu of the channel and linearly extrapolating the compression ratio for different fneu for different background noise levels. As expected, the TDRAPM/PCMn increases with increasing noise levels (due to added events generated by background noise) and results in a drop in the CR. Higher the fneu higher is the APM/PCM event generation rate ( Rp in Eq. 5), resulting in a drop in the CR as fneu increases. An ideal SPDWOR- like implementation transmits all the samples (8-12 bits per sample besides the address bits) corresponding to each of the spikes, irrespective of the change in the signal value compared to the previously recorded sample. In contrast, APM transmits 1 bit and PCM transmits nON + nOFF (typically < 12) bits only when the signal exceeds ThON/OFF, thereby transmitting fewer bits per spike compared to SPDWOR. Therefore, at neural firing rates of ≈ 30 − 60 Hz, the CR of the proposed method is ≈ 20 − 50 and 6 − 8× SPDWOR [25] as shown in Fig.5(e). Even though CR in proposed methods decreases with increasing electrode numbers (Fig. 5(f)) due to a higher number of bits needed to encode the address, it is still ≈ 3× better than [25] for 10K channels. For the 100-channel NHP dataset with an average neural firing rate per channel of ≈ 62 Hz, CR1 = 3.23, CR2 = 25.2, CR3 = 15.4, and CR4 = 850 were obtained, which is consistent with the estimated CR in Fig. 5(f). G. Effect of compression As discussed in Section IV-C, signal recovery and spike detection metrics are used to understand the effect of compres- sion. Table I summarizes the effect of neuromorphic compres- sion on large iBMI for the two modes of pulse transmission- APM and PCM for bin widths that are 1,2 and 4 times (PCM1, PCM2, and PCM3) the sampling interval used in conventional neural signal transmission for all the three datasets introduced earlier. Over 90% of the spike shape can be fully recovered on average, from APM, whereas the recovered signal is relatively degraded with increasing levels of compression in PCM, as seen in the RMSE and CC columns of Table I. While PCM4 ensures lower TDR compared to APM, it comes at the cost of spike shape recovery. The higher the level of PCM, the coarser the event generation and recovery of the signal. Spike detection is a typical but significant task in firing- rate based iBMI systems. Therefore, it is necessary to as- sess whether the proposed APM/PCM effectively captures the spike-time information. We do this by performing spike detection on the signal recovered from the APM/PCM events, comparing the detection from the recovered signal with the ground truth, and quantitatively computing the metrics (A, S and FDR) described in Section IV-C2, it can also be observed that the sensitivity of spike detection is not significantlyIEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 9 TABLE I ANALYSIS OF THE EFFECT OF NEUROMORPHIC COMPRESSION . THE BEST RESULTS FOR EACH OF THE CRITERIA ARE HIGHLIGHTED IN BOLDFACE . Threshold = ± 0.3 × V spike − max Dataset #Channels Mode RMSE CC A S FDR DR (Mbps) Synthetic ( σnoise = 0.05) 10K APM 0.1054 0.923 92.82 92.82 0 76.75 PCM1 0.1236 0.8937 99.31 99.86 0.55 80.22 PCM2 0.13 0.8827 99.04 99.57 0.53 78 PCM4 0.7268 0.2681 71.23 71.81 1.13 63.59 Synthetic ( σnoise = 0.1) 10K APM 0.1196 0.9077 93.58 93.58 0 81.12 PCM1 0.1386 0.8752 97.68 99.72 2.06 85.51 PCM2 0.1401 0.8729 96.71 97.93 1.27 83.19 PCM4 0.4165 0.4506 70.47 71.17 1.37 67.19 Synthetic ( σnoise = 0.15) 10K APM 0.1368 0.8903 93.99 94.09 0.12 114.22 PCM1 0.1559 0.8561 95.53 96.45 0.99 124.86 PCM2 0.156 0.8563 89.15 90.32 1.43 126.25 PCM4 0.2344 0.7117 71.84 72.88 1.95 108.66 Synthetic ( σnoise = 0.2) 10K APM 0.1459 0.892 92.02 93.23 1.4 178.24 PCM1 0.1705 0.8499 89.5 90.38 1.08 202.29 PCM2 0.1696 0.8521 80.29 81.3 1.52 212.26 PCM4 0.2468 0.7183 71.19 71.91 1.38 192.54 NHP 100 APM 0.0983 0.8997 90.28 96.91 6.7 1.19 PCM1 0.1318 0.8515 87.32 93.2 8 1.95 PCM2 0.1443 0.8234 86.51 92.58 0.08 0.36 PCM4 0.2311 0.7422 65.01 88.38 0.29 0.04 Neuropixel 384 APM 0.0881 0.8981 84.94 89.45 5.54 6.58 PCM1 0.088 0.9005 74.39 91.04 19.63 10.99 PCM2 0.1186 0.854 71.36 89.63 22.15 2.03 PCM4 0.1922 0.7201 57.86 87.97 36.86 1.83 Neuropixel 100 APM 0.0808 0.8389 88.44 94.36 6.53 0.25 PCM1 0.0678 0.9101 72.58 91.2 21.88 0.46 PCM2 0.0979 0.8571 72.28 89.62 21.05 0.48 PCM4 0.2119 0.6238 63.39 86.17 28.93 0.42 affected by the poor spike shape recovery. A higher sensitivity ensures that spikes are not missed, especially those closer to the noise level, and this factor is all the more significant in noisy channels. It can be seen that spike detection on signals recovered from APM, PCM1, and PCM2 results in very high sensitivity. As expected, spike detection performance, particularly FDR and accuracy, degrades with increasing noise (0.05 to 2.0) and compression (APM to PCM4). For the NHP and Neuropixel datasets, the evaluation of performance metrics relied on how ‘potential spikes’ were defined in the absence of the ground truth. We determined the absolute threshold for spike detection, which is a few factors higher than the noise margin as typically done in AT-SPD, and recorded the time of the positive and negative spike-detection threshold ( ThSPD) crossing as potential spike times. Despite the constraint, the sensitivity of SPD and CC are decent for the NHP and Neuropixel datasets. We found that the spike information (spike time and shape) is well-preserved in the proposed APM/PCM. We leave spike detection directly using the generated APM/PCM events as future work. H. Collision Analysis Simulations performed in this work using the synthetic datasets inherently test the robustness of the neuromorphic compression pipeline, owing to the replication of single- channel signal to 10K channels. This results in creating a worst-case scenario where events are simultaneously generated by each of the 10K channels, requiring arbitration without loss of events. Thanks to the sparsity of spikes and the sampling interval, the arbiter has sufficient time to complete the AER handshaking and reading out of events in time without dropping any event, which is evident from the high CC and S recorded in Table I. The AER-based read-out process however introduces a small delay owing to the arbitration time in case of collision as shown in Fig. 6(a). For the NHP dataset, it was determined that an average of 10.0748±3.5741 channels collide in the recording, with 0−74 collisions at any instance. Similarly, for the Neuropixel dataset, 29.5113±33.51 channels collide, with ≈ 5 of them carrying spike samples. There are 0 −321 collisions at any instance in the Neuropixel recording. For the NHP and Neuropixel datasets, ≈ 25% of events correspond to spike samples undergoing collision. If spike-sample only transmission scheme akin to SPDWOR- like read-out were to be used in scenarios like this, it would result in the loss of the spike samples that undergo collision. While SPDWOR uses collisions to an advantage to suppress samples corresponding to the background activity, it also loses some samples corresponding to the spike. The reference [25] reports 80 − 90% spike recovery measure in terms ofIEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 10 Fig. 6. (a) Comparison of collision and no-collision scenarios (b) comparison of error due to collision spike sorting performance, which indicates over a tenth of spike information is lost. The spike sample loss using the SPDWOR scheme might be more prominent in high-density neural probes that exhibit a high spatiotemporal correlation among the neighboring recording sites/channels [47]. Owing to the inherent noise sample suppression of the ADM and collision handling mechanism of the AER-arbiter, the loss of spike samples is negligible for the proposed APM/PCM pipeline. This can be inferred from the change in RMSE due to collision is negligible and mainly due to delay in the sample as shown in Fig. 6(b). V. D ISCUSSION A. Scalability simulation and worst-case analysis In the simulations for the APM/PCM pipeline using the synthetic dataset, it is to be noted that the CRs reported are for the worst-case scenario where events are simultaneously generated in each of the 10K channels and the AER-arbiter handles the collisions. The worst-case analysis is a by-product of simulations with single-channel synthetic data of varying noise levels copied to 10K channels. We noticed there is only a small difference in the signal recovery performance due to TABLE II SPARSITY αb OF PCM EVENTS σnoise PCM1 PCM2 PCM4 0.05 0.0256 0.0407 0.0615 0.1 0.03625 0.06165 0.1013 0.15 0.0649 0.11855 0.21045 0.2 0.09505 0.17765 0.31255 the arbitration delay in the proposed pipeline. Since we did not implement the SPDWOR, we estimate the data rate based on the assumption that under ideal conditions, SPDWOR captures all the samples corresponding to the spike. Work in [25] demonstrates the scalability of the wired-OR readout for up to 512-channel electrode array with about 80% spike recovery, however, the increase in complexity of wiring and the effect of collision on the quality of spike information captured for higher electrode counts is still unknown. In order to analyze the scalability of the proposed neu- romorphic compression scheme for the real recordings, we take a subset of 100 channels with the highest firing-rate channels from the 384-channel Neuropixel dataset. As seen in the last few rows of Table I, TDR for APM/PCM drops when the number of channels is limited to 100 high firing- rate channels. We notice that TDR increases significantly for APM and PCM1 when scaled from 100 to 384 channels of the Neuropixel dataset while the increase is not as high for PCM2 and PCM4. The probability of non-empty bins (studied further in Section V-B) is higher for PCM2 and PCM4, as a result, the TDR does not change dramatically with scaling. On examining the cause for higher TDR in APM/PCM1 with scaling, we identified that a significant number of events generated by an ADM in APM/PCM with a single fixed threshold ( ThON/OFF with k = 0.3), correspond to background activity, resulting in higher TDR (as shown in green in Fig. 8(a)). In order to reduce TDR and improve CR, it is necessary to choose ( ThON/OFF) such that the ADM generates events only for the spike and suppresses background activity in the channel. In Section V-C we explore some schemes to reduce TDR and improve CR by suppressing event generation due to background activity. B. Sparsity of events While transmitting in PCM, as opposed to APM seems to have a better CR for lower noise and thresholds as seen from the results in the previous sections, the improvement in compression does not change dramatically at higher noise levels. To understand this effect, Table II presents the sparsity of PCM events, in other words, the probability of non-zero event counts in PCM bins ( αb in Eq. 6). It can be seen that there is a higher probability of non-empty bins in PCM at higher noise and for lower pulse generation thresholds, which results in higher TDR and therefore lower CR. Owing to the design of the asynchronous delta modulator ADC (ADM- based pulse generator) used in this work, a fast-rising or falling input signal such as spikes results in dense pulse generation which translates to higher APM event rate and higher PCM event counts per bin, whereas a smooth/flat input signal results in few or no event pulses. Thus, the sparsity of APM events orIEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 11 Fig. 7. (a) logical block diagram of a dual threshold pulse generation asynchronous delta modulator (b) event-filters to filter out spurious events and pass events potentially corresponding to spike. non-zero event counts depends on the choice of the ON/OFF pulse generation threshold. C. Further Compression A good threshold for ADM is critical in ensuring higher compression. Higher thresholds i.e., increasing the factor k can ensure that background activity causes no pulse/event gen- eration and improves overall compression, however, the spike shape captured might be coarser resulting in poor recovery. An adaptive delta modulator as introduced in [36] or a delta modulator with a two-level threshold - one for detecting the spike and the other for finer asynchronous sampling of the spike, may be implemented. Fig. 7(a) presents a logical block diagram for a simple dual threshold ADM with two sets of thresholds - ThHigh and ThLow. In this approach, the ADM initially operates with the higher threshold, until the input signal level exceeds ThHigh and then switches to a lower threshold ThLow for a fixed time Ttimer determined by the timer (typically, spike duration ≈ 1 − 3 ms) to capture the spike shape with finer step size. When the timer elapses, the ADM switches back to asyn- chronous sampling with ThHigh. Following the notation used in Eq. 4, ThHigh and ThLow are adjusted by varying the factors k1 and k2 respectively. Table IV summarizes the observation for two different values of k2 (k2 = 0.1 and 0.3), fixed value of Ttimer = 1ms and a fixed higher k1 = 2×k = 0.6 (k = 0.3 as discussed in Section IV-D). k2 = 0.3 results in higher CR (≈ 2× more than single-threshold ADM) and 16−18× higher compression than SPDWOR. As shown in Fig. 8(a), a dual- threshold ADM captures the spike shape well depending on the chosen k2 and prevents spurious events from background activity that are captured by single-threshold ADM. Another approach for increasing compression and suppress- ing background activity events is presented in Fig. 7(b), where Fig. 8. Comparison of single-threshold ( k = 0.3) and dual-threshold ( k1 = 0.6, k2 = 0.3/0.1) APM recovered signal with the band-pass filtered version of the original signal. (a) A dual-threshold ADM prevents background activity from generating events (b) Depending on the choice of the factor k2 for ThLow, spike shape can be captured with a coarser threshold ( k2 = 0.3) resulting in high compression or a finer threshold ( k2 = 0.1) resulting in better signal recovery performance. event-based temporal filters may be employed following a single-ADM neuromorphic compression based readout to limit the transmission of APM/PCM events corresponding to spikes only. Leveraging on the sparsity of events and pulse generation pattern discussed in Section V-B, the event filters may track the event rate (for APM) or event counts (for PCM) within a temporal neighborhood to determine whether the current event corresponds to a spike or not. An event from a channel may be said to result from a spike if it is preceded by a dense spike train in the near past from the same channel. A low- complexity event-based spike detector may thus be realized as a byproduct of the event filters. However, implementation of this approach is left for future work. With an average of 4 − 8 events per spike, the filtered event stream is estimated to result in up to 8× more compression per channel. Thus, with future improvements to the proposed pipeline to transmit events corresponding to detected spikes only, a CR of about 50 − 100 can be achieved for a mean neural firing rate of fAP = 60Hz.IEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 12 TABLE III COMPARISON OF CHARACTERISTICS AND IMPLEMENTATION OF DIFFERENT I BMI NEURAL RECORDING SYSTEMS Compression Technique/ Factor Full-sample recording [10]–[12] Spike-sample recording [47] On-chip spike sorting/ decoding [20]–[24], [27] SPDWOR [25] This work Spike shape preservation Y Y N Y Y Circuit complexity and wiring Moderate High High High Low Collision handling - - - N Y Scalability (No. of Channels) 384 (Neuropixel) 2048 (Argo) 256 64-384 512 10K (Synthetic) 384 (Neuropixel) Compression ratio (CR) 1 2-40 >240 2-20 20-100 TABLE IV COMPARISON OF CR FOR APM USING A DUAL THRESHOLD ASYNCHRONOUS DELTA MODULATOR AT FIRING RATE , fAP = 60HZ σnoise k1 k2 TDR (Mbps) CR2 CR2:CR1 0.05 0.6 0.1 199.9102 13 6 0.6 0.3 57.4173 42 18 0.1 0.6 0.1 204.856 12 5 0.6 0.3 60.2111 40 17 0.15 0.6 0.1 214.2423 12 5 0.6 0.3 63.0822 39 16 0.2 0.6 0.1 225.3674 11 5 0.6 0.3 66.2197 37 16 D. Hardware implementation We do not present circuit simulation of the Asynchronous Delta Modulator since such schemes have been published in several papers such as [34] or even the recent [35], [36]. Unlike SPDWOR, no expensive wiring is needed to handle collision scenarios, which are inherently handled by the AER-based readout strategy in our proposed pipeline. Neuromorphic circuits are known for low power consumption, and thus a neuromorphic compression based iBMI system is expected to consume far less power than conventional neural recording systems with full-sample transmission. Relying on the hardware measurements from [35] which implements an ADM-based pulse transmission in 40 nm CMOS technology, the ADM configured consumes ≈ 7 µW per channel. A neuromorphic compression based readout as proposed in this work, operating in low-power mode can be estimated to result in a surface power density of ≈ 4.36 mW/cm2, which lies well within the power dissipation budget of ≈ 80 mW/cm2 for iBMI [14]. E. Comparison with other works A summary of the high-level comparison of the proposed neuromorphic compression based neural sensing with prior works is presented in Table III. The proposed pipeline is a low-complexity, scalable, and high-compression neural record- ing system that preserves spike shape by handling collision scenarios effectively without the need for additional wiring. High compression may be obtained by recording systems that capture spike times only or perform on-chip decoding, however, this limits their adaptability to a limited set of iBMI tasks. VI. C ONCLUSION Neural electrode scaling for Nx-iBMI is severely limited by data rate and power budget. In this work, we quantitatively evaluate the effectiveness and extent of a neuromorphic com- pression based neural sensing architecture for iBMI, inspired by an asynchronous event detection image sensor such as the dynamic vision sensor. Transmission of pulses in APM or PCM results in compression ratios that are 2 − 5 times that of transmitting spike samples as in [25] and 15 − 20 times that of full sample transmission as in conventional CMOS- image sensor like readout architecture. We show that even with such high compression using the proposed neuromorphic architecture, there is ≈ 90% similarity of the recovered spike shape while ensuring a spike detection accuracy of more than 92% and a negligible false detection rate. A very high spike detection accuracy was obtained even for higher compression ratios of 5 − 50, thus demonstrating that neuromorphic com- pression based neural sensing systems can perform iBMI tasks well while lowering the data rate and the transmission power of Nx-iBMI. Transmission of event pulses corresponding to spikes alone can boost the compression ratio further to50−100 times that of full sample transmission and 5 − 18 times more compression than spike sample transmission. Future work will analyze the effects of neuromorphic compression on spike sorting and motor decoding tasks in large Nx-iBMI, and explore event-based processing for spike detection and iBMI decoding using spiking neural networks. ACKNOWLEDGMENT The work described in this paper was partially supported by a grant from the Singapore Ministry of Education Aca- demic Research Fund Tier 2 grant (MOE-T2EP20220-0002) and the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. CityU 11200922). REFERENCES [1] M. S. Beauchamp, D. Oswalt, P. Sun, B. L. Foster, J. F. Magnotti, S. Niketeghad, N. Pouratian, W. H. Bosking, and D. Yoshor, “Dynamic stimulation of visual cortex produces form vision in sighted and blind humans,” Cell, vol. 181, no. 4, pp. 774–783.e5, 2020. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S0092867420304967 [2] F. R. Willett, E. M. Kunz, C. Fan, D. T. Avansino, G. H. Wilson, E. Y . Choi, F. Kamdar, M. F. Glasser, L. R. Hochberg, S. Druckmann, K. V . Shenoy, and J. M. Henderson, “A high-performance speech neuroprosthesis,” Nature, vol. 620, no. 7976, pp. 1031–1036, Aug 2023. [Online]. Available: https://doi.org/10.1038/s41586-023-06377-xIEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 13 [3] S. N. Flesher, J. E. Downey, J. M. Weiss, C. L. Hughes, A. J. Herrera, E. C. Tyler-Kabara, M. L. Boninger, J. L. Collinger, and R. A. Gaunt, “A brain-computer interface that evokes tactile sensations improves robotic arm control,” Science, vol. 372, no. 6544, pp. 831–836, 2021. [Online]. Available: https://www.science.org/doi/abs/10.1126/science.abd0380 [4] K. Shen, O. Chen, J. L. Edmunds, D. K. Piech, and M. M. Maharbiz, “Translational opportunities and challenges of invasive electrodes for neural interfaces,” Nature Biomedical Engineering , vol. 7, no. 4, pp. 424–442, Apr 2023. [Online]. Available: https://doi.org/10.1038/s41551-023-01021-5 [5] I. H. Stevenson and K. P. Kording, “How advances in neural recording affect data analysis,” Nature Biomedical Engineerng , vol. 14, pp. 139– 142, Jan 2011. [6] I. Stevenson, “Tracking advances in neural recording.” [Online]. Available: https://stevenson.lab.uconn.edu/scaling/ [7] F. R. Willett, D. T. Avansino, L. R. Hochberg, J. M. Henderson, and K. V . Shenoy, “High-performance brain-to-text communication via handwriting,” Nature, vol. 593, no. 7858, pp. 249–254, May 2021. [Online]. Available: https://doi.org/10.1038/s41586-021-03506-2 [8] D. A. Moses, S. L. Metzger, J. R. Liu, G. K. Anumanchipalli, J. G. Makin, P. F. Sun, J. Chartier, M. E. Dougherty, P. M. Liu, G. M. Abrams, A. Tu-Chan, K. Ganguly, and E. F. Chang, “Neuroprosthesis for decoding speech in a paralyzed person with anarthria,” New England Journal of Medicine , vol. 385, no. 3, pp. 217–227, Jul. 2021. [Online]. Available: https://doi.org/10.1056/nejmoa2027540 [9] I. Basu, A. Yousefi, B. Crocker, R. Zelmann, A. C. Paulk, N. Peled, K. K. Ellard, D. S. Weisholtz, G. R. Cosgrove, T. Deckersbach, U. T. Eden, E. N. Eskandar, D. D. Dougherty, S. S. Cash, and A. S. Widge, “Closed-loop enhancement and neural decoding of cognitive control in humans,” Nature Biomedical Engineering , vol. 7, no. 4, pp. 576–588, Nov. 2021. [Online]. Available: https://doi.org/10.1038/s41551-021-00804-y [10] X. Yuan, A. Hierlemann, and U. Frey, “Extracellular recording of entire neural networks using a dual-mode microelectrode array with 19584 electrodes and high snr,” IEEE Journal of Solid-State Circuits , vol. 56, no. 8, pp. 2466–2475, 2021. [11] S. Wang, S. K. Garakoui, H. Chun, D. G. Salinas, W. Sijbers, J. Putzeys, E. Martens, J. Craninckx, N. Van Helleputte, and C. M. Lopez, “A com- pact quad-shank cmos neural probe with 5,120 addressable recording sites and 384 fully differential parallel channels,” IEEE Transactions on Biomedical Circuits and Systems , vol. 13, no. 6, pp. 1625–1634, 2019. [12] J. Dragas, V . Viswam, A. Shadmani, Y . Chen, R. Bounik, A. Stettler, M. Radivojevic, S. Geissler, M. E. J. Obien, J. M ¨uller, and A. Hi- erlemann, “In vitro multi-functional microelectrode array featuring 59 760 electrodes, 2048 electrophysiology channels, stimulation, impedance measurement, and neurotransmitter detection channels,” IEEE Journal of Solid-State Circuits , vol. 52, no. 6, pp. 1576–1590, 2017. [13] K. Sahasrabuddhe, A. A. Khan, A. P. Singh, T. M. Stern, Y . Ng, A. Tadi ´c, P. Orel, C. LaReau, D. Pouzzner, K. Nishimura, K. M. Boergens, S. Shivakumar, M. S. Hopper, B. Kerr, M.-E. S. Hanna, R. J. Edgington, I. McNamara, D. Fell, P. Gao, A. Babaie-Fishani, S. Veijalainen, A. V . Klekachev, A. M. Stuckey, B. Luyssaert, T. D. Y . Kozai, C. Xie, V . Gilja, B. Dierickx, Y . Kong, M. Straka, H. S. Sohal, and M. R. Angle, “The argo: a high channel count recording system for neural recording in vivo,” Journal of Neural Engineering , Dec. 2020. [Online]. Available: https://doi.org/10.1088/1741-2552/abd0ce [14] P. D. Wolf, “Thermal considerations for the design of an implanted cortical brain–machine interface (bmi),” in Indwelling Neural Implants: Strategies for Contending with the In Vivo Environment, W. M. Reichert, Ed. Boca Raton (FL): CRC Press/Taylor & Francis, 2008, ch. 3, https: //www.ncbi.nlm.nih.gov/books/NBK3932/. [15] E. M. and, “An integrated brain-machine interface platform with thousands of channels,” Journal of Medical Internet Research , vol. 21, no. 10, p. e16194, Oct. 2019. [Online]. Available: https://doi.org/10.2196/16194 [16] M. Shoaran, M. M. Lopez, V . S. R. Pasupureddi, Y . Leblebici, and A. Schmid, “A low-power area-efficient compressive sensing approach for multi-channel neural recording,” in 2013 IEEE International Sym- posium on Circuits and Systems (ISCAS) , 2013, pp. 2191–2194. [17] T. Okazawa and I. Akita, “A time-domain analog spatial compressed sensing encoder for multi-channel neural recording,” Sensors, vol. 18, no. 2, p. 184, Jan 2018. [Online]. Available: http://dx.doi.org/10.3390/ s18010184 [18] W. Zhao, B. Sun, T. Wu, and Z. Yang, “On-chip neural data compression based on compressed sensing with sparse sensing matrices,” IEEE Transactions on Biomedical Circuits and Systems , vol. 12, no. 1, pp. 242–254, 2018. [19] T. Wu, W. Zhao, E. Keefer, and Z. Yang, “Deep compressive autoencoder for action potential compression in large-scale neural recording,” Journal of Neural Engineering, vol. 15, no. 6, p. 066019, oct 2018. [Online]. Available: https://dx.doi.org/10.1088/1741-2552/aae18d [20] Y . Liu, J. L. Pereira, and T. G. Constandinou, “Clockless continuous- time neural spike sorting: Method, implementation and evaluation,” in 2016 IEEE International Symposium on Circuits and Systems (ISCAS) , 2016, pp. 538–541. [21] Y . Liu, S. Luan, I. Williams, A. Rapeaux, and T. G. Constandinou, “A 64-channel versatile neural recording soc with activity-dependent data throughput,” IEEE Transactions on Biomedical Circuits and Systems , vol. 11, no. 6, pp. 1344–1355, 2017. [22] Z. Zhang, O. W. Savolainen, and T. G. Constandinou, “Algorithm and hardware considerations for real-time neural signal on-implant processing,” Journal of Neural Engineering , vol. 19, no. 1, p. 016029, feb 2022. [Online]. Available: https://dx.doi.org/10.1088/1741-2552/ ac5268 [23] Y . Chen, B. Tacca, Y . Chen, D. Biswas, G. Gielen, F. Catthoor, M. Verhelst, and C. M. Lopez, “A 384-channel online-spike-sorting ic using unsupervised geo-osort clustering and achieving 0.0013mm2/ch and 1.78µW/ch,” in 2023 IEEE International Solid-State Circuits Con- ference (ISSCC), 2023, pp. 486–488. [24] M. Pagin and M. Ortmanns, “A neural data lossless compression scheme based on spatial and temporal prediction,” in 2017 IEEE Biomedical Circuits and Systems Conference (BioCAS) , 2017, pp. 1–4. [25] D. G. Muratore, P. Tandon, M. Wootters, E. J. Chichilnisky, S. Mitra, and B. Murmann, “A data-compressive wired-or readout for massively parallel neural recording,” in 2019 IEEE International Symposium on Circuits and Systems (ISCAS) , 2019, pp. 1–5. [26] N. Even-Chen, D. G. Muratore, S. D. Stavisky, L. R. Hochberg, J. M. Henderson, B. Murmann, and K. V . Shenoy, “Algorithm and hardware considerations for real-time neural signal on-implant processing,” Nature Biomedical Engineering, vol. 4, pp. 984–996, Aug 2020. [27] S. Shaikh, R. So, T. Sibindi, C. Libedinsky, and A. Basu, “Towards intel- ligent intracortical bmi (i2bmi): Low-power neuromorphic decoders that outperform kalman filters,” IEEE Transactions on Biomedical Circuits and Systems, vol. 13, no. 6, pp. 1615–1624, 2019. [28] Z. Zhang and T. G. Constandinou, “Firing-rate-modulated spike detection and neural decoding co-design,” Journal of Neural Engineering, vol. 20, no. 3, p. 036003, may 2023. [Online]. Available: https://dx.doi.org/10.1088/1741-2552/accece [29] S. Shaikh and A. Basu, Intelligent Intracortical Brain-Machine Interfaces. New York, NY: Springer New York, 2022, pp. 869–889. [Online]. Available: https://doi.org/10.1007/978-1-4614-3447-4 64 [30] P. Lichsteiner, C. Posch, and T. Delbruck, “A 128× 128 120 db 15 µs latency asynchronous temporal contrast vision sensor,” IEEE Journal of Solid-State Circuits, vol. 43, no. 2, pp. 566–76, 2008. [31] S.-C. Liu, A. van Schaik, B. A. Minch, and T. Delbruck, “Asynchronous binaural spatial audition sensor with 2 × 64 × 4 channel output,” IEEE Transactions on Biomedical Circuits and Systems , vol. 8, no. 4, pp. 453–464, 2014. [32] W. W. Lee, S. L. Kukreja, and N. V . Thakor, “A kilohertz kilotaxel tactile sensor array for investigating spatiotemporal features in neuromorphic touch,” in 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), 2015, pp. 1–4. [33] N. Imam and T. A. Cleland, “Rapid online learning and robust recall in a neuromorphic olfactory circuit,” Nature Machine Intelligence , vol. 2, no. 3, pp. 181–191, Mar. 2020. [Online]. Available: https://doi.org/10.1038/s42256-020-0159-4 [34] F. Corradi and G. Indiveri, “A neuromorphic event-based neural record- ing system for smart brain-machine-interfaces,” IEEE Transactions on Biomedical Circuits and Systems , vol. 9, no. 5, pp. 699–709, 2015. [35] Y . He, F. Corradi, C. Shi, S. van der Ven, M. Timmermans, J. Stuijt, P. Detterer, P. Harpe, L. Lindeboom, E. Hermeling, G. Langereis, E. Chicca, and Y .-H. Liu, “An implantable neuromorphic sensing system featuring near-sensor computation and send-on-delta transmission for wireless neural sensing of peripheral nerves,” IEEE Journal of Solid- State Circuits, vol. 57, no. 10, pp. 3058–3070, 2022. [36] M. Sharifshazileh and G. Indiveri, “An adaptive event-based data con- verter for always-on biomedical applications at the edge,” in 2023 IEEE International Symposium on Circuits and Systems (ISCAS) , 2023, pp. 1–5. [37] J. Chen, H. Wu, X. Liu, R. Eskandari, F. Tian, W. Zou, C. Fang, J. Yang, and M. Sawan, “Neuro bmi: A new neuromorphic implantable wireless brain machine interface with a 0.48 µw event-driven noise- tolerant spike detector,” in 2023 IEEE 5th International Conference on Artificial Intelligence Circuits and Systems (AICAS) , 2023, pp. 1–5.IEEE TRANSACTIONS PAPER, VOL. XX, NO. XX, MONTH XXXX 14 [38] V . Mohan, W. P. Tay, and A. Basu, “Architectural exploration of neuromorphic compression based neural sensing for next-gen wireless implantable-bmi,” in 2023 IEEE International Symposium on Circuits and Systems (ISCAS) , 2023, pp. 1–5. [39] C. Yi, A. Basu, and et al, “A digitally assisted, signal folding neural recording amplifier,” IEEE Transactions on Biomedical Circuits and Systems, vol. 8, no. 4, pp. 528–42, 2014. [40] A. M. T. Linn, D. A. Tuan, C. Shoushun, and Y . K. Seng, “Adaptive priority toggle asynchronous tree arbiter for aer-based image sensor,” in 2011 IEEE/IFIP 19th International Conference on VLSI and System-on- Chip, 2011, pp. 66–71. [41] R. Q. Quiroga, Z. Nadasdy, and Y . Ben-Shaul, “Unsupervised Spike Detection and Sorting with Wavelets and Superparamagnetic Clustering,” Neural Computation , vol. 16, no. 8, pp. 1661–1687, 08 2004. [Online]. Available: https://doi.org/10.1162/089976604774201631 [42] C. Libedinsky, R. So, and et al, “Independent mobility achieved through a wireless brain-machine interface,” PLOS One, vol. 11, no. 11, 2016. [43] C. M. Lopez, S. Mitra, J. Putzeys, B. Raducanu, M. Ballini, A. An- drei, S. Severi, M. Welkenhuysen, C. Van Hoof, S. Musa, and R. F. Yazicioglu, “22.7 a 966-electrode neural probe with 384 configurable channels in 0.13µm soi cmos,” in 2016 IEEE International Solid-State Circuits Conference (ISSCC) , 2016, pp. 392–393. [44] N. Steinmetz, M. Carandini, and K. D. Harris, “Single phase3 and dual phase3 neuropixels datasets,” 3 2019. [Online]. Available: https: //figshare.com/articles/ Single Phase3 Neuropixels Dataset/7666892 [45] Z. Zhang and T. G. Constandinou, “Adaptive spike detection and hardware optimization towards autonomous, high-channel-count bmis,” Journal of Neuroscience Methods , vol. 354, p. 109103, 2021. [Online]. Available: https://www.sciencedirect.com/science/article/pii/ S0165027021000388 [46] S. Mukhopadhyay and G. Ray, “A new interpretation of nonlinear energy operator and its efficacy in spike detection,” IEEE Transactions on Biomedical Engineering, vol. 45, no. 2, pp. 180–187, 1998. [47] S.-Y . Park, J. Cho, K. Lee, and E. Yoon, “Dynamic power reduction in scalable neural recording interface using spatiotemporal correlation and temporal sparsity of neural signals,” IEEE Journal of Solid-State Circuits, vol. 53, no. 4, pp. 1102–1114, 2018. Vivek Mohan (S’ 14) received his B.Tech. degree in Electronics and Communication Engineering from JNTU - Hyderabad in 2016 and a Joint M.Sc. in Integrated Circuit Design from the Technical University of M ¨unchen and Nanyang Technological University (NTU) Singapore in 2019. After grad- uation, he worked as a Research Associate in the BRAIN Systems Lab - CICS, NTU in the area of neuromorphic computer vision. Vivek is currently a Ph.D. candidate at the School of Electrical and Electronic Engineering, NTU. His research interests include integrated circuit design, neurotechnology, neuromorphic engineering, and brain-machine interface. Wee Peng Tay (SM) received the B.S. degree in Electrical Engineering and Mathematics, and the M.S. degree in Electrical Engineering from Stanford University, Stanford, CA, USA, in 2002. He received the Ph.D. degree in Electrical Engineering and Com- puter Science from the Massachusetts Institute of Technology, Cambridge, MA, USA, in 2008. He is a Professor of Signal and Information Processing in the School of Electrical and Electronic Engineering at Nanyang Technological University, Singapore. Dr. Tay received the Tan Chin Tuan Exchange Fellow- ship in 2015. He is a co-author of the best student paper award at the Asilomar Conference on Signals, Systems, and Computers in 2012, the IEEE Signal Processing Society Young Author Best Paper Award in 2016, and the best paper award at the International Conference on Smart Power & Internet Energy Systems in 2022. He was an Associate Editor for the IEEE Transactions on Signal Processing (2015 – 2019) and an Editor for the IEEE Transactions on Wireless Communications (2017 – 2023). He is currently an Associate Editor for the IEEE Transactions on Signal and Information Processing over Networks, an Associate Editor for the IEEE Internet of Things Journal, and an Editor for the IEEE Open Journal of Vehicular Technology. His research interests include signal and information processing over networks, distributed inference and estimation, statistical privacy, and robust machine learning. Arindam Basu Arindam Basu received the B.Tech and M.Tech degrees in Electronics and Electrical Communication Engineering from the Indian Insti- tute of Technology, Kharagpur in 2005, the M.S. degree in Mathematics and PhD. degree in Electrical Engineering from the Georgia Institute of Technol- ogy, Atlanta in 2009 and 2010 respectively. Dr. Basu received the Prime Minister of India Gold Medal in 2005 from I.I.T Kharagpur. He is currently a Professor in City University of Hong Kong in the Department of Electrical Engineering and was a tenured Associate Professor at Nanyang Technological University before this. He is currently an Associate Editor of the IEEE Sensors journal, Frontiers in Neuroscience, IOP Neuromorphic Computing and Engineering, and IEEE Transactions on Biomedical Circuits and Systems. He has served as IEEE CAS Distinguished Lecturer for the 2016-17 period. Dr. Basu received the best student paper award at the Ultrasonics symposium, in 2006, the best live demonstration at ISCAS 2010, and a finalist position in the best student paper contest at ISCAS 2008. He was awarded MIT Technology Review’s TR35 Asia Pacific award in 2012 and inducted into Georgia Tech Alumni Association’s 40 under 40 class of 2022.",
      "references": [
        "Dynamic stimulation of visual cortex produces form vision in sighted and blind humans",
        "A high-performance speech neuroprosthesis",
        "A brain-computer interface that evokes tactile sensations improves robotic arm control",
        "Translational opportunities and challenges of invasive electrodes for neural interfaces",
        "How advances in neural recording affect data analysis",
        "Tracking advances in neural recording",
        "High-performance brain-to-text communication via handwriting",
        "Neuroprosthesis for decoding speech in a paralyzed person with anarthria",
        "Closed-loop enhancement and neural decoding of cognitive control in humans",
        "Extracellular recording of entire neural networks using a dual-mode microelectrode array with 19584 electrodes and high snr",
        "A compact quad-shank cmos neural probe with 5,120 addressable recording sites and 384 fully differential parallel channels",
        "In vitro multi-functional microelectrode array featuring 59 760 electrodes, 2048 electrophysiology channels, stimulation, impedance measurement, and neurotransmitter detection channels",
        "The argo: a high channel count recording system for neural recording in vivo",
        "Thermal considerations for the design of an implanted cortical brain–machine interface (bmi)",
        "An integrated brain-machine interface platform with thousands of channels",
        "A low-power area-efficient compressive sensing approach for multi-channel neural recording",
        "A time-domain analog spatial compressed sensing encoder for multi-channel neural recording",
        "On-chip neural data compression based on compressed sensing with sparse sensing matrices",
        "Deep compressive autoencoder for action potential compression in large-scale neural recording",
        "Clockless continuous-time neural spike sorting: Method, implementation and evaluation",
        "A 64-channel versatile neural recording soc with activity-dependent data throughput",
        "Algorithm and hardware considerations for real-time neural signal on-implant processing",
        "A 384-channel online-spike-sorting ic using unsupervised geo-osort clustering and achieving 0.0013mm2/ch and 1.78µW/ch",
        "A neural data lossless compression scheme based on spatial and temporal prediction",
        "A data-compressive wired-or readout for massively parallel neural recording",
        "Towards intelligent intracortical bmi (i2bmi): Low-power neuromorphic decoders that outperform kalman filters",
        "Firing-rate-modulated spike detection and neural decoding co-design",
        "Intelligent Intracortical Brain-Machine Interfaces",
        "A 128× 128 120 db 15 µs latency asynchronous temporal contrast vision sensor",
        "Asynchronous binaural spatial audition sensor with 2 × 64 × 4 channel output",
        "A kilohertz kilotaxel tactile sensor array for investigating spatiotemporal features in neuromorphic touch",
        "Rapid online learning and robust recall in a neuromorphic olfactory circuit",
        "A neuromorphic event-based neural recording system for smart brain-machine-interfaces",
        "An implantable neuromorphic sensing system featuring near-sensor computation and send-on-delta transmission for wireless neural sensing of peripheral nerves",
        "An adaptive event-based data converter for always-on biomedical applications at the edge",
        "Neuro bmi: A new neuromorphic implantable wireless brain machine interface with a 0.48 µw event-driven noise-tolerant spike detector",
        "Architectural exploration of neuromorphic compression based neural sensing for next-gen wireless implantable-bmi",
        "A digitally assisted, signal folding neural recording amplifier",
        "Adaptive priority toggle asynchronous tree arbiter for aer-based image sensor",
        "Unsupervised Spike Detection and Sorting with Wavelets and Superparamagnetic Clustering",
        "Independent mobility achieved through a wireless brain-machine interface",
        "22.7 a 966-electrode neural probe with 384 configurable channels in 0.13µm soi cmos",
        "Single phase3 and dual phase3 neuropixels datasets",
        "Adaptive spike detection and hardware optimization towards autonomous, high-channel-count bmis",
        "A new interpretation of nonlinear energy operator and its efficacy in spike detection",
        "Dynamic power reduction in scalable neural recording interface using spatiotemporal correlation and temporal sparsity of neural signals"
      ],
      "meta_data": {
        "arxiv_id": "2312.09503v1",
        "authors": [
          "Vivek Mohan",
          "Wee Peng Tay",
          "Arindam Basu"
        ],
        "published_date": "2023-12-15T03:01:35Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This work introduces a neuromorphic compression-based neural sensing architecture with address-event representation (AER) inspired readout protocol for massively parallel, next-generation wireless implantable Brain Machine Interfaces (iBMI). Key contributions include achieving a data compression ratio of 50-100 (5-18x more than prior work) by selectively transmitting neural spike event pulses, with a correlation coefficient of ≈ 0.9 and spike detection accuracy over 90%. The paper explores AER-based readout with handshaking signals for elegant collision management, assesses the impact of neuromorphic compression on spike shape and spike detection performance for 'All Pulse Mode' (APM) and 'Pulse Count Mode' (PCM), evaluates task-specific fidelity across synthetic and real datasets, analyzes collision handling capabilities, and discusses scalability and potential for further compression.",
        "methodology": "The proposed neuromorphic compression based neural sensing system integrates a DVS-pixel-like ON and OFF threshold crossing detection circuit in each cell. The front end comprises a low-noise amplifier, a programmable gain stage, and an asynchronous delta modulator (ADM) that generates ON (+1) and OFF (-1) pulses/events when the input signal's change exceeds/falls below predefined thresholds (ThON/ThOFF). An AER-based readout strategy, facilitated by arbitration logic, address decoders, and handshaking signals (Req, Ack), uses a toggle tree fair X, Y-arbiter to manage simultaneous event requests and prevent data loss due to collisions. Events are then packetized for wireless transmission in two modes: APM, where all generated pulses (address, polarity) are transmitted asynchronously; and PCM, where electrode-wise ON/OFF event counts are accumulated within fixed-time bins (e.g., PCM1, PCM2, PCM4) before transmission. Signal recovery is achieved through stair-step reconstruction of these event pulses, followed by high-pass filtering to remove baseline drift.",
        "experimental_setup": "Simulations were performed using a data processing pipeline incorporating realistic arbitration delays (few nanoseconds) estimated from a toggle-tree fair arbiter simulated in a 65 nm CMOS process using Cadence Virtuoso. Three datasets were used: a synthetic dataset from [41] (single-channel replicated to 10K channels for worst-case, varying noise levels from 0.05 to 0.2, 24 KHz sampling, with ground truth for spike detection); 100-channel non-human primate (NHP) motor cortex recordings from [27], [42] (30 KHz sampling, ground truth derived from absolute threshold detection on original signal); and 384-channel mice visual cortex Neuropixel recordings from [43], [44] (30 KHz sampling, ground truth derived similarly to NHP). Input-referred pulse generation thresholds (ThON/OFF) were calibrated as ±k × Vspike−max, with k typically 0.3 for signal recovery and 0.45 for spike detection. Evaluation metrics included Root-Mean-Square Error (RMSE) and Pearson's Correlation Coefficient (CC) for signal recovery, and Accuracy (A), Sensitivity (S), and False Detection Rate (FDR) for spike detection (using AT-SPD or NEO-SPD).",
        "limitations": "The compression level in PCM modes, especially PCM4, can lead to degraded spike shape recovery (higher RMSE, lower CC), although spike detection sensitivity remains high. The use of a single fixed threshold in the asynchronous delta modulator can result in a significant number of events generated by background activity, leading to higher transmission data rates and reduced compression ratios. For the NHP and Neuropixel datasets, the absence of true ground truth necessitated the use of 'potential spike times' derived from absolute threshold crossing on the original signal, which may introduce inaccuracies. While the AER-based readout effectively handles collisions, it introduces a small arbitration delay. The current work evaluates spike detection on *recovered* signals rather than directly using the generated event streams, and does not yet address the effects of compression on downstream tasks like spike sorting and motor decoding.",
        "future_research_directions": "Future work includes exploring adaptive delta modulators, such as an enhanced adaptive ADM or a dual-threshold ADM (ThHigh for initial detection, ThLow for finer spike shape capture), to further minimize event generation from background activity and improve compression. Another direction involves implementing event-based temporal filters to transmit only events corresponding to detected spikes, leveraging the sparsity and generation patterns of events to boost compression ratios. The research also aims to analyze the effects of neuromorphic compression on spike sorting and motor decoding tasks in large Nx-iBMI directly using the generated APM/PCM events, and to explore event-based processing for spike detection and iBMI decoding using spiking neural networks.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Reasoning Like Program Executors",
      "full_text": "The fate of the “vacuum point” and of grey solitons in dispersive quantum shock waves in a one-dimensional Bose gas S. A. Simmons, 1 J. C. Pillay, 2 and K. V. Kheruntsyan 1 1School of Mathematics and Physics, The University of Queensland, Brisbane, Queensland 4072, Australia 2Quantum Brilliance Pty Ltd, 60 Mills Road, Acton ACT 2601, Australia (Dated: July 27, 2023) We continue the study of dispersive quantum shock waves in a one-dimensional Bose gas beyond the mean-field approximation. In a recent work by Simmons et al. [Phys. Rev. Let. 125, 180401 (2020)], the oscillatory shock wave train developing in this system from an initial localized density bump on a uniform background was interpreted as a result of quantum mechanical self-interference, wherein the interference contrast would diminish with the loss of matter-wave phase coherence. Such loss of coherence, relative to the mean-field Gross-Pitaevskii description, occurs due to either quantum or thermal fluctuations, as well as in the strongly interacting regime. In this work, we extend the analysis of dispersive quantum shock waves in this context to other dynamical scenarios. More specifically, the scenarios studied include evolution of a sufficiently high density bump, known to lead to the so-called “vacuum point” in the mean-field description, and evolution of an initial density dip, known to shed a train of grey solitons in the same mean-field approximation. We study the fate of these nonlinear wave structures in the presence of quantum and thermal fluctuations, as well as at intermediate and strong interactions, and show that both the vacuum point and grey solitons cease to manifest themselves beyond the mean-field approach. On the other hand, we find that a vacuum point can occur in an ideal (noninteracting) Bose gas evolving from a ground state of a localized dimple potential. Due to the ubiquity of dispersive shock waves in nature, our results should provide useful insights and perspectives for a variety of other physical systems known to display nonlinear wave phenomena. I. INTRODUCTION During the last two decades there has been a keen in- terest, which has peaked again just recently, in the study of the formation and propagation of dispersive quan- tum shock waves in ultra-cold atomic gases and Bose- Einstein condensates [1–27]. This is, in part, due to the high degree of experimental control that we have over these systems, making them an ideal platform to real- ize scenarios best suited to understand the rich out-of- equilibrium phenomena that arise in interacting quan- tum many-body systems. Exploring these phenomena in one-dimensional (1D) Bose gases has an added benefit that comes with the integrability of the underlying Lieb- Liniger model and the availability of various theoretical tools that make dynamical many-body simulations com- putationally tractable. The focus on dispersive quantum shock waves in ultra- cold Bose gases originates largely through the use of the mean-field Gross-Pitaevskii equation (GPE). Such a mean-field treatment, however, ignores the effects of quantum fluctuations and correlations, despite the fact that the phenomenon under investigation is referred to as a ‘quantum’ shock wave. On the other hand, the 1D GPE is a specific form of the widely applicable non-linear Schr¨ odinger (NLS) equation [28] and is closely related to the Korteweg–de Vries (KdV) equation [29], both of which are staple foundations used in the hydrodynamic description of wave breaking in dispersive media and soli- ton formation in nonlinear media [23, 27, 30, 31]. More- over, in the Thomas-Fermi or weakly-dispersive regime of the GPE, Whitham modulation theory [32] provides an approximate analytic description [27] of the formation, shape, and internal structure of dispersive shock waves, allowing for deep analytical insights to be obtained in these matter-wave systems. In this work, we continue the study of quantum shock wave scenarios in a 1D Bose gas that extend beyond the weakly-dispersive, mean-field GPE regime. More specifi- cally, we study the dissolution of a localized density bump on an otherwise uniform background through dispersive shock waves and extend the analysis of Ref. [25] to the case of a relatively large bump amplitude. At the mean- field GPE level, this scenario is known to lead to the formation of a vacuum point wherein the fluid density becomes zero at some point inside the shock wave train. Here, we go beyond the mean-field GPE description by incorporating the effects of quantum or thermal fluctua- tions using, respectively, the truncated Wigner (TWA) or classical-field stochastic Gross-Pitaveskii (SPGPE) [33– 35] approaches. Additionally, we treat the regimes of in- termediate and strong interactions using infinite matrix product states (iMPS). We show that all these beyond- mean-field effects lead to the disappearance of the vac- uum point. On the other hand we find that the vacuum point can exist in the limit of a noninteracting (ideal) Bose gas at zero temperature. In addition to the density bump scenario, we also study the dynamics of the dissolution of a localized density dip. This scenario is known to lead to a train of grey solitons at the mean-field GPE level, however, we show that in- corporating the effects of quantum and thermal fluctu- ations, as well as strong inter-particle interactions, can strongly modify the shape of the solitons or indeed cease arXiv:2305.17647v3  [cond-mat.quant-gas]  26 Jul 20232 their very formation. In all scenarios, apart from analyzing the dynamics of the mean particle number density ρ(x, t) as in Ref. [25], we also monitor the evolution of the mean current den- sity j(x, t). The current density arises naturally in the context of the mean-field superfluid hydrodynamic equa- tions [see Eqs. (9) and (10) below], which are mathemat- ically equivalent to the GPE, and serves as an impor- tant and fundamental tool in the field of transport and non-equilibrium physics. In particular, it has been used to characterize the dynamics of ultra-cold Bose gases in scenarios where shock waves are generated by pushing a homogeneous gas against a hard wall boundary [21, 26]. In this situation the gas sloshes back and forth against the two walls and the current density can be used to iden- tify and characterize the velocity of the generated wave fronts. For such a purpose, the rich oscillatory features of the current that reside in the shock front can be ignored. Here, however, we examine those features explicitly and focus on the insights they provide into our understand- ing of the fluid dynamics in dispersive shock scenarios. Moreover, we highlight the fact that whilst this discus- sion takes place in the context of ultra-cold Bose gases, a broad range of physical systems and dynamical situa- tions are known to produce these types of shock waves [11–17, 19, 23, 36–46] (which have been recognized for their fundamental and ubiquitous nature [23]), and any system governed by the nonlinear Schr¨ odinger equation or GPE is likely to benefit from a number of the insights we obtain here as well. II. MODEL We start by recalling the Lieb-Liniger Hamiltonian for a uniform 1D Bose gas, in the second quantized form, describing a system ofN particles free to move along a 1D ring (i.e., with periodic boundary conditions) of length L and interacting via repulsive contact interactions [47]: ˆH1D = − ℏ2 2m ˆ dx ˆΨ†(x) ∂2 ∂x2 ˆΨ(x) + g1D 2 ˆ dx ˆΨ†(x)ˆΨ†(x)ˆΨ(x)ˆΨ(x). (1) Here, ˆΨ(x) is the bosonic field operator, m is the mass of the particles, and g1D is the interaction strength, with g1D > 0 for repulsive interactions. Away from con- finement induced resonances, the interaction strength is given by g1D ≃ 2ℏaω⊥ [48], where a is the three- dimensional s-wave scattering length and ω⊥ is the fre- quency of the transverse trapping potential, which is as- sumed harmonic. We study shock waves that are generated from an ini- tial density bump or dip on top of a non-zero uniform background. During evolution, the initial density gradi- ents steepen, and a shock front forms. We monitor the fate of this shock front at a range of interaction strengths and initial equilibrium temperatures. In addition to ana- lyzing the evolution of the mean particle number density ρ(x, t) = ⟨Ψ†(x, t)Ψ(x, t)⟩, (2) as in our previous work Ref. [25], here we also present and discuss the results for the mean current density, j(x, t) = ℏ 2mi * ˆΨ†(x, t)∂ ˆΨ(x, t) ∂x − ˆΨ(x, t)∂ ˆΨ†(x, t) ∂x + . (3) The current density provides a useful probe in shock wave dynamics, since it offers a measure of the flow of particles and allows one to discern the motion of particles inside the shock region. The local sign of the current determines the direction of density flow in that region, where a pos- itive (negative) sign indicates that particles are moving to the right (left) and zero current indicates that there is no net flow at that position in the fluid. In all shock wave scenarios simulated below, the dy- namics are initiated from an initial (t = 0) density profile of the form ρ(x, 0) = Nbg(1 + βe−x2/2σ2 )2/L, (4) Such a density profile can be prepared prior to timet = 0, either exactly or approximately [49], as the ground state (at zero temperature T = 0) or a thermal equilibrium state (at nonzero temperature T ̸= 0) of a suitably cho- sen trapping potential. The trapping potential is then suddenly removed at time t = 0, and the system with the above initial density profile is evolved under the Lieb- Liniger Hamiltonian, i.e., in a uniform potential of length L with periodic boundary conditions. The above ini- tial condition in the superfluid hydrodynamic description (see below) also assumes a zero initial current density j0(x, 0) = 0, or equivalently a zero initial velocity field v0(x, 0). In the mean-field and ideal Bose gas descrip- tions, on the other hand, we assume that the initial wave functions are given by Ψ 0(x, 0) = Ψ( x, 0) = p ρ(x, 0) (i.e., we assume that they are real-valued, without loss of generality). In all cases, σ in Eq. (4) controls the width of the density profile and Nbg = |ψbg|2L = ρbgL denotes the number of particles in the homogeneous background, related to the total number of particles via N =Nbg \u0000 1 + √πβσ L [β erf( L 2σ ) + 2 √ 2 erf( L 2 √ 2σ )] \u0001 . Further- more, β provides the height or depth of the initial density perturbation, and we place no restriction on the sign of β so as to explore both density bump situations where β > 0, and additionally density dip situations where −1 < β <0. Within the Lieb-liniger model, a uniform system can be characterized by the dimensionless interaction param- eter γ = mg1D/ℏ2ρ, which provides the ratio of interac- tion energy to kinetic energy within the system. In the non-uniform density bump and dip scenarios under con- sideration, we choose to characterize the initial state of3 the gas using the value γbg = mg1D ℏ2ρbg , (5) at the background density ρbg. The weakly interacting regime of the 1D Bose gas is realized when γbg ≪ 1 [47], and at zero temperature (T = 0) the dynamics of this system can be well approximated by the Gross-Pitaevskii equation (GPE) [ ? ], iℏ ∂ ∂t Ψ0 = \u0012 − ℏ2 2m ∂2 ∂x2 + g1D|Ψ0|2 \u0013 Ψ0, (6) for the complex mean-field amplitude Ψ 0(x, t), given by Ψ0(x, t) = ⟨ˆΨ(x, t)⟩ in the spontaneously broken symme- try approach. Performing a Madelung transformation on the GPE to a set of real valued density and phase variables, Ψ0(x, t) = p ρ0(x, t)eiS0(x,t), and then defining the fluid velocity via v0(x, t)= ℏ m ∂ ∂x S0(x, t), leads to the superfluid hydrodynamic equations ∂ρ0 ∂t = − ∂ ∂x (ρ0v0), (7) ∂v0 ∂t = − ∂ ∂x \u00121 2v2 0 + g1Dρ0 m − ℏ2 2m2 1√ρ0 ∂2 ∂x2 √ρ0 \u0013 . (8) While these equations are written in terms of the fluid density and velocity, it is much more natural from a hydrodynamic perspective to consider equations for the conserved ‘charges’ of the system. For a Bose gas at zero temperature one would consider these to be the fluid den- sity ρ0(x, t) and the fluid ‘momentum’ or current density j0(x, t). Hence, the current density arises naturally in the conservative form of the superfluid hydrodynamic equa- tions which are given by ∂ρ0 ∂t = − ∂ ∂x j0, (9) ∂j0 ∂t = − ∂ ∂x \u0014 1 ρ0 j2 0 + P m \u0015 + ρ0 ∂ ∂x \u0014 ℏ2 2m2√ρ0 ∇2√ρ0 \u0015 , (10) where the current density j0(x, t) [51] and the pressure of the gas P(x, t) are given by j0 = ρ0v0 = ℏ 2mi \u0012 Ψ∗ 0 ∂Ψ0 ∂x − Ψ0 ∂Ψ∗ 0 ∂x \u0013 , (11) and P(x, t) = g1Dρ0(x, t)2 2 , (12) respectively. The last term in Eq. (10) represents the so-called quan- tum pressure, which is responsible for the formation of the interference patterns that are produced in quantum shock waves [25]. This term does not behave like the usual pressure P(x, t) but rather it acts like a source term or effective Bohm potential [52] in the dynamics of the Bose gas, generating forces based on spacial variations in the density. We note that Eq. (3) simplifies to the mean-field ex- pression j0 = ρ0v0 of Eq. (11) under the spontaneous symmetry breaking assumption ⟨ˆΨ⟩ = Ψ0. In the ideal Bose gas regime (see below) one uses the same expression of Eq. (11) but with the mean-field amplitude Ψ0(x, t) re- placed by the actual Schr¨ odinger wave function Ψ(x, t). Expressions to compute the current density within the other approaches we employ in this work can be found in their respective appendices. Hereafter we denote the current density and particle number density within any approach simply as j and ρ respectively, with the un- derstanding that different expressions apply depending on the interaction regime and approach used to calculate these quantities. III. DENSITY BUMP SCENARIOS We begin our analysis by examining situations where β > 0, i.e., for scenarios with an initial density bump that then expands into the non-zero background during evolution. A. Ideal Bose gas and weakly interacting regimes In the ideal (noninteracting) Bose gas regime at T = 0, the dynamics are governed by the standard time- dependent Schr¨ odinger equation for the wave function evolving from the ground-state Ψ( x, 0) = p ρ(x, 0) = ψbg(1 +βe−x2/2σ2 ). In the single-particle case, the wave function is normalized to one, whereas in the many- particle case, considered here, it is normalized to N, and therefore |ψbg|2 = Nbg in order to give the initial den- sity profile of Eq. (4). The time-dependent solution to the Schr¨ odinger equation for this problem can be derived analytically and is given by [25] Ψ(x, t) = ψbg   1 + βσp σ2 + iℏt/m e−x2/2(σ2+iℏt/m) ! , (13) or alternatively Ψ(x, t)= ψbg[1+B(x, t)eiφ(x,t)] with am- plitude B(x, t) ≡ βσ [σ4 + ℏ2t2/m2]1/4 e−x2σ2/2[σ4+ℏ2t2/m2] (14) and phase φ(x, t) ≡ ℏtx2 2m[σ4 + ℏ2t2/m2] − 1 2 arctan \u0012 ℏ2t2 m2σ4 \u0013 . (15)4 This allows for the density of the gas to be written as ρ(x, t) = |Ψ(x, t)|2 = Nbg[1 + B(x, t)2 + 2B(x, t) cosφ(x, t)]/L, (16) where it becomes clear from the form of Eq. (16) that the oscillations which arise dynamically are the result of quantum mechanical interference. Furthermore, one can determine that the current den- sity in the ideal gas regime, j(x, t) = ρ(x, t)v(x, t), is given by (see Appendix A) j(x, t) =ℏNbg mL {B′(x, t) sinφ(x, t) + φ′(x, t) \u0002 B(x, t) cosφ(x, t) + B2(x, t) \u0003\t , (17) with B′(x, t) = ∂B(x, t) ∂x = − βσ3x (σ4 + ℏ2t2/m2)5/4 × e−x2σ2/2(σ4+ℏ2t2/m2), (18) φ′(x, t) = ∂φ(x, t) ∂x = ℏtx m (σ4 + ℏ2t2/m2). (19) In Fig. 1 (a) we show the dynamics of the dimension- less density ¯ρ(ξ, τ) = ρ(x, t)L (where the dimensionless position and time are defined, respectively, according to ξ = x/L and τ = tℏ/mL2) and in (b) the dimensionless current ¯j(ξ, τ) = j(x, t) mL2/ℏ of a shock wave generated in an ideal Bose gas, with γbg = 0 and a total of N = 50 particles (where N = 50 is chosen for comparison with other interaction regimes; see below). For clarity of de- tail and ease of interpretation we hereafter plot only the region ξ = x/L ≥ 0 at select dimensionless times τ, with the recognition that the particle number density ρ(x, t) is an even function ρ(x, t)= ρ(−x, t), whereas the current density j(x, t) is an odd function j(x, t)= −j(−x, t) of x. Accordingly, in Fig. 1 (c) we re-plot some of the data in (a) and (b), where it becomes more clear that the oscilla- tions, which arise in the shock wave train, have a period that is on the order of ∼2σ [25]. We also observe a tell- tail sign that reinforces our interpretation from Ref. [25] of the shock wave oscillations as a self-interference pat- tern: adjacent oscillations in the current density are com- mensurate with those in the particle number density and the oscillations alternate in sign. This indicates there are local counter-flows throughout the fluid, where for one in- terference fringe (any of the local density peaks) there is particle flow to the right, and for the next (any of the lo- cal density troughs) there is particle flow to the left, and so on. This implies there are counter-propagating fluid flows, which is similar to the mechanism of matter-wave interference occurring in counter-propagating BECs set by Bragg pulses (see, e.g., [53]). Overall, however, there is a net positive flow of particles to the right for ξ ≥ 0, indicative of the expansion of the initial bump into the 0 0 0.5 50 100 0.001 0 150 0.002 -0.5 -2000 0 0.5 0 0.001 0 2000 0.002 -0.5 0 0.1 0.2 0.3 0.4 0.5 0 1000 2000 0 50 100 150 0 0.1 0.2 0.3 0.4 0.5 0 1000 2000 0 50 100 150 FIG. 1. Shock waves generated (from an initial density bump) in an ideal and weakly-interacting 1D Bose gas at zero-temperature (T = 0). The gas begins with density pro- file ρ = Nbg(1 +βe−x2/2σ2 )2/L and evolves in a box of length L with periodic boundary conditions. (a) The dimensionless particle number density ¯ρ = ρL for an ideal gas ( γbg = 0) of N = 50 particles, with β = 1 and σ/L = 0 .02 (leading to Nbg ≃ 44.03), at dimensionless times τ = tℏ/mL2. The dimensionless position ξ is scaled by the box length L so that ξ = x/L. (b) The dimensionless current density ¯j = jmL2/ℏ corresponding to the same situation and times as in (a). (c) Both the dimensionless number density (dotted lines) and cur- rent density (solid lines) for select times of (a) and (b). While (a) and (b) provide a clear visualization of the shock wave sce- nario, hereafter we display results as in (c) to reduce space and provide clarity of detail with the recognition that the number density is an even function and current density an odd function. (d) The same as in (c) but now for a weakly interacting Bose gas with γbg = 0.1.5 0 0.1 0.2 0.3 0.4 0.5 -2 0 2 4 6 8 0 5 10 0 0.5 1 1.5 0 10 20 30 0 0.5 1 1.5 0 0.5 1 1.5 -15 -10 -5 0 5 0.5 1 1.5 FIG. 2. A vacuum point scenario in an ideal 1D Bose gas of N = 50 particles at T = 0, for β = 5 and σ/L = 0.01. Here, one of the number density fringes touches zero density during the evolution, due to a sufficiently large initial bump height β. (a) The dimensionless number density (dotted lines) and current density (solid lines) at different times τ. The appear- ance of a vacuum point in the number density is marked by the disappearance (or reappearance) of sign-alternation in the current density at that location. We refer to this location as ξ1 and it is denoted by the arrowheads in (a). (b) Magnitude of the minimum in the trailing number density fringe over the course of the dynamics. This fringe touches zero twice dur- ing the dynamics and the times at which this occurs can be determined from Eq. (20). (c) Magnitude of the minimum in the trailing current density fringe over the course of the dynamics. The vertical dashed lines in (b) and (c) show the times at which a vacuum point occurs at the trailing interfer- ence fringe. The horizontal dashed line in (c) helps guide the eye to highlight where ¯j(ξ1, τ) changes sign, i.e., at the same time as ¯ρ(ξ1, τ) = 0. background. In Fig. 1 (d) we move into the weakly-interacting regime (with γbg = 0 .1 and the same N = 50 as be- fore), where the solution is given by the numerical in- tegration of the mean-field GPE or superfluid hydrody- namic equations. The dynamics of the dissolution of the initial density bump here is similar to the ideal Bose gas case; however, we point out that the interactions cause an overall increase in the strength of the currents pro- duced and that the initial bump begins to separate into two distinct traveling wave packets in this case (one to the left, not shown, and one to the right, shown), indi- cated by the decrease in the density at x = 0 closer to the background level. This is consistent with the expec- tation that the now non-zero pressure P = g1Dρ2 0/2 is helping to push particles out of any local density bumps. In addition, we note that the trailing edge of the shock wave now travels at the speed of sound in the background cbg = p g1Dρbg/m (whereas in the T = 0 ideal Bose gas, the speed of sound was zero). We next investigate situations where the initial bump height β is comparatively large ( β ≫ 1), so that the initial peak density ρ(0, 0) is much larger than the back- ground density ρbg. For the sake of analytical insight we present results again in the ideal Bose gas regime and note that the situation remains very similar for weak in- teractions. In the case of large β, we investigate a so- called vacuum point scenario [54] where the density of an interference fringe touches zero during evolution and the fluid velocity becomes undefined at that location. De- spite the singularity that occurs at the vacuum point in the fluid velocity, the current density (or fluid momen- tum) at this location remains finite and is well defined. The vacuum point scenario is well known to the hydro- dynamic community in the weakly-dispersive Thomas- Fermi regime of a weakly-interacting 1D Bose gas [10, 15, 23, 27, 44, 45, 54], yet we are unaware of its identi- fication in the non-interacting, strongly-dispersive ideal Bose gas regime. Due to the analytics we have readily available here, it is possible to determine the minimum or critical height βcr required for the vacuum point to occur at time tvac during evolution, and this depends on the initial bump width σ. By considering the lower envelope of the density (16) one finds that the critical height can be determined using (see Appendix B), β(n) cr = 1 σ \u0000 σ4 + ℏ2t2 vac/m2\u00011/4 × e mσ2 ℏtvac \u0014 π(2n−1)+ 1 2 arctan \u0012 ℏ2t2 vac m2σ4 \u0013\u0015 (20) where n = 1, 2, 3, ...denotes the fringe number (increas- ing from the trailing to leading edge) that will become a vacuum point, and tvac the time(s) that this will occur. As opposed to the case for a step-like initial density profile, which is most often considered in the literature [10, 23, 54], for the initial (raised) Gaussian squared pro- file that we consider here it is possible for the vacuum point to occur more than once at a given interference fringe during the system’s evolution. The times at which the vacuum point occurs are marked by the disappear- ance (or reappearance) of sign-alternation in the corre- sponding fringe of the current densityj(x, t). Figure 2 (a) provides snapshots of the density and current for an ex-6 ample where a vacuum point occurs twice at the trailing (n = 1) interference fringe. Figs. 2 (b) and (c) trace the time evolution of the corresponding heights at the min- imum of the trailing interference fringe, whose spatial location we label with ξ1(τ). For the parameters used here, β = 5 and σ/L = 0 .01, Eq. (20) predicts that the vacuum point will occur at the trailing fringe at di- mensionless times τvac = tvacℏ/mL2 ≃ 0.479 × 10−3 and 1.45 × 10−3, which are marked in Figs 2 (b) and (c) with vertical dashed lines. At these times, the current at ξ1(τ) changes sign, providing a transient regime between the occurrences of the vacuum points where the current is no longer sign-alternating for that interference fringe. B. Weakly interacting Bose gas in the Thomas-Fermi limit Considering now a much larger number of particles in the system ( N = 2000) at γbg = 0.01 brings us into the Thomas-Fermi regime of the weakly interacting 1D Bose gas (i.e., where the mean-field interaction energy of the particles dominates their kinetic energy). In this regime, we start by exploring the shock wave densities and cur- rents that arise in the previously examined situation of Fig. 1 (c) from Ref. [25] with β = 1, and then inves- tigate those that emerge in a weakly-dispersive vacuum point scenario for β = 5. In both cases we complement the mean-field solution with results from the truncated Wigner approach (which we initialize as the Bogoliubov ground-state of a suitably chosen trapping potential, see Appendix C) and the classical-field stochastic projected Gross-Pitaevskii equation (SPGPE, see Appendix D). These alternative approaches go beyond the mean-field GPE description and allow us to discuss, respectively, the effect of quantum fluctuations and finite temperature in these scenarios. In the Thomas-Fermi regime, the pressure P = g1Dρ2/2 of the gas is greater than in the nearly-ideal sit- uation of 1 (d) and this causes the left (not shown) and right moving wave trains to separate from each other quickly, see Figs. 3 (a) and (d) which are for a bump height of β = 1. Moreover, the oscillations in this case have a period on the order of the healing length in the background lh = ℏ/√mgρbg = L/Nbg √γbg [25], which is now smaller than σ and is the shortest relevant length- scale in the system. Here, the current affords a similar interpretation as in the preceding section, and the oscilla- tions are sign-alternating and therefore produce counter- flows between adjacent peaks and troughs. Again, such an insight gained from the current density provides a re- inforcement of the conclusion that these oscillations can be interpreted as interference fringes across the phase co- herent gas. Furthermore, Figs. 3 (b) and 3 (e), which are for a bump height of β = 5, provide similarly the density and current for a scenario during which a vacuum point oc- curs. In this situation, the location of the vacuum point drifts (from left to right) into the middle of the shock wave during evolution and multiple interference fringes touch zero density throughout the mean-field dynamics. Moreover, for the initial Gaussian squared profile we con- sider, the vacuum point eventually drifts back out of the shock front (from right to left) at longer times, and by the end of the simulation shown here (at τ = 0.0003), each fringe minimum that at some point became a vacuum point has re-touched zero density again. This behavior is captured in Figs. 3 (c) and (f) which trace the min- imum height, according to the mean-field prediction, of the trailing fringe in the density and current respectively. We see that this fringe minimum quickly becomes a vac- uum point and returns to zero density again late in the dynamics. The times at which this interference fringe becomes a vacuum point can be determined numerically and are approximatelyτvac ≈ 0.30×10−4 and 2.67×10−4. To the best of our knowledge, this is the first time such behavior of the vacuum point has been reported. When one includes the effect of quantum fluctuations using the truncated Wigner approach (see yellow lines in Fig. 3), the visibility of the interference pattern (i.e., the amplitude of the density oscillations), along with the strength of the currents produced, is diminished com- pared with the mean-field GPE result. This reduction can be credited to shot-to-shot fluctuations in the posi- tion of the oscillations around their mean [25]. Moreover, in Figs. 3 (b) and (e) we see that the quantum fluctua- tions prevent an actual vacuum point from occurring, i.e., the mean density never touches zero. Hence we conclude that the vacuum point observed in the GPE is a strictly mean-field effect, i.e., it is an artifact of the mean-field approximation and it disappears once the effects of quan- tum fluctuations are taken into account [55]. This was conjectured in Ref. [54]; “in the vicinity of the vacuum point the NLS-approximation can fail and a more com- plex model should be evolved”. We have thus confirmed this conjecture here through the simulation of both the truncated Wigner and SPGPE models. The effect of thermal fluctuations, that we simulated for finite temperature systems through the SPGPE ap- proach, is similar to that of quantum fluctuations; see black dashed lines in Fig. 3, which are predictions of the SPGPE for dimensionless temperatures of T = T/Td = 0.05 and 0.5 (see figure caption for details). The thermal fluctuations cause such a reduction in the visibility of the interference contrast, or in the strength of the currents which are produced, that the oscillatory wave train disap- pears entirely. As such, thermal fluctuations also prevent the vacuum point from occurring in the mean density, see Fig. 3 (b). We point out, however, that the vacuum point may still occur in individual stochastic trajectories, which themselves can be thought of as representing the outcomes of individual experimental runs [35].7 0 0.1 0.2 0.3 0.4 0.5 0 1000 2000 3000 4000 5000 6000 7000 0 0.1 0.2 0.3 0.4 0.5 -2 0 2 4 6 8 10 0 0.1 0.2 0.3 0.4 0.5 0 2000 4000 6000 8000 0 0.1 0.2 0.3 0.4 0.5 0 1 2 3 4 0 1 2 3 0 200 400 600 800 1000 1200 2 2.5 3 0 50 0 1 2 3 -5 0 5 10 15 20 FIG. 3. Shock waves generated (from an initial density bump) in the Thomas-Fermi regime of a 1D Bose gas. Blue (shades of gray, at different times) lines indicate the mean-field solution, yellow (light gray) dotted lines indicate the results of the truncated Wigner approach (TWA, see Appendix C for details) and black dashed lines provide the prediction of the SPGPE for which the initial state is characterized by a dimensionless temperature of T = T/Td, where Td = ℏ2ρ2 bg/2mkB is the temperature of quantum degeneracy at the background density (see Appendix D for further details). Panels (a) and (d) show the particle number density and current density, respectively, for the situation with β = 1 and σ/L = 0.02 explored previously in Ref. [25]; the SPGPE results shown in these panels are for T = 0.05. Panels (b) and (e) show the same, but for a situation with β = 5 and σ/L = 0.01 which causes a vacuum point to occur in the mean-field solution; the SPGPE results in this case are for T = 0.5. In (b) the height of the initial profile is cropped so as to display more detail in the density at subsequent times. Panels (c) and (f) trace the respective height of the trailing interference fringe over the course of the mean-field GPE dynamics, which remains initially at the background value until the wave breaks and a dispersive shock occurs. The vertical dashed lines denote the times τvac ≈ 0.30 × 10−4 and 2.67 × 10−4 that this fringe becomes a vacuum point (determined numerically), with the horizontal dashed line in (f) used to guide the eye. All TWA and SPGPE results presented in this work are ensemble averages over 100,000 stochastic trajectories. C. Strongly interacting and Tonks-Girardeau regimes Here, we re-consider the scenario initiated from the same density profile as in Fig. 2 (a) for N = 50 particles and β = 5, but we now sweep the interaction strength into the weakly interacting γbg ≪ 1, intermediate γbg ∼ 1, strongly γbg > 1, and Tonks-Girardeau (TG)γbg → ∞ regimes. The density and current in these situations are shown in Figs. 4 (a) through (d). Here, the solutions for γbg = 0 .1, γbg = 1, and γbg = 10 are obtained using matrix product state methods (similarly to Ref. [25], see Appendix VI of that work), whereas for γbg → ∞we use Fermi-Bose mapping [56, 57] and exact diagonalization of the free fermions problem (see Appendix E). It has already been identified in Ref. [25], and can be seen again here, that the interference contrast in the den- sity disappears as the interaction strength is increased. This is due to a decrease in coherence length of the sys- tem, which reduces down to the size of the mean inter- particle separation 1 /ρbg in the TG limit. In this limit the initial state itself is not smooth but rather contains small-scale Friedel oscillations [58] whose period is on the order of 1/ρbg, and which are not visible initially on the scale plotted in Fig. 4 (d). During evolution, the oscilla-8 0 0.1 0.2 0.3 0.4 0.5 0 2 4 0 200 400 0 0.1 0.2 0.3 0.4 0.5 0 2 4 0 200 400 0 0.1 0.2 0.3 0.4 0.5 0 10 20 0 200 400 0 0.1 0.2 0.3 0.4 0.5 0 10 20 0 200 400 FIG. 4. Shock waves generated (from an initial density bump) in a 1D Bose gas at weak (γbg = 0.1), intermediate (γbg = 1), strong (γbg = 10), and infinitely strong ( γbg → ∞) interac- tions. (a) through (d) show the particle number densities and currents for the same initial situation as in Fig. 2 (a) i.e. for N = 50, β = 5, and σ/L = 0 .01, except now for non-zero interaction strengths. The initial peak density in these plots is approximately ρ(0, τ= 0) ≃ 1060, however, the vertical axis is cropped here to make the details more visible in the time-evolved densities at later times. tions that appear towards the front of the shock are sim- ply deformations of the pre-existing Friedel oscillations, whereas the irregular peaks that develop dynamically to- ward the rear of the shock can be interpreted as Friedel- type oscillations between different branches of the Fermi momentum (in phase-space) [59]. In either case, with the increase of interaction strength, these density modu- lations shift towards occurring predominantly above the 0 0.1 0.2 0.3 0.4 0.5 0 500 1000 0 10 20 30 40 FIG. 5. Shock waves generated (from an initial density bump) in a 1D Bose gas in the TG limit ( γbg → ∞), for N = 4, β = 5, and σ/L = 0 .005. In this scenario, the width of initial density bump σ is much smaller than the period of pre- existing Friedel oscillations in the initial density profile. The initial peak density here is approximately ρ(0, τ= 0) ≃ 70, however, the vertical axis is cropped to make the details more visible in the time-evolved density at later times. level of the background density rather than around it, which is unlike the density oscillations in the dispersive shock-wave interference fringes. Through Fig. 4 (a) to (c), which are for N = 50, we show that the crossover between interference and the lack thereof is easily identi- fiable by examining the oscillations in the current density, which lose their sign-alternating and large oscillatory na- ture at higher interaction strengths, indicating that there are no longer any counter-flows, or interference, in the fluid. All these features imply that any vacuum point that may have existed for weak or zero interaction under a certain combination of parameters would cease to exist as the interaction strength is increased. In the TG limit for N = 4 (rather than N = 50), when the initial bump is much narrower than the mean inter- particle separation ( σ ≪ 1/ρbg), see Fig. 5, one again observes the emergence of interference fringes. They are, however, contained within a single Friedel oscillation pe- riod (which are visible in the initial state of this sce- nario) over which the gas is coherent. Indeed, the phase coherence length in the TG regime is on the order of the mean interparticle separation, 1 /ρbg [60], which is also the characteristic period of Friedel oscillations [58]. As such, an initial bump with a width that is much smaller than 1 /ρbg expands into a locally phase coher- ent background—hence the interference fringes that we see in this regime. The current density in Fig. 5 at times τ = 1.75 ×10−4 and τ = 7 ×10−4 shows clearly this con- tainment of sign-alternating counter-flows to the order of the mean inter-particle separation. We thus conclude again that the interference fringes are phenomena pro- duced when the local phase coherence length of the gas is much larger than the width of the initial bump.9 IV. DENSITY DIP SCENARIOS Shifting our focus to situations where β <0, the initial density profile now contains a dip, which the background density subsequently evolves to fill. Here we consider scenarios that mimic their density bump counterpart and choose whole total particle numbers N which provide the closest value of Nbg to the respective density bump situ- ation. A. From the ideal Bose gas to the strongly interacting regime We first explore the effect of inter-particle interactions for relatively small atom numbers ( N = 42) as we vary the interaction strength from the ideal Bose gas regime at T = 0 through to the TG regime of infinitely strong inter- actions. The resulting snapshots of the density and cur- rent in these examples are shown in Figs. 6 (a) through (c). While density dip scenarios are usually well known for producing trains of gray solitons [1, 2, 8, 9, 17, 27], here in the ideal Bose gas regime (which does not support solitons) we see that this situation results purely in self- interference. Similar to the equivalent bump scenario of Fig. 1 (c), the current remains sign-alternating with counter-flows across the fluid, yet the sign of these os- cillations themselves is flipped, indicating a change in direction of net particle flow which is now towards the center of the system such that the background is filling in the initial density dip. In the regime of weak interactions the physical picture remains much the same as in the equivalent bump case Fig. 1 (d), where the oscillations in the density are still the result of quantum interference, and the initial dip, as it begins to fill, splits into two counter-propagating oscil- latory waves due to a non-zero pressure P = g1Dρ2 0/2. As one reaches the infinitely-interacting TG limit, we observe small-scale Friedel oscillations in the initial state of this scenario [58] (see Fig. 4 (c)), but the large density ripples present for weak interactions disappear. In these density dip situations, the actual shock itself occurs on the inside (or left) edge of the propagating wave, where in this TG limit there emerges some intriguing large scale (wavelength) oscillations that span several Friedel oscilla- tions, and whose corresponding oscillations in the current show weak sign-alternating counter-flows. We note that for large particle numbers N density rip- ples, that we refer to as Bettelheim-Glazman oscillations (see Appendix F and [61–63]), exist across the shock front; these larger wavelength oscillations span numerous Friedel oscillations and their frequency is chirped toward the center of the wave-packet. The oscillations which emerge here at small N, how- ever, do not appear to posses this same style of chirp and it does not seem appropriate to identify them as Bettelheim-Glazman oscillations, which are known to 0 0.1 0.2 0.3 0.4 0.5 -400 -200 0 200 400 0 20 40 60 0 0.1 0.2 0.3 0.4 0.5 -400 -200 0 200 400 0 20 40 60 0 0.1 0.2 0.3 0.4 0.5 -1000 -500 0 500 0 20 40 60 0 0.1 0.2 0.3 0.4 0.5 -100 -50 0 50 100 0 2 4 6 FIG. 6. Shock waves generated (from an initial density dip) in a zero-temperature ( T = 0) 1D Bose gas for a range of interaction strengths. The particle number densities and cur- rent densities are shown for: (a) N = 42, γbg = 0 [the density dip equivalent of Fig. 1 (c)]; (b) N = 42, γbg = 0.1 [the den- sity dip equivalent of Fig. 1 (d)]; (c) N = 42, γbg → ∞[the density dip equivalent of Fig. 4 (c)]; and (d) N = 4, γbg → ∞ [the the density dip equivalent of Fig. 4 (d)]. only appear in the limit of large N [61, 62]. In fact, their true origin is quite unclear; they emerge dynami- cally on the opposite side of the shock wave compared to the usual interference pattern generated in the weakly- interacting regime, and they posses a much more periodic nature compared with the usual density peaks that arise as a result of Friedel-type oscillations between different Fermi-momentum branches [59]. Nevertheless, determin- ing their true cause is beyond the scope of this paper and we leave the identification of their origin for future work. For even smaller N, however, where the initial bump10 width is smaller than the mean inter-particle separa- tion, as in Figs. 6 (d), we again observe interference fringes that are contained within single Friedel oscilla- tions, where the sign of the alternating currents is sim- ilarly flipped compared with the equivalent bump case from Fig. 4. B. Shedding of gray solitons in the weakly interacting Bose gas in the Thomas-Fermi limit Finally, we explore the fate of an initial density dip in the weakly-dispersive Thomas-Fermi regime, which is achieved for much larger values of N than in previous subsection. It is in this limit that the generation of gray soliton trains is possible and we identify the role that the initial dip width plays in the production of these solitons. We first consider the mean-field GPE solution and the insights it has to offer, before discussing the effects of quantum and thermal fluctuations in these scenarios. For small widths, see Fig. 7 (a), where σ/L = 0.005 is becoming comparable to the healing length lh ≃ 0.0057, two gray solitons are shed from the initial density profile (one propagating in each direction) and a clearly identi- fiable interference chirp that precedes them in time is generated, which expands away from the solitons and reduces in visibility. These observations are reinforced by the current density in Fig. 7 (d) which shows sign- alternating currents in the interference region and a clear peak marking the location of the rightward moving soli- ton, whose propagation is supported due to particle flow towards the left. As the dip width is increased and one goes deeper into the Thomas-Fermi regime, the interference pattern di- minishes and the number of generated solitons grows. For a width σ/L = 0 .02, see Fig. 7 (b) [equivalent to the bump case of Fig. 3 (a)], the initial dip sheds a total of six solitons with three traveling in each direction, and minimal interference is generated in front of the leading solitons, which can be confirmed from the current den- sity in Fig. 7 (e). Due to their increasingly (to the right) shallower depths and therefore increasingly faster prop- agation velocities [64], the solitons separate from each other as the system continues to evolve. With twice larger width σ/L = 0 .04, see Fig. 7 (c), five or more solitons are shed on either side of the ini- tial profile and interference fringes no longer appear in front of the soliton train. We note then that one can use the initial dip width σ to precisely control and gen- erate specific numbers of solitons on demand. On top of this, a sweep from interference fringes to controllable soli- ton generation can be achieved via any means that shift the system away from the strongly-dispersive ideal gas regime into the weakly-dispersive Thomas-Fermi limit, i.e., any change that decreases the healing length lh = ℏ/√mg1Dρbg = L/Nbg √γbg compared with σ. In par- ticular, with a deeper initial profile (controlled by β) or increased Nbg, or γbg, it is possible to generate longer trains of numerous well separated solitons. These find- ings are consistent with results presented in Ref. [27] that indicate the number of solitons generated from an initial density dip profile depends only on the initial distribu- tion of the local speed of sound c(x, 0) = p g1Dρ(x, 0)/m. For clarity, in Appendix G we provide details around the identification of solitons in the mean-field GPE solution. When quantum fluctuations are introduced (see open circles in each panel of Fig. 7), they act to weaken or de- stroy the large-amplitude features present in the mean- field solution. For the interference present in Figs. 7 (a) and (d) the situation is the same as for the density bump; the interference contrast is reduced due to a reduction in the strength of sign-alternating counter-flows in the fluid, which itself is due to shot-to-shot fluctuations in the po- sition of oscillations about their mean. This same shot- to-shot averaging has a somewhat more dramatic effect on the solitons—namely, on reducing the amplitude and broadening the soliton width—to the extent that, with- out the mean-field solution, it would be very difficult to identify that the density dips in the truncated Wigner result were remnants of solitons from single shots. Fur- thermore, the effect of thermal fluctuations on these fea- tures, for an initially finite-temperature system, is similar yet even more pronounced (see black dashed lines in each panel of Fig. 7). So much so that for the situations of Fig. 7 (b) and (e), and (c) and (f), there remains little trace of any local density dip features, and the SPGPE result provides almost a smoothed average of the trun- cated Wigner dips or oscillations. V. SUMMARY In summary, we have explored quantum shock wave scenarios in 1D Bose gases for a broad range of initial den- sity perturbations and interaction strengths, including those far outside the commonly studied weakly-dispersive Thomas-Fermi regime. Through the use of the current density j(x, t), in addition to particle number density ρ(x, t), we have consolidated further the interpretation and understanding of quantum interference and soliton generation in these systems. Moreover, the ubiquitous nature of the shocks that we consider allows other fields of physics to obtain a useful perspective on the interest- ing and rich dynamics of these scenarios. For Bose gases in initial density bump situations, the dispersive shock waves are the result of quantum interfer- ence and their visibility is reduced as the phase coherence length of the gas decreases, i.e., for increasing interaction strengths or the introduction of quantum or thermal fluc- tuations [25]. Here, we have shown that this reduced vis-11 0 0.1 0.2 0.3 0.4 0.5 0 500 1000 1500 2000 0 0.1 0.2 0.3 0.4 0.5 -10 -5 0 5 0 0.1 0.2 0.3 0.4 0.5 0 500 1000 1500 2000 0 0.1 0.2 0.3 0.4 0.5 -10 -5 0 5 0 0.1 0.2 0.3 0.4 0.5 0 500 1000 1500 2000 0 0.1 0.2 0.3 0.4 0.5 -10 -5 0 5 FIG. 7. Particle number densities (top row) and currents (bottom row) that form from an initial density dip in a 1D Bose gas in the Thomas-Fermi regime. Solid lines represent the mean-field solution, open circles provide the result of the truncated Wigner approach (see Appendix C for details) and black dashed lines indicate the prediction of the SPGPE at a dimensionless temperature of T = T/Td = 0.01 where Td = ℏ2ρ2 bg/2mkB is the temperature of quantum degeneracy at the background density (see Appendix D for further details). The initial bump width increases from left to right where one generates more solitons for larger σ. In each case the total number of particles N is chosen to be a whole number that gives the closest Nbg to that of Fig. 3 (a); Nbg ≃ 1761 which leads to lh/L ≃ 0.0057. ibility is accompanied by reduced sign-alternating cur- rents across the fluid. Furthermore, we have identified the possibility of vacuum points in the strongly-dispersive ideal Bose gas regime and shown that such vacuum points can occur more than once at a given interference fringe, where the times that they occur are marked by the dis- appearance or reappearance of sign-alternating currents at that location. In the weakly-dispersive Thomas-Fermi limit of a weakly interacting 1D Bose gas we have shown that quantum and thermal fluctuations reduce the visibil- ity of interference fringes and prevent an actual vacuum point from occurring in the mean density. In density dip situations, the generation of both in- terference and mean-field gray solitons is possible in the weakly interacting regime whereγbg < 1. We have shown that the number of solitons produced in the Thomas- Fermi limit can be effectively controlled through the width and depth of the initial density dip. We have also shown that quantum and thermal fluctuations each di- minish the visibility of the aforementioned phenomena so that the remnant (in the average density) of the mean- field soliton core becomes broader and shallower. The generation of soliton trains also disappears as the interaction strength of the gas is increased into the strongly interacting regime, where, for large par- ticle numbers, we instead observe the emergence of Bettelheim-Glazman oscillations [61–63] on top of a broad density depletion, similar to those predicted for a density bump scenario. We note that these oscilla- tions have a different origin to Friedel oscillations, as they occur on different (larger) lengthscales. In the ex- amples presented here (see Appendix F), which were for N > 800, we have been able to discriminate between the Friedel oscillations, which have a period of 1 /N, and the Bettelheim-Glazman density ripples, which have a larger oscillation period. This was not possible in the prior work of Ref. [25], which treated a much smaller number of particles ( N = 50) in the Tonks-Girardeau regime. We also note that Bettelheim-Glazman oscilla- tions are chirped towards the center of the overall density depletion (which propagates away from the origin). This is different from the interference oscillations that occur in the weakly-interacting regime, which are chirped to- wards the front of the shock wave envelope. Overall, the mechanism behind the formation of Bettelheim-Glazman oscillations is yet to be understood, and we expect that such oscillations should be present also at very large but finite interaction strengths, i.e., away from the strictly12 TG limit of infinitely strong interactions. ACKNOWLEDGMENTS K. V. K. acknowledges support by the Australian Research Council Discovery Project Grant No. DP190101515. Appendix A: Current density for an ideal Bose gas Given the time-dependent wave function, Eq. (13), for the ideal Bose gas, which is of the form Ψ( x, t)= ψbg[1+ B(x, t)eiφ(x,t)], the current density j(x, t) = ρ(x, t)v(x, t) = ρ(x, t) ℏ m ∂S(x, t) ∂x (A1) can be determined using the phase S(x, t) = 1 2i ln \u0012 Ψ(x, t) Ψ∗(x, t) \u0013 = 1 2i ln \u0012 1 + B(x, t)eiφ(x,t) 1 + B(x, t)e−iφ(x,t) \u0013 . (A2) The velocity is thus given by v(x, t) = ℏ m ∂S(x, t) ∂x = ℏ m \u0014 B′(x, t) sinφ(x, t) 1 + 2B(x, t) cosφ(x, t) + B2(x, t) + φ′(x, t) \u0002 B(x, t) cosφ(x, t) + B2(x, t) \u0003 1 + 2B(x, t) cosφ(x, t) + B2(x, t) # , (A3) leading to Eq. (17) j(x, t) = ρ(x, t)v(x, t) = ℏNbg mL {B′(x, t) sinφ(x, t) + φ′(x, t) \u0002 B(x, t) cosφ(x, t) + B2(x, t) \u0003\t (A4) with B′(x, t) and φ′(x, t) defined in Eqs. (18) and (19). Appendix B: Critical height for a vacuum point to occur in the ideal Bose gas Here we wish to identify, for a given bump width σ, the necessary height βcr required for a vacuum point to occur in the mean-field GPE approximation, i.e., for the minimum of an interference fringe in the particle number density to touch zero at a given time tvac. The location of each fringe minimum is difficult to find generally, how- ever, we can determine the location of a fringe if it were to touch zero density by considering the function which envelopes the density from below. This is given by ρenv−(x, t) = Nbg L \u0002 1 − 2B(x, t) + B2(x, t) \u0003 = Nbg L [1 − B(x, t)]2 , (B1) where one simply sets cos φ(x, t)= −1 in the actual den- sity [Eq. (16) ], ρ(x, t) = Nbg L \u0002 1 + 2B(x, t) cosφ(x, t) + B2(x, t) \u0003 . (B2) In general, the density oscillations do not necessarily touch the enveloping function ρenv−(x, t) at exactly the location of the fringe minimums. This means that the fringe minimums do not correspond exactly with the min- imum locations of cos φ(x, t). However, when a vacuum point occurs, the density at that location and time is ex- actly ρ(xvac, tvac) = ρenv−(xvac, tvac) = 0. Hence, at the vacuum point itself, it must be true that the fringe min- imum occurs at exactly the corresponding minimum of cos φ(x, t). This allows one to express the location of a vacuum point using the following steps, −1 = cos φ(xvac, tvac), π(2n − 1) = φ(xvac, tvac), π(2n − 1) = ℏtvacx2 vac 2m (σ4 + ℏ2t2vac/m2) − 1 2 atan \u0012ℏ2t2 vac m2σ4 \u0013 , x2 vac = \u0014 π(2n − 1) + 1 2 atan \u0012ℏ2t2 vac m2σ4 \u0013\u0015 × 2m \u0000 σ4 + ℏ2t2 vac/m2\u0001 ℏtvac , (B3) where n = 1, 2, 3, ...denotes the fringe number (increas- ing from the trailing to leading edge) of the shock wave. Additionally, the requirement ρ(xvac, tvac) = ρenv−(xvac, tvac) = 0 implies that one has ρenv−(xvac, tvac) = Nbg L [1 − B(xvac, tvac)]2 = 0 (B4) at the location of the vacuum point [according to Eq. (B1)] and therefore, B(xvac, tvac) = 1. (B5) This then provides a means of determining the value β necessary to generate a vacuum point. One uses the re- quirement that 1 = B(xvac, tvac) = βσ (σ4 + ℏ2t2vac/m2)1/4 e−x2 vacσ2/2(σ4+ℏ2t2 vac/m2), (B6) and substitutes in the vacuum point location (B3). Rear-13 ranging the resulting expression then allows one to find the critical value of β required for a vacuum point to occur at fringe n at time tvac; β(n) cr = 1 σ \u0000 σ4 + ℏ2t2 vac/m2\u00011/4 × e mσ2 ℏtvac \u0014 π(2n−1)+ 1 2 arctan \u0012 ℏ2t2 vac m2σ4 \u0013\u0015 . (B7) This is the formula given in the main text as Eq. (20), . Appendix C: The truncated Wigner approach For the results presented in this work that incorporate quantum fluctuations, the truncated Wigner approach (TWA) [35, 65, 66] is used with Bogoliubov initial condi- tions. Specifically, the 1D version of the Bogoliubov– de Gennes (BdG) equations for the mode amplitudes uj(x, 0) and vj(x, 0) [50, 67–69] µΨ0 = \u0014−ℏ2 2m ∂2 ∂x2 + V + g1D |Ψ0|2 \u0015 Ψ0, (C1) Ejuj = \u0012−ℏ2 2m ∂2 ∂x2 + V − µ + 2g1D|Ψ0|2 \u0013 uj + g1DΨ2 0vj, (C2) −Ejvj = \u0012−ℏ2 2m ∂2 ∂x2 + V − µ + 2g1D|Ψ0|2 \u0013 vj + g1D(Ψ∗ 0)2uj, (C3) (where µ is the chemical potential of the condensate mode Ψ0, and Ej the energy of the jth Bogoliubov mode) are solved simultaneously using the trapping potential [25] V (x) = ℏ2β 2mσ4 \u0000 x2 − σ2\u0001h ex2/2σ2 + β i−1 − g1DNbg L h 1 + βe−x2/2σ2 i2 (C4) in order to obtain the initial state of the system, which includes both the condensate (j = 0) and non-condensate (j = 1, 2, 3, ...) modes. We note that the BdG equations admit both positive and negative energy solutions. Specifically, for each so- lution with positive energy, Ej, there is an equivalent solution with negative energy, −Ej, that provides an equivalent physical description. In practice then, one need only work with, say, the positive solutions. Since the number of particles in the non-condensed Bogoli- ubov field is given by Nnc = ´ ⟨δ ˆψ†(x, 0)δ ˆψ(x, 0)⟩dx =´ P j≥1 |vj(x, 0)|2 dx, the total number of particles in the system can be computed using N = N0 + Nnc =´ (|Ψ0(x, 0)|2 + P j≥1 |vj(x, 0)|2) dx. In practice, the sum over excited states used to compute Nnc must be trun- cated to some finite number of modes M. The trapping potential (C4) was originally derived in Appendix II of Ref. [25], but we now allow β < 0 in order to generate initial density dips as well. This trap produces exactly the desired density ρ0(x, 0) = Nbg[1 + βe−x2/2σ2 ]2/L as the ground state solution to the GPE (C1). Therefore, the population of the ex- cited Bogoliubov modes does cause small deviations from this desired initial density. Provided the total number of non-condensed atoms Nnc remains sufficiently small com- pared with the total number of condensed atoms N0, this deviation should be negligible. This is the case for the scenarios we examine in this work, i.e., Nnc ≪ N0. Following diagonalization of the BdG equations, each stochastic trajectory is then initialized according to the Bogoliubov prescription [35, 65, 66] ψW (x, 0) = Ψ0(x, 0) + MX j=1 \u0002 ηjuj(x) + η∗ j v∗ j (x) \u0003 , (C5) and evolved according to the time-dependent GPE (6). Here, ηj is a complex Gaussian noise term with zero mean, ⟨ηj⟩stoch = 0, and variance ⟨η∗ j ηk⟩stoch = 1 2 δjk. Moreover, Ψ 0(x, 0) is the ground state of the time- independent GPE (C1) which we renormalize to the oc- cupation of the condensate mode N0 so that the total number of atoms N = N0 + Nnc remains the same as in the mean-field GPE simulations with which we compare the TWA results to in the main text. We note that, for the simulation presented in Figs. 3 (a) and (d) the mode population used in the TWA was N/M ≈ 5. Similarly: for Figs. 3 (b) and (e), N/M ≈ 1; for Figs. 7 (a) and (d), N/M ≈ 3.4; for Figs. 7 (b) and (e), N/M ≈ 3.3; and for Figs. 7 (c) and (f), N/M ≈ 3.2. While some of these may seem rather small, it has been made known in the literature that the effect of varying the mode occupation for TWA in one-dimension has a very limited effect (if any at all) on the results [70–73]. Furthermore, in the truncated Wigner approach, stochastic averages over phase-space variables correspond to symmetrically ordered expectation values of their re- spective operators such that one would have, for example, ⟨ˆΨ†(x, t)ˆΨ(x, t)⟩sym = 1 2⟨ˆΨ†(x, t)ˆΨ(x, t) + ˆΨ(x, t)ˆΨ†(x, t)⟩ = ⟨ eψW (x, t)ψW (x, t)⟩. (C6) This means that, to compute observables which are nor- mally ordered, one needs to employ the Bose commu- tation relations to correctly represent the symmetrically ordered operator expectation values of the Wigner ap- proach. For the density itself, this results in ρ(x, t) = ⟨ˆΨ†(x, t)ˆΨ(x, t)⟩ (C7) = ⟨ψ∗ W (x, t)ψW (x, t)⟩ −1 2δc(x, x), (C8) where 1 2 ´ δc(x, x) dx = 1 2 M represents the half quantum of vacuum noise per mode M included in the Wigner14 formalism, which must now be subtracted off to obtain the mean density of the gas. Using an appropriate computational grid, which has spacing ∆x = L/Nx, with Nx being the number of grid points, the projected delta function δc(x, x) of Eq. (C8) is simply δc(x, x) = M/(Nx∆x) = M/L [35]. In this case one can compute the density in the TWA approach using ρ(x, t) ≡ ⟨ˆΨ†(x, t)ˆΨ(x, t)⟩ = ⟨|ψW (x, t)|2⟩ −M 2L. (C9) On the other hand, the current density can be computed using [74], j(x, t) = ℏ 2mi * ˆΨ†(x, t)∂ ˆΨ(x, t) ∂x − ˆΨ(x, t)∂ ˆΨ†(x, t) ∂x + (C10) = ℏ 2mi \u001c ψ∗ W (x, t) ∂ ∂x ψW (x, t) − ψW (x, t) ∂ ∂x ψ∗ W (x, t) \u001d (C11) = ℏ m Im \u001c ψ∗ W (x, t) ∂ ∂x ψW (x, t) \u001d . (C12) Appendix D: The stochastic projected Gross-Pitaevskii equation (SPGPE) Simulations of shock wave dynamics initiated from a thermal equilibrium state at a nonzero initial tempera- ture are performed using the stochastic Gross-Pitaevskii approach [33–35]; here we follow the prescription detailed in Appendix III of Ref. [25]. The essence of this approach is to treat the highly occupied modes of the Bose field operator as a classical field. The initial thermal equi- librium state of this classical field is determined using the simple growth stochastic projected Gross-Pitaevskii equation (SPGPE) [35] in which the field is coupled to a thermal reservoir (the low-occupancy modes) at tem- perature T. This initial thermal equilibrium state is pre- pared in the trapping potential given by Eq. (C4) and the stochastic field realizations are then evolved in time according to the standard GPE (6), with V (x) = 0, in order to determine the actual shock wave dynamics. The current density can be computed according to Eq. (C12) above where the Wigner field ψW (x, t) is replaced by the classical field of the SPGPE prescription. We note that the SPGPE is valid for degenerate 1D Bose gases with temperatures in the range γ ≲ T ≲ 1, where T = T/Td and Td = ℏ2ρ2/2mkB is the temper- ature of quantum degeneracy ( kB is Boltzmann’s con- stant). For non-uniform gases like we consider here, one should ensure that the temperature T (x) remains within these bounds across the entirety (or at least the major- ity) of the system, since ρ and therefore γ vary spatially across the gas. One way to do this is to consider the spatially invariant temperature τ = kBT/(mg2 1D/2ℏ2) = T /γ2 and choose temperatures according to the bounds γ−1 ≲ τ ≲ γ−2. This is what we have done in this work. For further discussion on these regimes of validity we re- fer the reader to Appendix A of Ref. [75]. Appendix E: Exact diagonalization in the TG limit In the TG limit we employ the Fermi-Bose mapping of Refs. [56, 57] that allows us to describe the many-body dynamics of the TG gas in terms of single-particle wave functions of a free-fermion system. In particular, the N- body wave function Ψ of the TG gas can be obtained via Ψ(x1, . . . , xN ; t) = A(x1, . . . , xN )Ψ(F)(x1, . . . , xN ; t), (E1) where Ψ (F) represents a spin-less free fermion N-body wave function for the same dynamical scenario, and the unit antisymmetric function is given by A(x1, . . . , xN ) =Q 1≤j<i≤N sgn(xi − xj) which ensures the correct sym- metrization of the bosonic wave function. Moreover, the N-body fermionic wave function itself can be constructed using the Slater determinant Ψ(F)(x1, . . . , xN ; t) = 1p (N − 1)! N det i,j=1 [ϕi(xj, t)], (E2) where the single-particle wave functions ϕi(x, t) evolve according to the Schr¨ odinger equation from their ini- tial states ϕi(x, 0), which are eigenstates of the initial trapping potential V (x, 0) with eigenenergies Ei such that the total energy of the N-body wave function is ETG = PN i=1 Ei. For further details regarding the con- struction of the TG solution, as well as the required initial trapping potential V (x, 0) for the shock wave scenarios we consider, see Refs. [25, 56, 57, 76]. While it is known that the mean particle number den- sity of the TG gas can be computed using ρ(x, t) = NX i=1 |ϕi(x, t)|2 , (E3) here we provide a derivation of how one can construct the mean probability current j(x, t) from free-fermion wave functions. We begin by first writing this quantity in terms of the15 Slater determinant of single-particle wave functions, j(x1; t) = ℏ m Im \u0012ˆ Ψ∗(x1, x2, ..., xN ; t) × ∂Ψ(x1, x2, ..., xN ; t) ∂x1 dx2...dxN \u0013 = ℏ m Im \u0012ˆ Ψ∗(F)(x1, x2, ..., xN ; t) × ∂Ψ(F)(x1, x2, ..., xN ; t) ∂x1 dx2...dxN \u0013 = ℏ m Im \u0012 1 (N − 1)! ˆ N det i,j=1 [ϕ∗ i (xj; t)] × ∂ detN k,l=1[ϕk(xl; t)] ∂x1 dx2...dxN ! , (E4) where we have used A2(x1, x2, ..., xN ) = 1. At this point it is possible to significantly simply the result by consid- ering the terms which will arise in the integrand. Since any single-particle wave function withϕk(xl̸=1; t) can be pulled out of the derivative ∂/∂x1, we note that the integral over x2...xN will kill any cross terms for which the index pairs ( k, l) don’t match ( i, j), due to the orthonormality of the single-particle wave functions. Moreover, this orthonormality will leave a summation of many other terms, each of which can be written as ϕ∗ i (x1; t) ∂ ∂x1 ϕi(x1; t), where the pre-factors of these re- maining terms will integrate to one. In fact, there will be precisely ( N − 1)! of these terms for each i. That is, there will be ( N − 1)! of the terms above for each of the N single-particle wave functions, and this leads to ˆ N det i,j=1 [ϕ∗ i (xj; t)]∂ detN k,l=1[ϕk(xl; t)] ∂x1 dx2...dxN = (N − 1)! NX i=1 ϕ∗ i (x1; t) ∂ ∂x1 ϕi(x1; t). (E5) Substituting Eq. (E5) back into Eq. (E4) shows that it is possible to compute the probability current of a zero- temperature TG gas using j(x, t) = ℏ m Im  NX i=1 ϕ∗ i (x, t) ∂ ∂x ϕi(x, t) ! , (E6) which is nothing more than a summation over the single- particle currents [26, 77], ji(x, t) = ℏ m Im \u0012 ϕ∗ i (x, t) ∂ ∂x ϕi(x, t) \u0013 . (E7) In order to actually compute the particle number den- sity and current density, we use the same methodology as in Ref. [25]. Specifically, we obtain the initial and time-evolved wave functions ϕi(x, t) using the trapping potentials and procedures outlined in the Supplemental Material of that work, where we use β > 0 to generate traps which create initial density bumps, and β < 0 to generate traps which create initial density dips. We use Eq. (S33) of Ref. [25] for scenarios with N ≥ 40 and Eq. (S9) [with ¯g = 0] for scenarios with N = 4. Appendix F: Bettelheim-Glazman oscillations in the TG limit In the TG limit, for large particle numbers N, Bettelheim-Glazman oscillations [61, 62] are present in the shock wave envelope and we identify them in this Appendix. In Figs. 8 (a) and (c) we show the same sce- nario as in Fig. 6 (c) (which was for N = 42) but now for N = 844 particles and with Nbg ≃ 880. In this den- sity dip scenario, we identify Bettelheim-Glazman oscilla- tions as density modulations in the shock wave envelope whose wavelength becomes larger towards both the lead- ing (right) and trailing (left) edges of the envelope. That is to say, the frequency of oscillations is chirped toward the middle of the shock. This is in contrast to the chirp direction of the interference pattern seen in the Thomas- Fermi regime of a weakly interacting system, which is towards the leading edge of the shock wave. In an equivalent density bump scenario, with Nbg ≃ 880 but now N = 1000, shown in Figs. 8 (b) and (d), we see the same Bettelheim-Glazman oscillations: their frequency is chirped in the same way as before, i.e., from the trailing and leading edges of the shock wave envelope towards the middle. The inset of Fig. 8 (b) shows these oscillations more clearly in the leading edge of the shock wave envelope. They span many of the barely visible Friedel oscillations whose amplitude is on the order of the linewidth, and whose period ∼ 1/ρbg is also not visible on the scale of the figure. Bettelheim-Glazman oscillations were first predicted in Ref. [61] by considering leading order quantum correc- tions in the Wigner function representation of an ideal Fermi gas, whose particle number density can be mapped to the density of a bosonic TG gas. The results presented here confirm the existence of these oscillations in an ex- act quantum many-body calculation. By going to a larger total number of particles compared with the cases con- sidered in Ref. [25] we have thus been able to discern be- tween Bettelheim-Glazman and Friedel-type oscillations, as the characteristic period and amplitude of the former are now much larger than those of Friedel oscillations. We should point out that our simulations are not strictly in the regime considered in Ref. [61, 62]; where the density bump should be small compared to the back-16 0 0.1 0.2 0.3 0.4 0.5 0 200 400 600 800 1000 0 0.1 0.2 0.3 0.4 0.5 0 1000 2000 3000 0 0.1 0.2 0.3 0.4 0.5 -6 -4 -2 0 0 0.1 0.2 0.3 0.4 0.5 -1 0 1 2 3 4 5 FIG. 8. Shock waves generated in the TG regime of a zero-temperature 1D Bose gas. The probability densities (top row) and currents (bottom row) are shown for: (a) and (c)—a density dip scenario with N = 844, γbg → ∞, β = −0.5, σ/L = 0.02, leading to Nbg = 880 .3; (b) and (d)—a density bump scenario with N = 1000, γbg → ∞, β = 1, σ/L = 0 .02, leading to Nbg = 880 .5. In (a) identifiable longer-wavelength Bettelheim-Glazman oscillations appear on the left edge of the shock, whereas the inset in (b) shows more clearly the appearance of many such oscillations (that span numerous Friedel oscillations) in the scenario plotted there. ground density, yet still contain many particles itself. As such, our results possess some of the Friedel-type oscilla- tions between different branches of the Fermi momentum (in phase-space) which are described in Ref. [59]. These oscillations cause the irregular peaks which are visible across the shock front. Nevertheless, we still see the clear signature of longer-wavelength Bettelheim-Glazman os- cillations appearing on top of these features. Appendix G: Identification of solitons in the mean-field GPE Here we explain how we identified rigorously the pres- ence of solitons in the results of Figure 7. In the variables used throughout this work, the ana- lytic profile of a gray soliton whose minimum is initially (t = 0) positioned at the origin and then proceeds to travel to the right can be written as [50] Ψs = r Nbg L   i v cbg + s 1 − v2 c2 bg × tanh \" x − vt lh s 1 − v2 c2 bg #! , (G1) where we have the background particle number Nbg, speed of sound in the background cbg = p g1Dρbg/m = ℏNbg √γbg/mL, and the healing length in the background lh = ℏ/√mg1Dρbg = L/Nbg √γbg. Additionally, the ve- locity with which the soliton propagates (undisturbed) through the fluid can be computed using [50] v2 = c2 bg ρmin ρbg = c2 bg Nmin Nbg , (G2) which requires us to determine the density at the mini- mum of the soliton. Hence, in order to determine the presence of solitons we first determine the minimum density of any solitons17 0.1 0.2 0.3 0.4 0.5 600 800 1000 1200 1400 1600 1800 2000 0.1 0.2 0.3 0.4 0.5 - 0 FIG. 9. Mean-field GPE density and phase profiles at time τ = 0 .002 for the dynamical situation of Fig. 7 (b). Panel (a) provides the dimensionless density where the dashed lines correspond to fits of soliton profiles for each of the density dips (shifted so that the minimums match). The soliton fits match the density profile extremely well, indicating that it is correct to identify these structures as solitons. Panel (b) shows the phase profile where the vertical dashed lines correspond to the position of density minimums of panel (a), and the horizontal dashed lines denote the predicted soliton phase shifts across each dip. Again, the predicted phase shifts match very well with the phase drops across each dip, further confirming that these structures are solitons. that are shed from the initial density dip. Along with the knowledge of Nbg and γbg this then allows us to calcu- late the solitons velocity and construct the density profile (G1), that we manually shift to the location of the respec- tive soliton minimum, x0, using x − vt → x − x0. As an example we choose the situation of Fig. 7 (b) and show the outcome of such a fitting procedure in Fig. 9 (a)—for all three density dips in the soliton train that can indi- vidually be identified as a gray soliton. As we see, the agreement is excellent, even for the solitons which have not yet completely separated from each other. In addition to the profile of Eq. (G1), solitons also have a well-defined phase shift across their profile [50], ∆S = 2 arccos \u0012 v cbg \u0013 , (G3) and we calculate this across each soliton in the example -0.5 0 0.5 0 0.0005 0.001 0.0015 0.002 0 200 400 600 800 1000 1200 1400 1600 1800 FIG. 10. Evolution of the entire density profile for the situa- tion given in Fig. 7 (b). The slope of the dashed lines are given by the calculated soliton velocities ¯v1 ≃ ±108.0, ¯v2 ≃ ±141.5, ¯v3 ≃ ±163.6, where each line is shifted so as to match with the location of each soliton at the final time τ = 0.002. provided here. From left to right in Fig. 9 (a) the minimum den- sity of each soliton is ρ(1) minL ≃ 662.6, ρ(2) minL ≃ 1137.7, ρ(3) minL ≃ 1520.7. Using Nbg ≃ 1761 and γbg = 0 .01 leads to a dimensionless background speed of sound cbgmL/ℏ ≃ 176.1, and therefore the dimensionless veloc- ity (¯v = vmL/ℏ) of each soliton is ¯v1 ≃ 108.0, ¯v2 ≃ 141.5, ¯v3 ≃ 163.6. Substituting these into Eq. (G3) gives the predicted phase shifts across the solitons: ∆ S1 ≃ 1.821, ∆S2 ≃ 1.274, and ∆ S3 ≃ 0.756 radians. In Fig. 9 (b) we plot the actual phase profile of the Bose gas from the mean-field GPE simulations at the same simulation time τ = 0.002 as the density in Fig. 9 (a). Here we determine the ‘global’ phase of the gas from the center of the box (x = 0), and starting at this value we subtract off con- secutively the calculated phase shifts and denote each of those values with horizontal dashed lines. We see that these analytically predicted phase shifts agree extremely well with the actual drops in the phase profile of the gas, further confirming that these structures are solitons. Finally, in Fig. 10 we show that these solitons form and become stable very quickly after being shed from the initial density dip. We plot the density profile over the entire domain x/L ∈ [−0.5, 0.5] as a function of time, which allows us to identify that the density dips travel with a preserved shape and constant velocity almost im- mediately after the initial density dip separates into left and right moving parts. The slope of the dashed lines in- dicate the velocities calculated above and they are given by x = ±¯vLτ + x0, where each is shifted by some differ- ent amount x0 so that they match with the location of the solitons at time τ = 0.002.18 [1] Z. Dutton, M. Budde, C. Slowe, and L. V. Hau, Ob- servation of Quantum Shock Waves Created with Ultra- Compressed Slow Light Pulses in a Bose-Einstein Con- densate, Science 293, 663 (2001). [2] A. M. Kamchatnov, R. A. Kraenkel, and B. A. Umarov, Asymptotic soliton train solutions of the defocusing non- linear Schr¨ odinger equation, Phys. Rev. E 66, 036609 (2002). [3] A. M. Kamchatnov, A. Gammal, and R. A. Kraenkel, Dissipationless shock waves in Bose-Einstein condensates with repulsive interaction between atoms, Phys. Rev. A 69, 063605 (2004). [4] B. Damski, Shock waves in ultracold Fermi (Tonks) gases, Journal of Physics B: Atomic, Molecular and Optical Physics 37, L85 (2004). [5] B. Damski, Formation of shock waves in a Bose-Einstein condensate, Phys. Rev. A 69, 043610 (2004). [6] V. M. P´ erez-Garc´ ıa, V. V. Konotop, and V. A. Brazh- nyi, Feshbach Resonance Induced Shock Waves in Bose- Einstein Condensates, Phys. Rev. Lett. 92, 220403 (2004). [7] T. P. Simula, P. Engels, I. Coddington, V. Schweikhard, E. A. Cornell, and R. J. Ballagh, Observations on Sound Propagation in Rapidly Rotating Bose-Einstein Conden- sates, Phys. Rev. Lett. 94, 080404 (2005). [8] G. El and A. Kamchatnov, Spatial dispersive shock waves generated in supersonic flow of Bose-Einstein condensate past slender body, Physics Letters A 350, 192 (2006). [9] B. Damski, Shock waves in a one-dimensional Bose gas: From a Bose-Einstein condensate to a Tonks gas, Phys. Rev. A 73, 043601 (2006). [10] M. A. Hoefer, M. J. Ablowitz, I. Coddington, E. A. Cor- nell, P. Engels, and V. Schweikhard, Dispersive and clas- sical shock waves in Bose-Einstein condensates and gas dynamics, Phys. Rev. A 74, 023623 (2006). [11] I. Carusotto, S. X. Hu, L. A. Collins, and A. Smerzi, Bogoliubov-ˇCerenkov Radiation in a Bose-Einstein Con- densate Flowing against an Obstacle, Phys. Rev. Lett. 97, 260403 (2006). [12] G. A. El, A. Gammal, and A. M. Kamchatnov, Oblique Dark Solitons in Supersonic Flow of a Bose-Einstein Con- densate, Phys. Rev. Lett. 97, 180405 (2006). [13] G. A. El, Y. G. Gladush, and A. M. Kamchatnov, Two-dimensional periodic waves in supersonic flow of a Bose–Einstein condensate, Journal of Physics A: Mathe- matical and Theoretical 40, 611 (2007). [14] J. J. Chang, P. Engels, and M. A. Hoefer, Formation of Dispersive Shock Waves by Merging and Splitting Bose-Einstein Condensates, Phys. Rev. Lett.101, 170404 (2008). [15] M. A. Hoefer, M. J. Ablowitz, and P. Engels, Piston Dispersive Shock Wave Problem, Phys. Rev. Lett. 100, 084504 (2008). [16] R. Meppelink, S. B. Koller, J. M. Vogels, P. van der Straten, E. D. van Ooijen, N. R. Heckenberg, H. Rubinsztein-Dunlop, S. A. Haine, and M. J. Davis, Observation of shock waves in a large Bose-Einstein con- densate, Phys. Rev. A 80, 043606 (2009). [17] M. Hoefer, P. Engels, and J. Chang, Matter–wave inter- ference in Bose-Einstein condensates: A dispersive hy- drodynamic perspective, Physica D: Nonlinear Phenom- ena 238, 1311 (2009). [18] M. Kulkarni and A. G. Abanov, Hydrodynamics of cold atomic gases in the limit of weak nonlinearity, dispersion, and dissipation, Phys. Rev. A 86, 033614 (2012). [19] M. Crosta, S. Trillo, and A. Fratalocchi, Crossover dy- namics of dispersive shocks in Bose-Einstein condensates characterized by two- and three-body interactions, Phys. Rev. A 85, 043607 (2012). [20] S. Peotta and M. D. Ventra, Quantum shock waves and population inversion in collisions of ultracold atomic clouds, Phys. Rev. A 89, 013621 (2014). [21] S. Peotta, C.-C. Chien, and M. Di Ventra, Phase-induced transport in atomic gases: From superfluid to Mott in- sulator, Phys. Rev. A 90, 053615 (2014). [22] L. Salasnich, Shock waves in quasi one-dimensional Bose- Einstein condensate, The European Physical Journal Plus 131, 66 (2016). [23] G. El and M. Hoefer, Dispersive shock waves and modu- lation theory, Physica D: Nonlinear Phenomena 333, 11 (2016). [24] M. E. Mossman, E. S. Delikatny, M. M. Forbes, and P. Engels, Stability in turbulence: The interplay between shocks and vorticity in a superfluid with higher-order dis- persion, Phys. Rev. A 102, 053310 (2020). [25] S. A. Simmons, F. A. Bayocboc Jr., J. C. Pillay, D. Co- las, I. P. McCulloch, and K. V. Kheruntsyan, What is a Quantum Shock Wave?, Phys. Rev. Lett. 125, 180401 (2020). [26] R. Dubessy, J. Polo, H. Perrin, A. Minguzzi, and M. Olshanii, Universal shock-wave propagation in one- dimensional Bose fluids, Phys. Rev. Research 3, 013098 (2021). [27] A. M. Kamchatnov, Gurevich–Pitaevskii problem and its development, Physics-Uspekhi 64, 48 (2021). [28] N. Karjanto, The nonlinear Schr¨ odinger equation: A mathematical model with its wide-ranging applications, in Understanding the Schr¨ odinger Equation: Some [Non]Linear Perspectives, edited by V. A. Simpao and H. C. Little (Nova Science Publishers, 2020) p. 316. [29] D. J. Korteweg and G. de Vries, XLI. On the change of form of long waves advancing in a rectangular canal, and on a new type of long stationary waves, The London, Ed- inburgh, and Dublin Philosophical Magazine and Journal of Science 39, 422 (1895). [30] A. V. Gurevich and L. P. Pitaevskii, Nonstationary struc- ture of a collisionless shock wave, Soviet Journal of Ex- perimental and Theoretical Physics 38, 291 (1974). [31] A. M. Kamchatnov, Nonlinear Periodic Waves and Their Modulations (World Scientific, 2000). [32] G. B. Whitham, Linear and Nonlinear Waves (New York, Wiley, 1974). [33] Y. Castin, R. Dum, E. Mandonnet, A. Minguzzi, and I. Carusotto, Coherence properties of a continuous atom laser, Journal of Modern Optics 47, 2671 (2000). [34] M. J. Davis, S. A. Morgan, and K. Burnett, Simulations of Bose Fields at Finite Temperature, Phys. Rev. Lett. 87, 160402 (2001). [35] P. B. Blakie, A. S. Bradley, M. J. Davis, R. J. Ballagh, and C. W. Gardiner, Dynamics and statistical mechan- ics of ultra-cold Bose gases using c-field techniques, Ad- vances in Physics 57, 363 (2008).19 [36] V. E. Zakharov, Stability of periodic waves of finite am- plitude on the surface of a deep fluid, Journal of Applied Mechanics and Technical Physics 9, 190 (1968). [37] R. J. Taylor, D. R. Baker, and H. Ikezi, Observation of Collisionless Electrostatic Shocks, Phys. Rev. Lett. 24, 206 (1970). [38] M. Q. Tran, K. Appert, C. Hollenstein, R. W. Means, and J. Vaclavik, Shocklike solutions of the Korteweg-de Vries equation, Plasma Physics 19, 381 (1977). [39] Y. C. Mo, R. A. Kishek, D. Feldman, I. Haber, B. Beau- doin, P. G. O’Shea, and J. C. T. Thangaraj, Experi- mental Observations of Soliton Wave Trains in Electron Beams, Phys. Rev. Lett. 110, 084802 (2013). [40] E. Rolley, C. Guthmann, and M. Pettersen, The hy- draulic jump and ripples in liquid helium, Physica B: Condensed Matter 394, 46 (2007). [41] L. Dominici, M. Petrov, M. Matuszewski, D. Ballarini, M. De Giorgi, D. Colas, E. Cancellieri, B. S. Fern´ andez, A. Bramati, G. Gigli, et al., Real-space collapse of a po- lariton condensate, Nature Communications 6, 1 (2015). [42] J. E. Rothenberg and D. Grischkowsky, Observation of the Formation of an Optical Intensity Shock and Wave Breaking in the Nonlinear Propagation of Pulses in Op- tical Fibers, Phys. Rev. Lett. 62, 531 (1989). [43] W. Wan, S. Jia, and J. W. Fleischer, Dispersive superfluid-like shock waves in nonlinear optics, Nature Physics 3, 46 (2007). [44] G. A. El, A. Gammal, E. G. Khamis, R. A. Kraenkel, and A. M. Kamchatnov, Theory of optical dispersive shock waves in photorefractive media, Phys. Rev. A76, 053813 (2007). [45] P. A. P. Janantha, P. Sprenger, M. A. Hoefer, and M. Wu, Observation of Self-Cavitating Envelope Disper- sive Shock Waves in Yttrium Iron Garnet Thin Films, Phys. Rev. Lett. 119, 024101 (2017). [46] G. A. El, A. M. Kamchatnov, V. V. Khodorovskii, E. S. Annibale, and A. Gammal, Two-dimensional supersonic nonlinear Schr¨ odinger flow past an extended obstacle, Phys. Rev. E 80, 046317 (2009). [47] E. H. Lieb and W. Liniger, Exact Analysis of an Interact- ing Bose Gas. I. The General Solution and the Ground State, Phys. Rev. 130, 1605 (1963). [48] M. Olshanii, Atomic Scattering in the Presence of an Ex- ternal Confinement and a Gas of Impenetrable Bosons, Phys. Rev. Lett. 81, 938 (1998). [49] In the ideal, weakly-interacting mean-field, and infinitely- interacting Tonks-Girardeau regimes, analytic forms of the appropriate initial trapping potential to generate the density profile of Eq. (4) are known (see appendices of Ref. [25]). Outside of these regimes we approximate the initial state by empirically altering one of these existing analytic trapping potentials. [50] L. Pitaevskii and S. Stringari, Bose-Einstein Condensa- tion and Superfluidity (Oxford University Press, 2016). [51] When the current density (11) is normalised to one, it is instead usually referred to as the probability current. Moreover, if the number density ρ0 used here was re- placed by the mass density (related simply by a factor of m), the current density is then often referred to as the mass flux density. [52] D. Bohm, A Suggested Interpretation of the Quantum Theory in Terms of ”Hidden” Variables. I, Phys. Rev.85, 166 (1952); A Suggested Interpretation of the Quantum Theory in Terms of ”Hidden” Variables. II, Phys. Rev. 85, 180 (1952). [53] Y. Band, Interference of bose- einstein condensates, The Journal of Physical Chemistry B 112, 16097 (2008). [54] G. El, V. Geogjaev, A. Gurevich, and A. Krylov, De- cay of an initial discontinuity in the defocusing NLS hy- drodynamics, Physica D: Nonlinear Phenomena 87, 186 (1995). [55] We should point out that the truncated Wigner approach provides the result of an ensemble average. However, it may be possible that a vacuum point could manifest itself in individual stochastic Wigner trajectories. If such indi- vidual Wigner trajectories are interpreted as represent- ing possible outcomes of individual experimental runs (for further reading on this somewhat controversial is- sue, please see Refs. [78–82]), this would suggest that the vacuum point may still be observed in such individ- ual experimental runs, even though it disappears after ensemble averaging. [56] M. Girardeau, Relationship between Systems of Impene- trable Bosons and Fermions in One Dimension, Journal of Mathematical Physics 1, 516 (1960). [57] V. I. Yukalov and M. D. Girardeau, Fermi-Bose mapping for one-dimensional Bose gases, Laser Physics Letters 2, 375 (2005). [58] J. Friedel, Metallic alloys, II Nuovo Cimento (1955-1965) 7, 287 (1958). [59] I. V. Protopopov, D. B. Gutman, P. Schmitteckert, and A. D. Mirlin, Dynamics of waves in one-dimensional elec- tron systems: Density oscillations driven by population inversion, Phys. Rev. B 87, 045112 (2013). [60] M. Cazalilla, Bosonizing one-dimensional cold atomic gases, Journal of Physics B: Atomic, Molecular and Op- tical Physics 37, S1 (2004). [61] E. Bettelheim and L. Glazman, Quantum Ripples Over a Semiclassical Shock, Phys. Rev. Lett. 109, 260602 (2012). [62] T. Veness and L. I. Glazman, Fate of quantum shock waves at late times, Phys. Rev. B 100, 235125 (2019). [63] R. S. Watson, S. A. Simmons, and K. V. Kheruntsyan, Benchmarks of generalized hydrodynamics for one- dimensional bose gases, Phys. Rev. Res. 5, L022024 (2023). [64] The speed of propagation of a soliton is related to its depth, or minimum density ρmin, through vsoliton =p g1Dρmin/m, which is exactly the bulk sound velocity evaluated at the density ρmin. [65] M. J. Steel, M. K. Olsen, L. I. Plimak, P. D. Drummond, S. M. Tan, M. J. Collett, D. F. Walls, and R. Graham, Dynamical quantum noise in trapped Bose-Einstein con- densates, Phys. Rev. A 58, 4824 (1998). [66] A. D. Martin and J. Ruostekoski, Nonequilibrium quan- tum dynamics of atomic dark solitons, New Journal of Physics 12, 055018 (2010). [67] C. J. Pethick and H. Smith, Bose-Einstein Condensation in Dilute Gases , 2nd ed. (Cambridge University Press, 2008) p. 400. [68] F. Dalfovo, S. Giorgini, L. P. Pitaevskii, and S. Stringari, Theory of Bose-Einstein condensation in trapped gases, Rev. Mod. Phys. 71, 463 (1999). [69] X.-L. Chen, Y. Li, and H. Hu, Collective modes of a har- monically trapped one-dimensional Bose gas: The effects of finite particle number and nonzero temperature, Phys. Rev. A 91, 063631 (2015). [70] L. Isella and J. Ruostekoski, Nonadiabatic dynamics of20 a Bose-Einstein condensate in an optical lattice, Phys. Rev. A 72, 011601(R) (2005). [71] L. Isella and J. Ruostekoski, Quantum dynamics in split- ting a harmonically trapped Bose-Einstein condensate by an optical lattice: Truncated Wigner approximation, Phys. Rev. A 74, 063625 (2006). [72] J. Ruostekoski and A. D. Martin, The Truncated Wigner Method for Bose Gases, in Quantum Gases: Finite Tem- perature and Non-Equilibrium Dynamics (Imperial Col- lege Press, 2013) Chap. 13, pp. 203–214. [73] S. A. Simmons, J. C. Pillay, and K. V. Kheruntsyan, Phase-space stochastic quantum hydrodynamics for in- teracting Bose gases, Phys. Rev. A 106, 043309 (2022). [74] P. D. Drummond and B. Opanchuk, Truncated Wigner dynamics and conservation laws, Phys. Rev. A 96, 043616 (2017). [75] F. A. Bayocboc Jr. and K. V. Kheruntsyan, Frequency beating and damping of breathing oscillations of a harmonically trapped one-dimensional quasicondensate, Comptes Rendus. Physique 24, 1 (2023). [76] K. K. Das, M. D. Girardeau, and E. M. Wright, Inter- ference of a Thermal Tonks Gas on a Ring, Phys. Rev. Lett. 89, 170404 (2002). [77] A. Minguzzi and P. Vignolo, Strongly interacting trapped one-dimensional quantum gases: Exact solution, AVS Quantum Science 4, 027102 (2022). [78] R. J. Lewis-Swan, M. K. Olsen, and K. V. Kheruntsyan, Approximate particle number distribution from direct stochastic sampling of the Wigner function, Phys. Rev. A 94, 033814 (2016). [79] K. Sakmann and M. Kasevich, Single-shot simulations of dynamic quantum many-body systems, Nature Physics 12, 451 (2016). [80] P. D. Drummond and J. Brand, Comment on: ‘Single- shot simulations of dynamic quantum many-body systems’ [arXiv:1501.03224], arXiv:1610.07633 [cond- mat.quant-gas] (2016). [81] M. K. Olsen, J. F. Corney, R. J. Lewis-Swan, and A. S. Bradley, Correspondence on “Single-shot sim- ulations of dynamic quantum many-body systems”, arXiv:1702.00282 [quant-ph] (2017). [82] K. Sakmann and M. Kasevich, Reply to the corre- spondence of Drummond and Brand [arXiv:1610.07633], arXiv:1702.01211 [cond-mat.quant-gas] (2017).",
      "references": [
        "Observation of Quantum Shock Waves Created with Ultra- Compressed Slow Light Pulses in a Bose-Einstein Con- densate",
        "Asymptotic soliton train solutions of the defocusing non- linear Schr¨ odinger equation",
        "Dissipationless shock waves in Bose-Einstein condensates with repulsive interaction between atoms",
        "Shock waves in ultracold Fermi (Tonks) gases",
        "Formation of shock waves in a Bose-Einstein condensate",
        "Feshbach Resonance Induced Shock Waves in Bose- Einstein Condensates",
        "Observations on Sound Propagation in Rapidly Rotating Bose-Einstein Conden- sates",
        "Spatial dispersive shock waves generated in supersonic flow of Bose-Einstein condensate past slender body",
        "Shock waves in a one-dimensional Bose gas: From a Bose-Einstein condensate to a Tonks gas",
        "Dispersive and clas- sical shock waves in Bose-Einstein condensates and gas dynamics",
        "Bogoliubov-ˇCerenkov Radiation in a Bose-Einstein Con- densate Flowing against an Obstacle",
        "Oblique Dark Solitons in Supersonic Flow of a Bose-Einstein Con- densate",
        "Two-dimensional periodic waves in supersonic flow of a Bose–Einstein condensate",
        "Formation of Dispersive Shock Waves by Merging and Splitting Bose-Einstein Condensates",
        "Piston Dispersive Shock Wave Problem",
        "Observation of shock waves in a large Bose-Einstein con- densate",
        "Matter–wave inter- ference in Bose-Einstein condensates: A dispersive hy- drodynamic perspective",
        "Hydrodynamics of cold atomic gases in the limit of weak nonlinearity, dispersion, and dissipation",
        "Crossover dy- namics of dispersive shocks in Bose-Einstein condensates characterized by two- and three-body interactions",
        "Quantum shock waves and population inversion in collisions of ultracold atomic clouds",
        "Phase-induced transport in atomic gases: From superfluid to Mott in- sulator",
        "Shock waves in quasi one-dimensional Bose- Einstein condensate",
        "Dispersive shock waves and modu- lation theory",
        "Stability in turbulence: The interplay between shocks and vorticity in a superfluid with higher-order dis- persion",
        "What is a Quantum Shock Wave?",
        "Universal shock-wave propagation in one- dimensional Bose fluids",
        "Gurevich–Pitaevskii problem and its development",
        "The nonlinear Schr¨ odinger equation: A mathematical model with its wide-ranging applications",
        "XLI. On the change of form of long waves advancing in a rectangular canal, and on a new type of long stationary waves",
        "Nonstationary struc- ture of a collisionless shock wave",
        "Nonlinear Periodic Waves and Their Modulations",
        "Linear and Nonlinear Waves",
        "Coherence properties of a continuous atom laser",
        "Simulations of Bose Fields at Finite Temperature",
        "Dynamics and statistical mechan- ics of ultra-cold Bose gases using c-field techniques",
        "Stability of periodic waves of finite am- plitude on the surface of a deep fluid",
        "Observation of Collisionless Electrostatic Shocks",
        "Shocklike solutions of the Korteweg-de Vries equation",
        "Experi- mental Observations of Soliton Wave Trains in Electron Beams",
        "The hy- draulic jump and ripples in liquid helium",
        "Real-space collapse of a po- lariton condensate",
        "Observation of the Formation of an Optical Intensity Shock and Wave Breaking in the Nonlinear Propagation of Pulses in Op- tical Fibers",
        "Dispersive superfluid-like shock waves in nonlinear optics",
        "Theory of optical dispersive shock waves in photorefractive media",
        "Observation of Self-Cavitating Envelope Disper- sive Shock Waves in Yttrium Iron Garnet Thin Films",
        "Two-dimensional supersonic nonlinear Schr¨ odinger flow past an extended obstacle",
        "Exact Analysis of an Interact- ing Bose Gas. I. The General Solution and the Ground State",
        "Atomic Scattering in the Presence of an Ex- ternal Confinement and a Gas of Impenetrable Bosons",
        "Bose-Einstein Condensa- tion and Superfluidity",
        "A Suggested Interpretation of the Quantum Theory in Terms of ”Hidden” Variables. I",
        "A Suggested Interpretation of the Quantum Theory in Terms of ”Hidden” Variables. II",
        "Interference of bose- einstein condensates",
        "De- cay of an initial discontinuity in the defocusing NLS hy- drodynamics",
        "Relationship between Systems of Impene- trable Bosons and Fermions in One Dimension",
        "Fermi-Bose mapping for one-dimensional Bose gases",
        "Metallic alloys",
        "Dynamics of waves in one-dimensional elec- tron systems: Density oscillations driven by population inversion",
        "Bosonizing one-dimensional cold atomic gases",
        "Quantum Ripples Over a Semiclassical Shock",
        "Fate of quantum shock waves at late times",
        "Benchmarks of generalized hydrodynamics for one- dimensional bose gases",
        "Dynamical quantum noise in trapped Bose-Einstein con- densates",
        "Nonequilibrium quan- tum dynamics of atomic dark solitons",
        "Bose-Einstein Condensation in Dilute Gases",
        "Theory of Bose-Einstein condensation in trapped gases",
        "Collective modes of a har- monically trapped one-dimensional Bose gas: The effects of finite particle number and nonzero temperature",
        "Nonadiabatic dynamics of a Bose-Einstein condensate in an optical lattice",
        "Quantum dynamics in split- ting a harmonically trapped Bose-Einstein condensate by an optical lattice: Truncated Wigner approximation",
        "The Truncated Wigner Method for Bose Gases",
        "Phase-space stochastic quantum hydrodynamics for in- teracting Bose gases",
        "Truncated Wigner dynamics and conservation laws",
        "Frequency beating and damping of breathing oscillations of a harmonically trapped one-dimensional quasicondensate",
        "Inter- ference of a Thermal Tonks Gas on a Ring",
        "Strongly interacting trapped one-dimensional quantum gases: Exact solution",
        "Approximate particle number distribution from direct stochastic sampling of the Wigner function",
        "Single-shot simulations of dynamic quantum many-body systems",
        "Comment on: ‘Single- shot simulations of dynamic quantum many-body systems’ [arXiv:1501.03224]",
        "Correspondence on “Single-shot sim- ulations of dynamic quantum many-body systems”",
        "Reply to the corre- spondence of Drummond and Brand [arXiv:1610.07633]"
      ],
      "meta_data": {
        "arxiv_id": "2305.17647v3",
        "doi": "10.1103/PhysRevA.108.013317",
        "authors": [
          "S. A. Simmons",
          "J. C. Pillay",
          "K. V. Kheruntsyan"
        ],
        "published_date": "2023-05-28T06:39:06Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper extends the study of dispersive quantum shock waves in a one-dimensional Bose gas beyond the mean-field approximation. The main contributions include demonstrating that 'vacuum points' and 'grey solitons', which manifest in mean-field descriptions, cease to exist when quantum or thermal fluctuations, or strong interactions, are considered. It shows that a vacuum point can still occur in an ideal (noninteracting) Bose gas. The research consolidates the understanding of quantum interference and soliton generation by also analyzing the mean current density j(x, t). It identifies that quantum and thermal fluctuations diminish interference visibility and prevent vacuum points in the mean density. Furthermore, the study reveals that the number of generated solitons in density dip scenarios can be precisely controlled by adjusting the initial dip's width and depth. Finally, it confirms the existence of Bettelheim-Glazman oscillations in exact quantum many-body calculations in the Tonks-Girardeau limit for large particle numbers, distinguishing them from Friedel oscillations.",
        "methodology": "The study employed a range of theoretical approaches to analyze the dynamics of a one-dimensional Bose gas. For mean-field descriptions, the Gross-Pitaevskii Equation (GPE) and its equivalent superfluid hydrodynamic equations were used. Beyond mean-field effects were incorporated using the Truncated Wigner Approach (TWA) for quantum fluctuations, the classical-field Stochastic Projected Gross-Pitaevskii Equation (SPGPE) for thermal fluctuations and finite temperatures, and infinite Matrix Product States (iMPS) for intermediate and strong interaction regimes. For the infinitely strong interaction (Tonks-Girardeau) limit, Fermi-Bose mapping combined with exact diagonalization of the free-fermion problem was utilized. Analytical solutions for the ideal Bose gas were also derived. The primary observables analyzed were the mean particle number density ρ(x, t) and the mean current density j(x, t).",
        "experimental_setup": "The research simulated a one-dimensional Bose gas in a uniform potential with periodic boundary conditions. Initial conditions were set as localized density bumps (β > 0) or dips (-1 < β < 0) on a uniform background, described by the profile ρ(x, 0) = Nbg(1 + βe⁻ˣ²⁄²σ²)²⁄L. Various parameters were explored, including total particle numbers (N ranging from 4 to 2000), dimensionless interaction strengths (γbg from 0 for ideal gas, to 0.1, 1, 10 for interacting regimes, and ∞ for Tonks-Girardeau limit), initial bump/dip heights (β = 1, 5, -0.5), and widths (σ/L = 0.005, 0.01, 0.02, 0.04). Validation of the GPE solutions for solitons involved fitting them with analytical profiles and comparing predicted phase shifts across the density dips. TWA and SPGPE results were obtained by ensemble averaging over 100,000 stochastic trajectories.",
        "limitations": "The mean-field Gross-Pitaevskii Equation (GPE) inherently neglects quantum fluctuations and correlations, leading to phenomena like the vacuum point being identified as a strictly mean-field artifact. The Stochastic Projected Gross-Pitaevskii Equation (SPGPE) is valid within specific temperature ranges (γ ≲ T ≲ 1 in dimensionless units), requiring careful parameter tuning. The exact origin of certain large-scale oscillations observed in density dip scenarios within the Tonks-Girardeau limit for small particle numbers remains unidentified. The paper notes that the interpretation of individual Wigner trajectories as representing outcomes of single experimental runs is a controversial topic. Additionally, the simulations conducted for Bettelheim-Glazman oscillations were not strictly within the ideal regime assumed by prior theoretical work (where the initial bump is small compared to the background but still contains many particles).",
        "future_research_directions": "Future research directions include investigating the true cause and origin of the intriguing large-scale oscillations observed in density dip scenarios in the Tonks-Girardeau limit for small particle numbers. Another area for exploration is to understand the mechanism behind the formation of Bettelheim-Glazman oscillations more thoroughly. The paper also suggests exploring the presence and characteristics of Bettelheim-Glazman oscillations at very large but finite interaction strengths, moving beyond the strictly Tonks-Girardeau limit.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Human-like Few-Shot Learning via Bayesian Reasoning over Natural Language",
      "full_text": "Federated Empirical Risk Minimization via Second-Order Method Song Bian∗ Zhao Song† Junze Yin‡ Abstract Many convex optimization problems with important applications in machine learning are formulated as empirical risk minimization (ERM). There are several examples: linear and logistic regression, LASSO, kernel regression, quantile regression, p-norm regression, support vector machines (SVM), and mean-field variational inference. To improve data privacy, federated learning is proposed in machine learning as a framework for training deep learning models on the network edge without sharing data between participating nodes. In this work, we present an interior point method (IPM) to solve a general ERM problem under the federated learning setting. We show that the communication complexity of each iteration of our IPM is eO(d3/2), where d is the dimension (i.e., number of features) of the dataset. ∗BianSongHZ@gmail.com. University of Wisonsin, Madison. †zsong@adobe.com. Adobe Research. ‡junze@bu.edu. Boston University. arXiv:2305.17482v1  [cs.LG]  27 May 2023Contents 1 Introduction 2 2 Related Work 3 3 Background 4 3.1 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.2 Empirical Risk Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.3 Central Path Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.4 Newton Step . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4 IPM under FL 7 4.1 Sketching Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 4.2 Our Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5 Theoretical Analysis 10 6 Compared to Standard Methods 12 7 Conclusion and Discussion 12 A Probability Tools and Basic Properties of Random Sketching Matrices 14 A.1 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 A.2 Properties obtained by random projection . . . . . . . . . . . . . . . . . . . . . . . . 15 B Sketch more than once 16 C Bounding error of sketching 19 C.1 Definition of P, bP, and eP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.2 Proof sketch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.3 Bounding |g⊤P h− g⊤ bP h| . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.4 Tools for bounding |g⊤P h− g⊤ bP h| . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 C.5 Bounding |g⊤ eP h− g⊤ bP h| . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.6 Tools for Bounding |g⊤ eP h− g⊤ bP h| . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 C.7 Bounding |g⊤P h− g⊤ eP h| . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 D Main Result 25 E Central Path 26 F Initial Point and Termination Condition 28 11 Introduction Empirical Risk Minimization (ERM) is one of the key problems in machine learning research. ERM appears in many machine learning problems including LASSO [Tib96], logistic regression [Cox58, HJLS13], support vector machines [CV95], AdaBoost [FS97], kernel regression [Nad64, Wat64], etc. Due to its wide applications, a great number of works have considered this problem. They not only study the statistical convergence properties but also investigate how to develop efficient algorithms for ERM. Among these efficient algorithms, Interior Point Methods is one of the most widely-used optimization algorithm. IPM is first proposed by [Kar84]. After that, IPM has become an active area in optimization research. There is a long line of work using IPM to speedup optimization prob- lems, such as linear programming [CLS19, JSWZ21, SY21], semi-definite programming [JKL +20a], and cutting plane method [JLSW20]. Recently, [LSZ19] develops a fast and robust algorithm to solve Empirical Risk Minimization (ERM). However, users are not willing to share data with others. Therefore, Federated Learning, which is a general framework for distributed learning on sensitive data, is paid more attention to recently. Motivated by the Sketched-SGD [IRU+19] and FetchSGD [RPU+20], there exists a large num- ber of works focus on reducing the communication cost [JST+14, KMY+16, LHM+17]. In addition, some works [LSZ+20] develop optimization algorithms under federated learning. Nevertheless, all of them develop distributed SGD, which is a first-order optimization algorithm. Due to the reason that first-order algorithms for ERM always depend polynomially on the Lipschitz constant of the gradient and the running time will also have to depend on the strong convexity of the optimization function [LSZ19]. In view of this, we focus on developing distributed second-order optimization algorithms in this paper. As for the distributed second-order optimization algorithm, [GGD +21] develops a distributed second-order method, which could address the bottleneck of distributed set- ting. However, in order to present convergence analysis, they make several strong assumptions that are unrealistic in practice. In this work, we mainly study the ERM under FL, we called it FERM (Federated Empirical Risk Minimization). We develop an IPM framework under FL settings to address FERM first. Then, considering the communication issue of the IPM framework under FL settings, we use sketching techniques to reduce the communication cost. In the end, we present the convergence analysis of our algorithm. Challenges. We have witnessed the success of the first-order optimization algorithm under FL. Nevertheless, it is non-trivial to design an IPM under FL. Especially, we need to use the sketching technique to reduce the communication cost and provide convergence analysis for IPM under FL. In the following sections, we focus on answering the following problems: • How to design a distributed IPM algorithm without data sharing? • How to use sketch matrices to compress the Hessian information under distributed setting? • Is it possible to present convergence guarantees of IPM under FL? Before we show the specific algorithms and analysis, we first state our main result here: Theorem 1.1 (Informal Main Result, see Appendix D for details). If the following conditions hold • Consider a convex problem under the FL setting minAx=b,x∈Πm i=1Ki c⊤x, where Ki is compact convex sets. • For each i ∈ [m], we are given a νi-self concordant barrier function ϕi for Ki. 2Then, there exists a FL algorithm (see Algorithm 1) runs in O(√ν log2 m log(ν δ )) iterations and each iteration sends O(bmaxn) bits to find a vector x up to δ error, where bmax is determined by the size of sketch matrices. Contributions. Our contributions are summarized as follows: • To the best of our knowledge, we first study the ERM under FL settings. And we propose an IPM under FL to solve FERM. • We are also the first one to sketch Hessian information of the IPM algorithm under the FL setting. Previous works only either sketch the gradient information under the FL setting or Hessian information under the classical distributed computing setting. • We show convergence guarantees of IPM under FL, which compresses the Hessian via sketch- ing methods to reduce communication costs. Due to the reason that IPM is a second-order optimization method, it is non-trivial for us to present such convergence without making strong assumptions. Organization. We present related work in Section 2. In Section 3, we present the background of this paper. And in Section 4, we formulate the problem first. Next, we give the sketching technique we used in our algorithm and the overview of our main algorithm. In Section 5, we present the theoretical analysis of our algorithm. In Section 6, we compare our algorithm with some naive models. And we conclude this paper in Section 7. 2 Related Work Distributed Optimization Methods. Nowadays, distributed optimization methods have gained popularity. As for the distributed first-order optimization methods, a large number of works fo- cus on developing communication-efficient distributed SGD. These work includes that distributed variants of stochastic gradient descent [ZWSL10, NRRW11, MKSB13], accelerated SGD [SS14], variance reduction SGD [LLMY17, RHS +15], dual coordinate ascent algorithms [Yan13, RT16, ZWX+17], and stochastic coordiante descent methods [FR16]. As for the distributed second-order optimization methods, DANE [SSZ14], AIDE [RKR +16], and DiSCO [ZL15] are well-known work. CoCoA [JST+14, MSJ+15, SFC+18] is similar to the second-order method, but it does not use any second-order information. Federated Learning. Federated learning is a special case of distributed machine learning. Fed- erated learning allows clients to train machine learning models without data sharing. The appli- cations of federated learning include healthcare [LGD +20, RHL+20], financial area [YZY+19], and autonomous vehicle [LLC +19]. Although federated learning has numerous advantages, the feder- ated learning is always limited by the communication issue. In view of this, a great number of methods [IRU +19, RPU +20] are developed to reduce communication cost in federated learning. FEDAVG [MMR+17] is the first work focus on solving communication efficiency problem in feder- ated learning. After that, a great number of gradient compression methods [IRU+19, RPU+20] have been proposed. Communication-efficient algorithms achieve success in practice. However, it is not easy to present convergence analysis for communication-efficient algorithms. Recently, [RPU +20] presents convergence analysis for SGD under federated learning. Federated learning convergence 3on one-layer neural networks is investigated in [LJZ +21]. Furthermore, [HLSY21] gives conver- gence guarantees of the general federated learning on neural networks. [SWYZ23] studies federated learning for convex, Lipschitz, and smooth functions. [LSY23] proposes an federated algorithm for adversarial training in deep learning. Another interesting angle of federated learning is differential privacy, a number of works [HSLA20, HSC+20, CLSZ21, CSTZ22] have studied the privacy inspired question related to federated learning. Privacy is not the major task in this paper. Sketching Technique. Sketching technique has been widely applied to many applications in ma- chine learning, such as low-rank approximation [CW13, NN13, MM13, BW14, SWZ17, ALS +18], linear regression, distributed problems [WZ16, BWZ16], reinforcement learning [WZD +20], ten- sor decomposition [SWZ19], sparsification of attention matrix [DMS23], discrepancy minimization [DSW22], clustering [EMZ21], online bipartite matching [SWY23, HST +22], exponential and soft- max regression [LSZ23, DLS23a, LSX +23, GSY23], integral optimization [JLSZ23], submodular problem [QSW23], generative adversarial networks [XZZ18], symmetric norm estimation [DSWZ22], optimizing neural tangent kernel [BPSW21, SYZ21, SZZ21, HSWZ22, Zha22, GQSW22, ALS +22], database [QJS +22], fast attention computation [AS23], dynamic kernel computation [QRS +22, DJS+22], matrix completion [GSYZ23], matrix sensing [DLS23b, QSZ23]. Count Sketch [CCF02] is used in [IRU+19, RPU+20] to reduce the cost of communication at each iteration. Count Sketch is able to approximate every coordinate of a vector with an ℓ2 guarantee. And it is also possible to recover an approximate vector from Count Sketch. In this paper, we use AMS matrices [AMS99] to compress the model updates at each iteration to reduce the communication cost of FL. 3 Background In Section 3.1, we explain the notations that we use. In Section 3.2, we introduce empirical risk minimization. In Section 3.3, we explain the central path method and the properties of the self- concordant barrier function. In Section 3.4, we present the Newton method. 3.1 Notations Given a positive value n, we use [ n] to denote {1, 2, ··· , n}. We use m to denote the number of clients. For each client i ∈ [m], it contains dataset Ai ∈ Rd×ni. We also assume that Pm i=1 ni = n. Moreover, we define x as the main variable, and s as the slack variable. We use xi, si, Wi and Ai to denote the variables that are computed in client ci. And we use xt i, st i, Wt i , and At i to denote the variables that are computed in client ci at t-th iteration. Next, we define two operations here. The operation ⊕ denotes concatenation operation, which indicates that x = ⊕m i=1xi = [x1, x2, . . . , xm]⊤ and s = ⊕m i=1si = [s1, s2, . . . , sm]⊤. And we use ⊗ to denote the following operation: W = ⊗m i=1Wi = diag(W1, W2, ··· , Wm). Given that f and g are two functions, f ≲ g means that f ≤ Cg, where C is a constant. Let v be a vector. ∥v∥ represents the standard Euclidean norm. E[] represents the expectation and Pr[] denotes the probability. We use ∇f(x) to denote the gradient of f, namely df dx . For any A ∈ Rm×n, ∥A∥2 represents its operator norm and ∥A∥F stands for its Frobenius norm. We also use some facts that ∥AB∥2 ≤ ∥A∥2 · ∥B∥2, ∥A∥F ≤ √n∥A∥2. Moreover, if the matrix A ∈ Rn×n is a block diagonal matrix, then A could be expressed as diag( A1, A2, ··· , Am), where A1 is a matrix whose dimensions is n1 × n1, A2 is a matrix whose dimensions is n2 × n2, and Am is a matrix whose dimensions is nm × nm. In addition, Pm i=1 ni = n. If A ∈ Rn×n is a symmetric positive semi-definite (PSD) matrix, i.e., A ⪰ 0 if for all vectors x ∈ Rn, then x⊤Ax ≥ 0, and we 4use ∥v∥A to denote (v⊤Av)1/2. If we are given a convex function f, we use ∥v∥x to denote ∥v∥∇2f(x) and ∥v∥∗ x to denote ∥v∥∇2f(x)−1 for simplicity. In general, we use R ∈ Rb×d or S ∈ Rb×d to denote sketches that are used to compress model updates. In order to distinguish different sketches, we use Ri ∈ Rbi×d and Si ∈ Rbi×d. Furthermore, in this paper, we consider the computation model to be word RAM model. In this model, each word has O(log n) bits and all the basic computation can be done in O(1). This is standard in the literature of algorithm design [CLRS09] and distributed algorithm [WZ16, BWZ16]. 3.2 Empirical Risk Minimization We give the definition of traditional Empirical Risk Minimization (ERM) as below: Definition 3.1 (Empirical Risk Minimization). Given a convex function fi : Rd → R, Ai ∈ Rd×ni, xi ∈ Rni and bi ∈ Rd, ∀i ∈ [m], we call the following optimization problem as Empirical Risk Minimization problem minx Pm i=1 fi(Aixi + bi). Then, we could rewrite the original problem by defining yi = Aixi + bi, zi = fi(Aixi + bi). After that, we could get the following problem: min x,y,z mX i=1 zi s.t. Ax+ b = y (yi, zi) ∈ Ki = {(yi, zi) : fi(yi) ≤ zi}, ∀i ∈ [m] In this paper, we mainly consider the following question, when the dimension of Ki could be arbitrary: min x∈Qm i=1 Ki,Ax=b c⊤x. In the next section, we briefly introduce the solutions to address the general form under cen- tralized setting. 3.3 Central Path Method In this section, we introduce the central path method. First, we recap the problem that we analyze: min x∈Qm i=1 Ki,Ax=b c⊤x (1) For each i ∈ [m], Ki is a convex set and xi is the i-th block of x respect to Ki. The interior point methods (IPM) consider the following path of solutions: x(t) = arg min Ax=b c⊤x + t mX i=1 ϕi(xi) (2) where ϕi(·) is called self-concordant barrier function. (Fig. 3 is an example of barrier function). And the path is always called central path. The IPM solves Eq. (1) by decreasing t → 0 (See Fig. 1). The running time of the central path based algorithm is determined by the self-concordant barrier function. In view of this, we first present the definition and properties of self-concordant barrier function here. 5x(1) x(t) x(0) Feasible region Figure 1: Here is an example of the central path. The curve denotes the central path. The hexagon denotes the feasible region. The start point of the central path is x(1), and the end point of the central path is x(0). Then we follow the path x(t) to from x(1) to x(0). Definition 3.2. Given a function ϕ, if any x ∈ domϕ and any u ∈ Rn, the following inequality holds |∇3ϕ(x)[u, u, u]| ≤2∥u∥3/2 x , ∥∇ϕ(x)∥∗ x ≤ √ν where ∥v∥x := ∥v∥∇2ϕ(x) and ∥v∥∗ x := ∥v∥∇2ϕ(x)−1 , for any vector v. Then, the function ϕ is called as a ν self-concordant barrier for K, where K = domϕ. Remark 3.3. In general, ν ≥ 1 for any self-concordant barrier function. [Nes98] demonstrates that for any open convex set K contained in the Euclidean space Rn, there exists a O(n) self-concordant barrier function. We focus on a specific convex set Ki which has a dimension of O(1) in this paper. We make the assumption that a νi self-concordant barrier function ϕi is given, and we can efficiently compute its gradient ∇ϕi and Hessian ∇2ϕi in constant time (O(1)). An important result we rely on regarding self-concordance is the stability of the norm ∥ · ∥x when we alter the value of x. Subsequently, we proceed to present certain properties of the self-concordant barrier function. Theorem 3.4 (Theorem 4.1.6 in [Nes98]) . If the following conditions hold • Suppose ϕ represents a self-concordant barrier function. • the norm ∥y − x∥x is less than 1 Then, the following inequalities hold true: ∇2ϕ(y) ⪰ (1 − ∥y − x∥x)2∇2ϕ(x) and ∇2ϕ(y) ⪯ (1 − ∥y − x∥x)−2∇2ϕ(x). Now, we consider the way to go alone with the path from x(1) to x(ϵ), where ϵ ∈ (0, 1), in the next section. 3.4 Newton Step In this section, we briefly introduce the Newton method in central path. It is a standard method, for details of the background, the readers could refer [NW06]. In order to follow the path from x(1) to x(ϵ) and control in error that caused in the progress, we consider the following problem s/t + ∇ϕ(x) = µ Ax = b A⊤y + s = tµ + c 6where ∇ϕ(x) = (∇ϕ1(x1), ϕ2(x2), ··· , ∇ϕm(xm)) and µ stands for the error that is caused in the progress. In order to control the error, the Newton step to move from µ to µ + h is given below: δ∗ s /t + ∇2ϕ(x) · δ∗ x = h Aδ∗ x = 0 A⊤δ∗ y + δ∗ s = 0 where ∇2ϕ(x) = diag(∇2ϕ1(x1), ∇2ϕ2(x2), ··· , ∇2ϕm(xm)). Then, we define that W := (∇2ϕ(x))−1 and we define the projection matrix P ∈ Rn×n below: P := W1/2A⊤(AW A⊤)−1AW1/2 (3) We could get the following solutions: δ∗ x = W1/2(I − P)W1/2h, δ ∗ y = −t · (AW A⊤)−1AW h, δ∗ s = t · W−1/2P W1/2h 4 IPM under FL In this section, we develop the interior point methods under FL. Before we introduce our algorithm, we first introduce the sketching technique used in section 4.1. Then, we give the overview of our algorithm in section 4.2. 4.1 Sketching Techniques In this subsection, we give the definition of AMS matrix [AMS99] and show the statistical properties of using the AMS matrix to sketch a fixed vector. See Appendix A for rigorous proof. Definition 4.1 (AMS sketch matrices [AMS99]) . Let h1, h2, . . . , hb be b random hash functions. The hash functions are picked from a 4-wise independent hash familyH = {h : [n] → {−1/ √ b, +1/ √ b}}. Then R ∈ Rb×n is an AMS sketch matrix if we set Ri,j = hi(j). The AMS matrix has great statistical properties to sketch a fixed vector. We provide the statement in the following lemma, which is standard in literature [LSZ19, SY21]. Lemma 4.2 (Statistical properties for sketching a fixed vector) . If the following conditions hold • h ∈ Rn is a fixed vector. • R is defined as in Definition 4.1. Then we have E[R⊤Rh] = h, E[(R⊤Rh)2 i ] ≤ h2 i + 1 b∥h∥2 2 Pr \u0014 |(R⊤Rh)i − hi| > ∥h∥2 log(n/δ)√ b \u0015 ≤ δ Note. Although AMS sketch matrix is also used in [LSZ19], there exists some difference between our paper and the previous work: Both of our work and that use AMS sketch matrix. However, the previous work uses the AMS sketch matrix outside of the projection matrix P to accelerated the whole process, where R ∈ Rb×n is an AMS matrix and P ∈ Rn×n is a projection matrix. And we add the AMS sketch matrix inside the projection matrix. The projection matrix with AMS sketch matrix in our paper is defined in Def. 4.3. There is a major issue we need to tackle: how to bound the error that is caused by adding sketching matrices outside the inverse part ( AW A⊤)? 7×𝑊!/# 𝐴⏉ × × × -1 × ×𝐴 𝑊!/#𝐴 𝑊 𝐴⏉𝑅! ⏉ 𝑅!× × 𝑅# 𝑅# ⏉× ×𝑅% ⏉ 𝑅%×× 𝑅&× ×𝑅& ⏉𝑃\" = ×𝑊!/# 𝐴⏉ × × -1 ×𝐴 𝑊!/#𝐴 𝑊 𝐴⏉𝑅! ⏉ 𝑅!× × 𝑅# 𝑅# ⏉× ×× ×𝑃$ = ×𝑊!/# 𝐴⏉ × × -1 ×𝐴 𝑊!/#𝐴 𝑊 𝐴⏉× ×𝑃 = Figure 2: P is an ideal case of the projection matrix. However, it is infeasible to construct P under FL. In view of this, we construct eP. In order to analyze that P is close to eP, which means that ∥g⊤P h−g⊤ eP h∥ could be bounded by g, h, A, and W. We create an artificial matrix bP. Note that bP is only used in analysis. The way how our analysis working is, we first show that P is close bP, then we show bP is close to eP. Combining two steps together, we finally prove that P is close to eP. 4.2 Our Algorithm In view of the properties of AMS sketch matrices, we could use AMS sketch matrices to bound the error caused by the sketching techniques. Next, we define the following notations to differentiate the projection matrices used in IPM: Definition 4.3 ( bP and eP). Given four independent AMS matrices, R1 ∈ Rb1×d, R2 ∈ Rb2×d, R3 ∈ Rb3×d, R4 ∈ Rb4×d, the matrix bP and eP are defined as below: bP = W1/2A⊤(R⊤ 1 R1AW A⊤R⊤ 2 R2)−1AW1/2 and eP = W1/2A⊤R⊤ 3 R3(R⊤ 1 R1AW A⊤R⊤ 2 R2)−1R⊤ 4 R4AW1/2 We want to remark that bP is only being used for the purpose of analysis. But, we use eP for both analysis and algorithm. The algorithm to address ERM under FL could be divided into several steps (Fig. 4 gives an overview of the algorithm): Setup. First, we give server and each client the same random seed. Then, for each client ci, the client generates four independent sketching matrices R1, R2, R3, and R4. Local update. For any i ∈ [m], the detailed process of local update in each client ci is shown as below: • Each client ci updates xt−1 i and st−1 i by gradient descent δt−1 xi and δt−1 si respectively. Then, we get that xt i = xt−1 i + δt−1 xi and st i = st−1 i + δt−1 si . • Each client ci computes Wt i . • Each client ci computes µt i(x, s) = st i/et + ∇ϕi(xi) and γt i (x, s) = ∥µt i(x, s)∥∇2ϕi(xt i)−1 . • Each client ci computes ht i = −α · ct i(x, s)µt i(x, s). • Each client ci sends its (Wt i )1/2A⊤ i R⊤ 1 , R2AiWt i A⊤ i R⊤ 3 , R4Ai(Wt i )1/2 and ht i to the server. 8Algorithm 1 Second-Order Algorithm for Empirical Risk Minimization in Federated Learning (Theorem 1.1) 1: Both Server and Client have the random seed for sketching matrix R1, R2, R3, and R4. 2: Initial δ0 x ← 0, δ0 s ← 0, λ ← 216 log(m), α ← 2−20λ−2, ξ ← 2−10α, et ← 1. 3: for t = 1 → T do 4: /* Client */ 5: for i = 1 → N do 6: /* Update local parameters for x and s */ 7: xt i ← xt−1 i + δt−1 xi . 8: st i ← st−1 i + δt−1 si . 9: Wt i ← (∇2ϕi(xt i))−1. 10: Compute µt i(x, s) ← si/et + ∇ϕi(xt i) 11: Compute γt i (x, s) ← ∥µt i(x, s)∥∇2ϕi(xt i)−1 12: Compute ct i(x, s) ← ( exp(λγt i (x,s))/γt i (x,s) (Pm i=1 exp(2λγt i (x,s)))1/2 if γt i (x, s) ≥ 96√α 0 otherwise 13: Compute ht i ← −α · ct i(x, s)µt i(x, s) 14: Send (Wt i )1/2A⊤ i R⊤ 1 , R2AiWt i A⊤ i R⊤ 3 , R4Ai(Wt i )1/2 and ht i to server. 15: end for 16: /* Server */ 17: Construct ht ← ⊕m i=1ht i ▷ ht ∈ Rn 18: Construct ePt ← (Wt)1/2A⊤R⊤ 1 R1(R⊤ 2 R2AWtA⊤R⊤ 3 R3)−1R⊤ 4 R4A(Wt)1/2 ▷ eP ∈ Rn×n 19: Compute δt x ← (Wt)1/2(I − ePt)(Wt)1/2ht. ▷ δt x ∈ Rn 20: Compute δt s ← et · (Wt)−1/2 ePt(Wt)1/2ht. ▷ δt s ∈ Rn 21: Send δt x and δt s to every client. 22: et = (1 − ξ√ν )et. 23: end for 24: Return an approximation solution to the convex problem. Global update. In each global communication round, the detailed process of global update is shown as below: • The server constructs eP as below eP = W1/2A⊤R⊤ 1 R1(R⊤ 2 R2AW A⊤R⊤ 3 R3)−1R⊤ 4 R4AW1/2 (4) • The server computes δt x and δt s as below: δt x = W1/2(I − eP)W1/2ht, δ t s = et · W−1/2 eP W1/2ht • The server sends δt x and δt s to every client. Communication cost. In this paper, we always assume that d ≥ n. The Algorithm 1 seeds O(bmaxn) words at each iteration, where bmax = max{b1, b2, b3, b4}. Generally, we choose bmax = O(√n). Compared with the naive algorithm mentioned in Model 3, the Algorithm 1 is more practical due to the reason that FL is always limited by the network bandwidth. 95 Theoretical Analysis In our algorithm, the main problem is how to handle the matrix P that is defined as Eq. (3), which is used in our IPM where W ∈ Rn×n is a block diagonal matrix and A ∈ Rd×n. The core of Theorem 5.3 is to show the equation |g⊤P h− g⊤ eP h| could be bounded by g, h, A, and W. In order to prove Theorem 5.3, we divide the proof into the following steps. Given two vectors g, h∈ Rd. (In the following statement and proof, we assume that g = h). We want to prove that • |g⊤P h− g⊤ bP h| could be bounded by g, h, A and W. We prove this by using Lemma C.3 with C = W1/2A⊤, B = (AW A⊤)−1, R = R1 and S = R2. • |g⊤ eP h− g⊤ bP h| could be bounded by g, h, A, W, and eB, we prove this by using Lemma C.5 with C = W1/2A⊤, eB = (R⊤ 1 R1AW A⊤R⊤ 2 R2)−1, R = R3, and S = R4. • Finally, we could use |g⊤P h− g⊤ eP h| ≤ |g⊤P h− g⊤ bP h| + |g⊤ bP h− g⊤ eP h| to obtain that |g⊤P h− g⊤ eP h| is bounded by g, h, A and W. In order to achieve the above-mentioned steps, we need to use the following lemmas. The detailed proof of the following lemma is deferred to Appendix C. Lemma 5.1. If the following conditions hold • eB ∈ Rd×d and C ∈ Rn×d are two matrices. • R ∈ Rb1×d and S ∈ Rb2×d are defined as in Definition 4.1. • g ∈ Rn and h ∈ Rn are vectors. • bmin = {b1, b2}. Then, we have g⊤C(R⊤R) eB(S⊤S)C⊤h − g⊤C eBC⊤h ≲ K0, with probability at least 1 − 1/ poly(n) and K0 is defined as follows: K0 := log1.5 d√bmin · (∥g⊤C∥2∥ eBC⊤h∥2 + ∥g⊤C eB∥2∥C⊤h∥2) + log3 d bmin · ∥g⊤C∥2∥C⊤h∥2∥ eB∥F By using the above lemma, we could obtain the following result by settingeB = (R⊤ 1 R1AW A⊤R⊤ 2 R2)−1 and C = W1/2A⊤, where both R1 and R2 are independent AMS matrices. |g⊤ eP h− g⊤ bP h| ≲ K0. Although the above lemma could be used to bound the error of the second step. However, it does not show that eB could be bounded by A and W. It is non-trivial to prove it. In order to bound the error, we first use the above lemma to obtain that |x⊤R⊤RB−1S⊤Sx − x⊤B−1x| ≤ϵ0λmin(B−1) where bmin = {b1, b2}, κ = λmax(B)/λmin(B), ϵ0 = O(√n log3 d/bmin)κ ∈ (0, 1/10), R ∈ Rb1×d and S ∈ Rb2×d are matrices defined as in Definition 4.1 and B = (AW A⊤)−1. Then we use the following lemma to bound the error that is caused by adding sketching matrices in the inverse part. 10Lemma 5.2. If the following conditions hold: • B ∈ Rd×d is a matrix. • R ∈ Rb1×d and S ∈ Rb2×d are defined as in Definition 4.1. • g ∈ Rn and h ∈ Rn are vectors. • ϵ0 ∈ (0, 1/10). Then, we have (1 − 2ϵ0)B ⪯ (R⊤RB−1S⊤S)−1 ⪯ (1 + 2ϵ0)B with probability at least 1 − 1/ poly(n). By using the above lemma, we could obtain that |g⊤P h− g⊤ bP h| ≤2ϵ0∥g⊤C∥2∥C⊤h∥2∥B∥2 with probability 1 − 1/ poly(n), where C = W1/2A⊤ and B = (AW A⊤)−1 ∈ Rd×d. Finally, we combine the result of Lemma 5.1 and Lemma 5.2 together to get the following theorem. Theorem 5.3. If the following conditions hold • Given A ∈ Rd×n and W ∈ Rn×n. • Let R1 ∈ Rb1×d, R2 ∈ Rb2×d, R3 ∈ Rb3×d and R4 ∈ Rb4×d be four independent AMS matrices. • P is defined as in Eq. (3). • eP is defined as in Def. 4.3. • Let g, h∈ Rn be two vectors. • Let bmin := min{b1, b2}. Then, we have |g⊤P h− g⊤ eP h| ≲ log6 d · ( 1√bmin + n b2 min ) · κ · ∥g⊤C∥2∥C⊤h∥2∥B∥2 with probability at least 1 − 1/ poly(n). Note that C = W1/2A⊤, B = ( AW A⊤)−1, and κ = λmax(B)/λmin(B). Due to the reason that κ >1, we could choose that bmin = ϵ−1√nκ2 log3 d and ϵ ∈ (0, 1/10). Then, we could obtain that |g⊤P h− g⊤ eP h| ≤ϵ∥g⊤C∥2∥C⊤h∥2∥B∥2. 116 Compared to Standard Methods In order to show the effectiveness and efficiency of our algorithm, we discuss the following three naive models and point out the disadvantages of each model respectively. We introduce the following three straightforward methods: Model 1 and Model 2 cannot get the right result under their respective framework. Model 3 can get the correct result, but it needs to send O(n2) words at each iteration. Moreover, Model 3 also requires clients to share their data with the untrusted server, which is not allowed under FL setting. Model 1: In the t-th step, each client does the following operations: (1) Compute Wt i and hi; (2) Compute local Pt i , where Pt i = (Wt i )1/2A⊤ i (AiWt i A⊤ i )−1Ai(Wt i )1/2; (3) By using local Pt i and hi, the client could compute local update δx,i and δs,i; (4) Finally, client sends its local update δx,i and δs,i to the server. The Server combines all gradients together. However, the main issue is that ⊕m i=1[(Wt i )1/2(I − Pt i )(Wt i )1/2hi] ̸= (Wt)1/2(I − Pt)(Wt)1/2h and ⊕m i=1[(Wt i )−1/2Pt i (Wt i )1/2hi] ̸= (Wt)−1/2Pt(Wt)1/2h where P = (Wt)1/2A⊤(AWtA⊤)−1A(Wt)1/2, and I is an identify matrix. Model 2: In the t-th step, each client does the following operations: (1) Compute Wt i and hi locally; (2) Send (AiWt i A⊤ i )−1 and hi to the server. However, this method does not work well. The reason is that ⊗m i=1 [(Wt i )1/2A⊤ i (AiWt i A⊤ i )−1Ai(Wt i )1/2] ⊕m i=1 hi ̸= [(Wt)1/2A⊤(AWtA⊤)−1A(Wt)1/2]h where Wt = ⊗m i=1Wt i , A= ⊕m i=1Ai, and h = ⊕m i=1hi. Model 3: Each client sends data to the server at the 0-th iteration. Then, in the t-th step, each client does the following operations: (1) Compute locally Wt i and hi; (2) Send Wt i and hi to the server. The server computes P by the following equation: P = (⊗m i=1(Wt i )1/2A⊤)(A ⊗m i=1 Wt i A⊤)−1(A ⊗m i=1 (Wt i )1/2) Compared to the above-mentioned two models, this method could get the correct result in the end. However, it has to send O(n2) words at each iteration. In reality, the distributed machine learning is always limited by the network bandwidth. Moreover, people usually are not willing to share their private data with the untrusted system because of data privacy. In view of this, we propose a communication-efficient distributed interior point method under FL. 7 Conclusion and Discussion In a nutshell, we present the first distributed interior point method algorithm ( FL-IPM) that is used to address empirical risk minimization under FL. There are differences between our algorithm and existing algorithms and the novelty of our work is shown below: (1) There exist a large number of works related to the distributed first-order optimization algorithms. However, our algorithm is 12a second-order optimization problem under federated learning settings. (2) We use the sketching technique to reduce the communication cost of federated learning, which is the bottleneck of fed- erated learning. (3) Compared with the existing distributed second-order optimization algorithms, we can provide convergence analysis for our solution without making strong assumptions. As for future work, there are several things we need to consider, if we want to apply our algorithm in the real system: First, we need to consider the stragglers and device heterogeneity in the real system environment. We need to design robust algorithms to deal with stragglers during the training. In addition, the scalability of large networks is also very important, especially the latency and throughput of the network. Finally, the computational cost of the devices and server should be taken into consideration. We present theoretical results in this paper, and we are not aware of any negative societal impact. 13Appendix Roadmap. The structure of the appendix is outlined as follows: • Section A claims the probability tools used in this paper and shows the properties of random sketching matrix. • Section B presents how to bound the error of adding two sketching matrices. • Section C shows that |g⊤P h− g⊤ eP h| is small. • Section D presents the primary outcome of this paper along with its corresponding proof. • Section E shows several basic results of Algorithm 1. • Section F states some basic results of self-concordance function. A Probability Tools and Basic Properties of Random Sketching Matrices In this paper, we care less about the running time of each client in our application. The issue we want to address in this paper is the limitation of the network bandwidth (bandwidth between server and clients). In view of this, we use subsampled randomized Hadamard/Fourier matrix1 and AMS matrices. The basic ideas of handling randomness in sketching matrices have been used in a number of previous work [PSW17, LSZ19, JSWZ21, SY21, SYYZ23]. However, in our case, we have more different sketching matrices and also need to apply sketching matrices inside inversion. In Section A.1, we introduce the concentration inequalities. In Section A.2, we present the properties obtained from random projection. A.1 Concentration inequalities We first state several useful inequalities. Lemma A.1 (Lemma 1 on page 1325 of [LM00]) . If the following conditions hold • X ∼ X2 k is a random variable, which is a chi-squared distribution and has k degrees of freedom. • Each of them has a mean of 0 and a variance of σ2. Then, we have Pr[X − kσ2 ≥ (2 √ kt + 2t)σ2] ≤ exp(−t) and Pr[kσ2 − X ≥ 2 √ ktσ2] ≤ exp(−t). 1We want to remark that SRHT has fast computation advantage compared to AMS. Using SRHT [LDFU13] allows multiplying the matrix withk vectors only takeskn log n time. This is much faster compared to AMS. In our application, we only use nice statistical properties of SRHT matrices without using any fast Fourier transform [CT65], or more fancy sparse Fourier transform [HIKP12b, HIKP12a, Pri13, IKP14, IK14, PS15, CKPS16, Kap16, Kap17, NSW19, GSS22, SSWZ22a, SSWZ22b]. 142 4 6 8 Variable(x) 5 4 3 2 1 Barrier Function (ln(1 + x) + ln(2x)) Figure 3: An example of the barrier function: −(ln(1 +x) + ln(2x)). The variable is changed from 0.5 to 10. In this case, A = [−1, −2]⊤, and b = [1, 0]⊤. Lemma A.2 (Khintchine’s Inequality). If the following conditions hold • σ1, ··· , σn are the independent and identically distributed sign random variables. • z1, ··· , zn are real numbers. Then, there exists positive constants, namely C and C′, satisfying that: Pr \"\f\f\f\f\f nX i=1 ziσi \f\f\f\f\f ≥ Ct∥z∥2 # ≤ exp(−C′t2) Lemma A.3 (Bernstein Inequality). If the following conditions hold • X1, ··· , Xn is a set of independent random variables with zero means. • For any arbitrary 1 ≤ i ≤ n, let the absolute value of each Xi is almost surely bounded by a constant M. Then, for any positive value t, the following inequality holds: Pr \" nX i=1 Xi > t # ≤ exp   − t2/2Pn j=1 E[X2 j ] + Mt/3 ! A.2 Properties obtained by random projection Here, we formally define the SRHT matrix and AMS sketching matrix and analyze their properties. Definition A.4 (Subsampled randomized Hadamard/Fourier transform (SRHT) matrix [LDFU13]). The SRHT matrix, denoted as R = p n/b · SHD , where R ∈ Rb×n, and S ∈ Rb×n represents a random matrix whose rows are b uniform samples (without replacement) from the standard basis of Rn, H ∈ Rn×n is a normalized Walsh-Hadamard matrix, and D ∈ Rn×n is a diagonal matrix whose diagonal elements are i.i.d. Rademacher random variables. 15Definition A.5 (AMS sketch matrix [AMS99]) . Let h1, h2, . . . , hb be b random hash functions picking from a 4-wise independent hash family H = {h : [n] → {−1/ √ b, +1/ √ b}}. Then, R ∈ Rb×n is a AMS sketch matrix if we set Ri,j = hi(j). Lemma A.6 (Lemma E.5 in [LSZ19]) . If the following conditions hold • Let h ∈ Rn be a fixed vector. • Let R ∈ Rb×n be a SRHT or AMS sketch matrix as in Definition A.4 and A.5. Then, we have E[R⊤Rh] = h, E[(R⊤Rh)2 i ] ≤ h2 i + 1 b∥h∥2 2 Pr \u0014 |(R⊤Rh)i − hi| > ∥h∥2 log(n/δ)√ b \u0015 ≤ δ B Sketch more than once Receive Approximate Global Hessian Send Partial Local Hessian Client Client Client Server Figure 4: This is an overview of our framework. In our framework, there is no need for clients to share data with the server. Clients share partial Hessian information with the server. And the server computes the update information by using Hessian information, then sends the update information to the client. Now, we can bound the error of adding two sketching matrices. Lemma B.1 (Error bound of adding two sketching matrices) . If the following conditions hold • R ∈ Rb1×n, S∈ Rb2×n are defined as in Def. A.5. • B ∈ Rn×n is a matrix. • u, v∈ Rn are vectors. Then, with probability 1 − 1/ poly(n), |u⊤R⊤RBS⊤Sv − u⊤Bv| ≲ log1.5 n√b1 · ∥u∥2∥Bv∥2 + log1.5 n√b2 · ∥u⊤B∥2∥v∥2 + log3 n√b1b2 · ∥u∥2∥v∥2∥B∥F . holds. 16Proof. Let i be in [n]. Let the i-th column of R be Ri ∈ Rb1 . Let the i-th column of S be Si ∈ Rb2 . Let σi be a random sign. Let R be an AMS matrix. Every column Ri of R follows the same distribution as σiRi. We have that R satisfies: 1. ⟨Ri, Ri⟩ = 1, ∀i ∈ [n]. (5) 2. Pr[⟨Ri, Rj⟩ ≤ p log(n/δ)√b1 , ∀i ̸= j ∈ [n]] ≥ 1 − δ. (6) Likewise, S is an AMS matrix, and the distribution of each column Si of S is identical to σ′ iSi, where σ′ i represents a random sign. Additional information can be found in [AMS99]. Then, we can get u⊤(R⊤R)B(S⊤S)v = X i,j,i′,j′ uivj′σiσjσ′ i′σ′ j′⟨Ri, Rj⟩Bj,i′⟨Si′, Sj′⟩ (7) Therefore, we can divide the summation in equation Eq. (7) into three components: 1. The first part involves two pairs of indices being identical: j = i and j′ = i′. 2. The second part occurs when one pair of indices is the same: either j = i and j′ ̸= i′, or conversely, j ̸= i and j′ = i′. 3. The third part arises when no pair of indices are the same: j ̸= i and j′ ̸= i′. Proof of Part 1. Suppose j = i and j′ = i′. We can get X i=j,i′=j′ uivj′σiσjσ′ i′σ′ j′⟨Ri, Rj⟩Bj,i′⟨Si′, Sj′⟩ = X i,i′ uivi′Bi,i′ = u⊤Bv (8) For the first step, we use the fact that ⟨Ri, Ri⟩ = ⟨Si′, Si′⟩ = 1 for all i and i′ in [n], as shown in Eq. (5). Proof of Part 2. Suppose that either j = i and j′ ̸= i′, or conversely, j ̸= i and j′ = i′. Without loss of generality, we suppose j = i and j′ ̸= i′. Then, we can get X i=j,i′̸=j′ uivj′σiσjσ′ i′σ′ j′⟨Ri, Rj⟩Bj,i′⟨Si′, Sj′⟩ = X i,i′̸=j′ uivj′σ′ i′σ′ j′Bi,i′⟨Si′, Sj′⟩ 17= X j′ σ′ j′vj′ X i′̸=j′ σ′ i′(B⊤u)i′⟨Si′, Sj′⟩, For the first step, we use the fact that ⟨Ri, Ri⟩ = 1 for all i in [n], as shown in Eq. (5). For the second step, we use P i uiBi,i′ = (B⊤u)i′. By the Union bound and Lemma A.2, we can get ( X j′ σ′ j′vj′ X i′̸=j′ σ′ i′(B⊤u)i′⟨Si′, Sj′⟩)2 ≲ log n · X j′ v2 j′( X i′̸=j′ σ′ i′(B⊤u)i′⟨Si′, Sj′⟩)2 ≲ log2 n · X j′ v2 j′ X i′̸=j′ (B⊤u)2 i′⟨Si′, Sj′⟩2 ≲ log3 n/b2 · X j′ v2 j′ X i′̸=j′ (B⊤u)2 i′ ≲ log3 n/b2 · ∥v∥2 2∥B⊤u∥2 2, with probability at least 1 − 1/ poly(n), where the first step follows from t = O(√log n) and Lemma A.2, the second step is obtained by t = O(√log n) and Lemma A.2 again, and the third step is derived from Eq. (6). Combining the previous two equations, and considering the symmetry of the case where i′ = j′ and i ̸= j, we can get that X i=j,i′̸=j′ or i′=j′,i̸=j uivj′σiσjσ′ i′σ′ j′⟨Ri, Rj⟩Bj,i′⟨Si′, Sj′⟩ ≲ log1.5 n/ p b1 · ∥u∥2∥Bv∥2 + log1.5 n/ p b2 · ∥u⊤B∥2∥v∥2 (9) with a probability of at least 1 − 1/ poly(n). Proof of Part 3. Suppose j ̸= i and j′ ̸= i′. We can show ( X i̸=j,i′̸=j′ uivj′σiσjσ′ i′σ′ j′⟨Ri, Rj⟩Bj,i′⟨Si′, Sj′⟩)2 = ( X i σiui X j′ σ′ j′vj′ X i′̸=j′ σ′ i′⟨Si′, Sj′⟩ X j̸=i σi⟨Ri, Rj⟩Bj,i′)2 ≲ log n · X i u2 i ( X j′ σ′ j′vj′ X i′̸=j′ σ′ i′⟨Si′, Sj′⟩ X j̸=i σi⟨Ri, Rj⟩Bj,i′)2 ≲ log2 n · X i u2 i X j′ v2 j′( X i′̸=j′ σ′ i′⟨Si′, Sj′⟩ X j̸=i σi⟨Ri, Rj⟩Bj,i′)2 ≲ log3 n · X i u2 i X j′ v2 j′ X i′̸=j′ ⟨Si′, Sj′⟩2( X j̸=i σi⟨Ri, Rj⟩Bj,i′)2 ≲ log4 n · X i u2 i X j′ v2 j′ X i′̸=j′ ⟨Si′, Sj′⟩2 X j̸=i ⟨Ri, Rj⟩2B2 j,i′ ≲ log6 n/(b1b2) · ∥u∥2 2∥v∥2 2∥B∥2 F , 18with probability 1 − 1/ poly(n), where 2nd step follows from t = O(√n) and Lemma A.2, the 3rd comes from t = O(√n), Lemma A.2, for all i ∈ [n], and employing the Union bound to combine the n inequalities, the 4th and 5th step can be justified based on the same reasoning as the 3rd step. For the 6th step, we use the fact that for all i′ ̸= j′ ∈ [n] and i ̸= j ∈ [n], with a probability of at least 1 − 1/ poly(n), ⟨Si′, Sj′⟩ ≲ p (log n)/b2 and ⟨Ri, Rj⟩ ≲ p (log n)/b1. For all i, j, i′, and j′ in [n], we apply the Union bound to combine 2 n2 such bounds. Therefore, we can get X i̸=j,i′̸=j′ uivj′σiσjσ′ i′σ′ j′⟨Ri, Rj⟩Bj,i′⟨Si′, Sj′⟩ ≲ log3 n/ p b1b2 · ∥u∥2∥v∥2∥B∥F . (10) with probability at least 1 − 1/ poly(n). Combining Part 1, Part 2, and Part 3. First, we add Eq. (8), (9), and (10) together. Then, we plug their sum into Eq. (7). Finally, through Union bound, we can get u⊤(R⊤R)B(S⊤S)v − u⊤Bv ≲ log1.5 n√b1 · ∥u∥2∥Bv∥2 + log1.5 n√b2 · ∥u⊤B∥2∥v∥2 + log3 n√b1b2 · ∥u∥2∥v∥2∥B∥F , with probability at least 1 − 1/ poly(n). Therefore, we complete the proof. C Bounding error of sketching This section is arranged as follows: • Section C.1 gives the definition of P, bP, and eP. • Section C.2 presents the steps to prove that P ≈ eP. • Section C.3 shows that |g⊤P h− g⊤ bP h| is small. • Section C.4 presents the tools that we use to bound |g⊤P h− g⊤ bP h|. • Section C.5 shows that |g⊤ eP h− g⊤ bP h| is small. • Section C.6 presents the tools that we use to bound |g⊤ eP h− g⊤ bP h|. • Section C.7 shows that |g⊤P h− g⊤ eP h| is small by combining the result of |g⊤P h− g⊤ bP h| and |g⊤ eP h− g⊤ bP h|. 19C.1 Definition of P, bP, and eP In this section, we formally define P, bP, and eP. Definition C.1 (Definition of Projection Matrices) . We define P ∈ Rn×n, bP ∈ Rn×n, and eP ∈ Rn×n as follows: P := W1/2A⊤(AW A⊤)−1AW1/2 bP := W1/2A⊤(R⊤ 1 R1AW A⊤R⊤ 2 R2)−1AW1/2 eP := W1/2A⊤R⊤ 3 R3(R⊤ 1 R1AW A⊤R⊤ 2 R2)−1R⊤ 4 R4AW1/2 where R1 ∈ Rb1×d, R2 ∈ Rb2×d, R3 ∈ Rb3×d, and R4 ∈ Rb4×d are sketching matrices. Among them, P is the ideal case of the projection matrix. eP is the projection matrix we use under FL. We construct bP to analyze that |g⊤P h− g⊤ eP h| is bounded by g, h, A, and W, for any g ∈ Rn and h ∈ Rn. C.2 Proof sketch In this section, we show that P ≈ eP. Our goal is to show that |g⊤P h− g⊤ eP h| is bounded by g, h, A and W. We split it into following steps. For any two vectors g, h∈ Rd, we want to prove that • |g⊤P h− g⊤ bP h| is small, we prove this by using Lemma C.3 with – C = W1/2A⊤, – B = (AW A⊤)−1, – R = R1, and – S = R2. • |g⊤ eP h− g⊤ bP h| is small, we prove this by using Lemma C.5 with – B = (R⊤ 1 R1AW A⊤R⊤ 2 R2)−1, – C = W1/2A⊤, – R = R3, and – S = R4. • |g⊤P h− g⊤ eP h| is small, we could prove it by using – |g⊤P h− g⊤ eP h| ≤ |g⊤P h− g⊤ bP h| + |g⊤ bP h− g⊤ eP h|, – ∥ eBC⊤h∥2 ≤ ∥eB∥2∥C⊤h∥2 ≤ (1 + ϵ0)∥B∥2∥C⊤h∥2, and – ∥ eB∥F ≤ √n∥ eB∥2 ≤ (1 + ϵ0)√n∥B∥2 20C.3 Bounding |g⊤Ph − g⊤ bPh| The goal of this section is to prove the following lemma to indicate that we could bound |g⊤P h− g⊤ bP h|. Note that we assume that g = h in this lemma. However, in order to make other lemma more general, we do not assume that g = h in other lemma in this section. Lemma C.2 (P and bP are close). If the following conditions hold • Let g ∈ Rn and h ∈ Rn be two vectors. • Let ϵ0 ∈ (0, 1/10). Then, we have |g⊤P h− g⊤ bP h| ≤2ϵ0g⊤CBC ⊤h ≤ 2ϵ0∥g⊤C∥2∥C⊤h∥2∥B∥2 with probability at least 1 − 1/ poly(n), where C = W1/2A⊤ and B = (AW A⊤)−1 ∈ Rd×d. Proof. We assume that f(B, R, S) = R⊤RB−1S⊤S. By using Lemma C.3, we could obtain that (1 − 2ϵ0)B ⪯ (f(B, R, S))−1 ⪯ (1 + 2ϵ0)B Then, for any two vectors g, h∈ Rd, we could obtain that (1 − 2ϵ0)g⊤CBC ⊤h ≤ g⊤C(f(B, R, S))−1C⊤h ≤ (1 + 2ϵ0)g⊤CBC ⊤h According to the above inequality, it is easy for us to get that |g⊤P h− g⊤ bP h| ≤2ϵ0g⊤CBC ⊤h. We could obtain that g⊤CBC ⊤h ≤ p g⊤CBC ⊤g · √ h⊤CBC ⊤h = ∥g⊤C∥2∥C⊤h∥2∥B∥2 by using the Cauchy-Schwarz inequality. Therefore, we could get that |g⊤P h− g⊤ bP h| ≤2ϵ0∥g⊤C∥2∥C⊤h∥2∥B∥2 This finishes the proof. C.4 Tools for bounding |g⊤Ph − g⊤ bPh| In this section, we present the tools for bounding |g⊤P h− g⊤ bP h|. Lemma C.3 (Tools for showing P and bP are close). If the following conditions hold • R ∈ Rb1×d, S ∈ Rb2×d are defined as in Definition 4.1. • g, h∈ Rn are two vectors. Then, we have that (1 − 2ϵ0)B ⪯ (R⊤RB−1S⊤S)−1 ⪯ (1 + 2ϵ0)B with probability at least 1 − 1/ poly(n), where ϵ0 ∈ (0, 1/10), and B ∈ Rd×d. 21Proof. Given any x ∈ Rd such that ∥x∥2 = 1, we could use Lemma B.1 to prove that |x⊤R⊤RB−1S⊤Sx − x⊤B−1x| ≤ϵ0λmin(B−1), where bmin = {b1, b2}, κ = λmax(B)/λmin(B) and ϵ0 = O(√n log3 d/bmin)κ. Then, we have to prove two cases: Case 1: From|x⊤R⊤RB−1S⊤Sx−x⊤B−1x| ≤ϵ0λmin(B−1), we could get thatλmax(R⊤RB−1S⊤S− B−1) ≤ ϵ0κλmin(B−1). Then, we could the following derivation process: 0 ≥ λmax(R⊤RB−1S⊤S − B−1) − ϵ0λmin(B−1) ≥ λmax(R⊤RB−1S⊤S − (1 + ϵ0)B−1) where the first step holds, because of we use Lemma B.1 to obtain the intermediate result. And the second step holds due to the properties of eigenvalue of the matrix. Finally, we could obtain that R⊤RB−1S⊤S ⪯ (1 + ϵ0)B−1 Case 2: From |x⊤R⊤RB−1S⊤Sx − x⊤B−1x| ≤ϵ0λmin(B−1), we could get that λmin(R⊤RB−1S⊤S − B−1) ≥ −ϵ0κλmin(B−1). Then, we could the following equation: 0 ≤ λmin(R⊤RB−1S⊤S − B−1) + ϵ0λmin(B−1) ≤ λmin(R⊤RB−1S⊤S − (1 − ϵ0)B−1) where the first step holds, because we use Lemma B.1 to obtain the intermediate result. The second step holds because of the properties of eigenvalue. Finally, according to the above equation, we could obtain that (1 − ϵ0)B−1 ⪯ R⊤RB−1S⊤S Combining two above results, due to the reason that ϵ0 = O(√n log3 d/bmin)κ, we could get that (1 − ϵ0)B−1 ⪯ R⊤RB−1S⊤S ⪯ (1 + ϵ0)B−1 for any vector x ∈ Rn and ∥x∥2 = 1. Finally, we could choose bmin = ϵ−1√nκ2 log3 d, where ϵ ∈ (0, 1/10), to make ϵ0 ∈ (0, 1/10). This finishes the proof. 22C.5 Bounding |g⊤ ePh − g⊤ bPh| We show that |g⊤ eP h− g⊤ bP h| can be bounded. Lemma C.4 ( eP and bP are close). If the following conditions hold • eB = (R⊤ 1 R1AW A⊤R⊤ 2 R2)−1. • C = W1/2A⊤. • Let g ∈ Rn and h ∈ Rn be two vectors. • Let R1 ∈ Rb1×d, R2 ∈ Rb2×d are two sketching matrices. • Let bmin = {b1, b2}. Then, We have |g⊤ eP h− g⊤ bP h| ≲ log1.5 d√bmin · (∥g⊤C∥2∥ eBC⊤h∥2 + ∥g⊤C eB∥2∥C⊤h∥2) + log3 d bmin · ∥g⊤C∥2∥C⊤h∥2∥ eB∥F with probability at least 1 − 1/ poly(n). Proof. We could using Lemma C.5 to prove the above lemma. By settingeB = (R⊤ 1 R1AW A⊤R⊤ 2 R2)−1, and C = W1/2A⊤ where R1 ∈ Rb1×d and R2 ∈ Rb2×d are two sketching matrices. C.6 Tools for Bounding |g⊤ ePh − g⊤ bPh| We present the tools for bounding |g⊤ eP h− g⊤ bP h|. Lemma C.5 (Tools for showing eP and bP are close). If the following conditions hold • Let eB ∈ Rd×d and C ∈ Rn×d be two matrices. • R ∈ Rb1×d, S ∈ Rb2×d are defined as in Definition 4.1. • g, h∈ Rn are vectors. • Let bmin = {b1, b2}. Then, we have g⊤C(R⊤R) eB(S⊤S)C⊤h − g⊤C eBC⊤h ≲ log1.5 d√bmin · (∥g⊤C∥2∥ eBC⊤h∥2 + ∥g⊤C eB∥2∥C⊤h∥2) + log3 d bmin · ∥g⊤C∥2∥C⊤h∥2∥ eB∥F with probability at least 1 − 1/ poly(n). Proof. This can be proved by using Lemma B.1. 23C.7 Bounding |g⊤Ph − g⊤ ePh| We show that |g⊤P h− g⊤ eP h| can be bounded. Lemma C.6 (P and eP are close). If the following conditions hold • Given A ∈ Rd×n and W ∈ Rn×n. • Let R1 ∈ Rb1×d, R2 ∈ Rb2×d, R3 ∈ Rb3×d, and R4 ∈ Rb4×d be four matrices, defined as in Definition 4.1. • Let g ∈ Rn and h ∈ Rn be two vectors. • Let P be defined as Eq. (3), bP and eP be defined as Def. 4.3. • Let bmin = min{b1, b2}. Then, we have that |g⊤P h− g⊤ eP h| ≲ log6 d · ( 1√bmin + n b2 min )κ∥g⊤C∥2∥C⊤h∥2∥B∥2 with probability at least1−1/ poly(n), where C = W1/2A⊤, B = (AW A⊤)−1, and κ = λmax(B)/λmin(B). Proof. In order to simplify the proof, we first define B as follows: eB := (R⊤ 1 R1AW A⊤R⊤ 2 R2)−1. We define C as follows: C := W1/2A⊤. We define B as follows: B := (AW A⊤)−1. By using triangle inequality, we could obtain that |g⊤P h− g⊤ eP h| ≤ |g⊤P h− g⊤ bP h| + |g⊤ bP h− g⊤ eP h|. By using Lemma C.3 and Lemma C.5, we could obtain that |g⊤P h− g⊤ bP h| ≤2ϵ0∥g⊤C∥2∥C⊤h∥2∥B∥2 and |g⊤ bP h− g⊤ eP h| ≲ log1.5 d√bmin · (∥g⊤C∥2∥ eBC⊤h∥2 + ∥g⊤C eB∥2∥C⊤h∥2) + log3 d bmin · ∥g⊤C∥2∥C⊤h∥2∥ eB∥F According to some facts, we could get that ∥AB∥2 ≤ ∥A∥2 ·∥B∥2 and ∥A∥2 ≤ ∥A∥F ≤ √n∥A∥2, for A ∈ Rm×n. 24Then, we could get that |g⊤P h− g⊤ eP h| ≤ |g⊤P h− g⊤ bP h| + |g⊤ bP h− g⊤ eP h| ≲ 2ϵ0∥g⊤C∥2∥C⊤h∥2∥B∥2 + log1.5 d√bmin · (∥g⊤C∥2∥ eBC⊤h∥2 + ∥g⊤C eB∥2∥C⊤h∥2) + log3 d bmin · ∥g⊤C∥2∥C⊤h∥2∥ eB∥F ≲ 2ϵ0∥g⊤C∥2∥C⊤h∥2∥B∥2 + log1.5 d√bmin · (1 + 2ϵ0)∥g⊤C∥2∥C⊤h∥2∥B∥2 + log3 d bmin · (1 + 2ϵ0)√n · ∥g⊤C∥2∥C⊤h∥2∥B∥2 ≲ (log1.5 d√bmin + log3 d bmin + √n log4.5 d b1.5 min + n log6 d b2 min ) · κ∥g⊤C∥2∥C⊤h∥2∥B∥2 ≲ log6 d · ( 1√bmin + n b2 min ) · κ∥g⊤C∥2∥C⊤h∥2∥B∥2 where the first step derives from the triangle inequality and the second step is due to Lemma C.3 and Lemma C.5. The third step comes from ∥ eB∥F ≤ √n∥ eB∥2 ≤ (1 + ϵ0)√n∥B∥2. Next, we show the reason that the fourth step holds log1.5 d√bmin + log3 d bmin + √n log4.5 d b1.5 min + n log6 d b2 min ≲ log6 d · ( 1√bmin + 1 bmin + √n b1.5 min + n b2 min ) ≲ log6 d · ( 1√bmin + n b2 min ) where the first step follows from log 6 d is the dominate item in the numerator, and the second step follows from 1/√bmin > 1/bmin, ∀bmin ≥ 1 and √n/b1.5 min < n/b2 min, ∀bmin ≤ n. D Main Result In this section, we state the main result of this paper. Next, we give the proof of this statement. Theorem D.1 (Formal Main Result). If the following conditions hold • minAx=b,x∈Πm i=1Ki c⊤x is a convex problem under the federated learning setting, where Ki is compact convex sets. • For each i ∈ [m], we are given a νi-self concordant barrier function ϕi for Ki. • We have x(0) = arg minx Pm i=1 ϕi(xi). • For all x ∈ Qm i=1 Ki, we have that ∥x∥2 is bounded by R (Diameter of the set). 25• ∥c∥2 is bounded by L (Lipschitz constant of the program). Then, there exists a federated learning algorithm (see Algorithm 1) that runs inO(√ν log2 m log(ν δ )) iterations and each iteration sends O(bn) words to find a vector x such that c⊤x ≤ min Ax=b,x∈Πm i=1Ki c⊤x + LR · δ ∥Ax − b∥1 ≤ 3δ(R dX i=1 nX j=1 |Ai,j| + ∥b∥1) where ∥c∥2 ≤ L, ∥x∥2 ≤ R, and ν = Pm i=1 νi. Proof. By combining Lemma E.3, Lemma F.2 and Lemma F.3, we could get that a vector x which satisfies the above conditions after O(√ν log2 m log(ν δ )) iterations. In addition, the Algorithm 1 sends O(bn) words at each iteration (Line 14 in Algorithm 1). This finishes the proof. E Central Path Here, we introduce some basic result of central path in Algorithm 1, which could be used to prove the guarantee of W and the main result of this paper. Central path algorithm is a very standard method for solving linear programming [CLS19, LSZ19, Son19, Bra20, DLY21, JSWZ21, SY21, GS22, QSZZ23], semi-definite programming [JKL +20b, HJS+22b, HJS+22a, GS22]. We first give the definition of some parameters here: Definition E.1. For any i ∈ [m], we let ϕi(x) be defined as in Definition 3.2 and let µt i(x, s) ∈ Rni, γt i (x, s) ∈ R, and ct i(x, s) ∈ R be defined as below: µt i(x, s) = si/et + ∇ϕi(xi) (11) γt i (x, s) = ∥µt i(x, s)∥∇2ϕi(xi)−1 (12) ct i(x, s) = ( exp(λγt i (x,s))/γt i (x,s) (Pm i=1 exp(2λγt i (x,s)))1/2 if γt i (x, s) ≥ 96√α 0 otherwise (13) where λ = O(log m), et = (1 − ξ/√ν)t−1, and ξ = O(log−2(m)). According to the Definition E.1 and Algorithm 1, we could obtain that ht i = −α · ct i(x, s)µt i(x, s) (14) where α = O(1/ log2 m). In addition, we define that Φt(xt, st) = mX i=1 exp(λ∥µt i(xt, st)∥∇2ϕi(xt i)−1 ) where λ = O(log m). Then, we could obtain the following lemma. Lemma E.2 (Bounding αi). If the following conditions hold • α represents the parameter in Algorithm 1. • For any i in [m], we have αi = ∥δx,i∥xi. 26Then, we have mX i=1 α2 i ≤ 4α2. Proof. Note that mX i=1 α2 i = ∥δx∥2 x = h⊤ eV 1/2(I − eP)eV 1/2∇2ϕ(x)eV 1/2(I − eP)eV 1/2h. Due to the reason that (1 − 2α)(∇2ϕi(xi))−1 ⪯ eVi ⪯ (1 + 2α)(∇2ϕi(xi))−1 we have that (1 − α)(∇2ϕ(x))−1 ⪯ eV ⪯ (1 + α)(∇2ϕ(x))−1. Using α ≤ 1 10000, we have that mX i=1 α2 i ≤ 2h⊤ eV 1/2(I − eP)(I − eP)eV 1/2h ≤ 2h⊤ eV h where we used that I − eP is an orthogonal projection at the end. Finally, we note that h⊤ eV h ≤ 2 mX i=1 ∥ht i∥∗2 xi = 2α2 mX i=1 ct i(x, s)2∥µt i(x, s)∥∗2 xi ≤ 2α2 mX i=1 (exp(2λγt i (x, s))/γt i (x, s)2 Pm i=1 exp(2λγt i (x, s)) ∥µt i(x, s)∥∗2 xi ) = 2α2 Pm i=1 exp(2λγt i (x, s))Pm i=1 exp(2λγt i (x, s)) = 2α2 where the second step is from the definition ofht i (Eq. (14)), the third step follows from the definition of ct i (Eq. (13)), the fourth step follows from definition of γt i (See Eq. (12)). Therefore, putting it all together, we can show mX i=1 α2 i ≤ 4α2. Lemma E.3 (Lemma A.8 in [LSZ19]) . If Φt(xt, st) ≤ 80m α , then Φt+1(xt+1, st+1) ≤ \u0012 1 − αλ 40√m \u0013 Φt(xt, st) + √mλ · exp(192λ√α). In particularly, we have Φt+1(xt+1, st+1) ≤ 80m α . 27F Initial Point and Termination Condition Now, we state some basic results of self-concordance function, which could be used to prove the main result of this paper. Lemma F.1 (Theorem 4.1.7, Lemma 4.2.4 in [Nes98]) . Let ϕ be any ν-self-concordant barrier. Then, for any x, y∈ domϕ, we have ⟨∇ϕ(x), y− x⟩ ≤ν, ⟨∇ϕ(y) − ∇ϕ(x), y− x⟩ ≥ ∥y − x∥2 x 1 + ∥y − x∥x . Let x∗ = arg minx ϕ(x). For any x ∈ Rn such that ∥x − x∗∥x∗ ≤ 1, we have that x ∈ domϕ. ∥x∗ − y∥x∗ ≤ ν + 2√ν. Lemma F.2 (Lemma D.2 in [LSZ19]) . If the following conditions hold • minAx=b,x∈Qm i=1 Ki c⊤x is a convex problem where for each i, Ki is a compact convex set. • ϕi is defined as in Definition 3.2 for Ki, where i is in [m]. • We have x(0) = arg minx Pm i=1 ϕi(xi). • Diameter of the set: For any x ∈ Qm i=1 Ki, we have that ∥x∥2 ≤ R. • Lipschitz constant of the program: ∥c∥2 ≤ L. Then, the modified program minAx=b,x∈Qm i=1 Ki×R+ c⊤x with A = [A | b − Ax(0)], b = b, and c = \u0014 δ LR · c 1 \u0015 satisfies the following, for any δ >0: 1. x = \u0014 x(0) 1 \u0015 , y = 0d and s = \u0014 δ LR · c 1 \u0015 are feasible primal dual vectors with ∥s+∇ϕ(x)∥∗ x ≤ δ where ϕ(x) = Pm i=1 ϕi(xi) − log(xm+1). 2. For any x such that Ax = b, x ∈ Qm i=1 Ki × R+ and c⊤x ≤ minAx=b,x∈Qm i=1 Ki×R+ c⊤x + δ2, the vector x1:n (x1:n is the first n coordinates of x) is an approximate solution to the original convex program in the following sense c⊤x1:n ≤ min Ax=b,x∈Qm i=1 Ki c⊤x + LR · δ, ∥Ax1:n − b∥1 ≤ 3δ ·  R dX i=1 nX j=1 |Ai,j| + ∥b∥1  , x1:n ∈ mY i=1 Ki. Lemma F.3 (Lemma D.3 in [LSZ19]) . If the following conditions hold 28• ϕi(xi) is defined as in Definition 3.2. • For any i ∈ [m], we possess si t + ∇ϕi(xi) = µi, A⊤y + s = c, and Ax = b. • ∥µi∥∗ x,i ≤ 1 for all i. Then, we have that ⟨c, x⟩ ≤ ⟨c, x∗⟩ + 4tν where x∗ = arg minAx=b,x∈Qm i=1 Ki c⊤x and ν = Pm i=1 νi. References [ALS+18] Alexandr Andoni, Chengyu Lin, Ying Sheng, Peilin Zhong, and Ruiqi Zhong. Subspace embedding and linear regression with orlicz norm. In ICML, pages 224–233, 2018. [ALS+22] Josh Alman, Jiehao Liang, Zhao Song, Ruizhe Zhang, and Danyang Zhuo. Bypass ex- ponential time preprocessing: Fast neural network training via weight-data correlation preprocessing. arXiv preprint arXiv:2211.14227 , 2022. [AMS99] Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the frequency moments. Journal of Computer and system sciences , 58(1):137–147, 1999. [AS23] Josh Alman and Zhao Song. Fast attention requires bounded entries. arXiv preprint arXiv:2302.13214, 2023. [BPSW21] Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (over- parametrized) neural networks in near-linear time. In ITCS, 2021. [Bra20] Jan van den Brand. A deterministic linear program solver in current matrix multi- plication time. In Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 259–278. SIAM, 2020. [BW14] Christos Boutsidis and David P Woodruff. Optimal cur matrix decompositions. In STOC, pages 353–362, 2014. [BWZ16] Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in distributed and streaming models. In STOC, pages 236–249, 2016. [CCF02] Moses Charikar, Kevin Chen, and Martin Farach-Colton. Finding frequent items in data streams. In Automata, Languages and Programming , pages 693–703. Springer, 2002. [CKPS16] Xue Chen, Daniel M Kane, Eric Price, and Zhao Song. Fourier-sparse interpolation without a frequency gap. In FOCS, pages 741–750, 2016. [CLRS09] Thomas H Cormen, Charles E Leiserson, Ronald L Rivest, and Clifford Stein. Intro- duction to algorithms . MIT press, 2009. [CLS19] Michael B Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the current matrix multiplication time. In STOC, 2019. 29[CLSZ21] Sitan Chen, Xiaoxiao Li, Zhao Song, and Danyang Zhuo. On instahide, phase retrieval, and sparse matrix factorization. In ICLR, 2021. [Cox58] David R Cox. The regression analysis of binary sequences. Journal of the Royal Statistical Society. Series B (Methodological) , pages 215–242, 1958. [CSTZ22] Sitan Chen, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Symmetric sparse boolean matrix factorization and applications. In ITCS, 2022. [CT65] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex Fourier series. Mathematics of computation , 19(90):297–301, 1965. [CV95] Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine learning , 20(3):273–297, 1995. [CW13] Kenneth L. Clarkson and David P. Woodruff. Low rank approximation and regression in input sparsity time. In STOC, 2013. [DJS+22] Yichuan Deng, Wenyu Jin, Zhao Song, Xiaorui Sun, and Omri Weinstein. Dynamic kernel sparsifiers. arXiv preprint arXiv:2211.14825 , 2022. [DLS23a] Yichuan Deng, Zhihang Li, and Zhao Song. Attention scheme inspired softmax regres- sion. arXiv preprint arXiv:2304.10411 , 2023. [DLS23b] Yichuan Deng, Zhihang Li, and Zhao Song. An improved sample complexity for rank-1 matrix sensing. arXiv preprint arXiv:2303.06895 , 2023. [DLY21] Sally Dong, Yin Tat Lee, and Guanghao Ye. A nearly-linear time algorithm for linear programs with small treewidth: A multiscale representation of robust central path. In STOC, 2021. [DMS23] Yichuan Deng, Sridhar Mahadevan, and Zhao Song. Randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension. arXiv preprint arXiv:2304.04397, 2023. [DSW22] Yichuan Deng, Zhao Song, and Omri Weinstein. Discrepancy minimization in input- sparsity time. arXiv preprint arXiv:2210.12468 , 2022. [DSWZ22] Yichuan Deng, Zhao Song, Omri Weinstein, and Ruizhe Zhang. Fast distance oracles for any symmetric norm. In NeurIPS, 2022. [EMZ21] Hossein Esfandiari, Vahab Mirrokni, and Peilin Zhong. Almost linear time density level set estimation via dbscan. In AAAI, 2021. [FR16] Olivier Fercoq and Peter Richt´ arik. Optimization in high dimensions via accelerated, parallel, and proximal coordinate descent. SIAM Review, 58(4):739–771, 2016. [FS97] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences , 1997. [GGD+21] Vipul Gupta, Avishek Ghosh, Michal Derezinski, Rajiv Khanna, Kannan Ramchan- dran, and Michael Mahoney. Localnewton: Reducing communication bottleneck for distributed learning. ., 2021. 30[GQSW22] Yeqi Gao, Lianke Qin, Zhao Song, and Yitan Wang. A sublinear adversarial training algorithm. arXiv preprint arXiv:2208.05395 , 2022. [GS22] Yuzhou Gu and Zhao Song. A faster small treewidth sdp solver. arXiv preprint arXiv:2211.06033, 2022. [GSS22] Yeqi Gao, Zhao Song, and Baocheng Sun. An o(k log n) time Fourier set query algo- rithm. arXiv preprint arXiv:2208.09634 , 2022. [GSY23] Yeqi Gao, Zhao Song, and Junze Yin. An iterative algorithm for rescaled hyperbolic functions regression. arXiv preprint arXiv:2305.00660 , 2023. [GSYZ23] Yuzhou Gu, Zhao Song, Junze Yin, and Lichen Zhang. Low rank matrix completion via robust alternating minimization in nearly linear time.arXiv preprint arXiv:2302.11068, 2023. [HIKP12a] Haitham Hassanieh, Piotr Indyk, Dina Katabi, and Eric Price. Nearly optimal sparse fourier transform. In STOC, pages 563–578, 2012. [HIKP12b] Haitham Hassanieh, Piotr Indyk, Dina Katabi, and Eric Price. Simple and practical algorithm for sparse Fourier transform. In SODA, pages 1183–1194, 2012. [HJLS13] David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant. Applied logistic regression. John Wiley & Sons, 2013. [HJS+22a] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. A faster quantum algorithm for semidefinite programming via robust ipm framework. arXiv preprint arXiv:2207.11154, 2022. [HJS+22b] Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving sdp faster: A robust ipm framework and efficient implementation. In FOCS, 2022. [HLSY21] Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang. Fl-ntk: A neural tangent kernel- based framework for federated learning analysis. In ICML, pages 4423–4434, 2021. [HSC+20] Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. Texthide: Tack- ling data privacy in language understanding tasks. 2020. [HSLA20] Yangsibo Huang, Zhao Song, Kai Li, and Sanjeev Arora. Instahide: Instance-hiding schemes for private distributed learning. In International conference on machine learn- ing, pages 4507–4518. PMLR, 2020. [HST+22] Hang Hu, Zhao Song, Runzhou Tao, Zhaozhuo Xu, and Danyang Zhuo. Sublinear time algorithm for online weighted bipartite matching. arXiv preprint arXiv:2208.03367 , 2022. [HSWZ22] Hang Hu, Zhao Song, Omri Weinstein, and Danyang Zhuo. Training overparametrized neural networks in sublinear time. In arXiv preprint arXiv: 2208.04508 , 2022. [IK14] Piotr Indyk and Michael Kapralov. Sample-optimal fourier sampling in any constant dimension. In FOCS, pages 514–523, 2014. [IKP14] Piotr Indyk, Michael Kapralov, and Eric Price. (Nearly) Sample-optimal sparse Fourier transform. In SODA, pages 480–499, 2014. 31[IRU+19] Nikita Ivkin, Daniel Rothchild, Enayat Ullah, Vladimir Braverman, Ion Stoica, and Raman Arora. Communication-efficient distributed sgd with sketching. ., 2019. [JKL+20a] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In FOCS, 2020. [JKL+20b] Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In FOCS, 2020. [JLSW20] Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane method for convex optimization, convex-concave games and its applications. In STOC, 2020. [JLSZ23] Haotian Jiang, Yin Tat Lee, Zhao Song, and Lichen Zhang. Convex minimization with integer minima in eO(n4) time. arXiv preprint arXiv:2304.03426 , 2023. [JST+14] Martin Jaggi, Virginia Smith, Martin Tak´ aˇ c, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan. Communication-efficient distributed dual coordinate ascent. ., 2014. [JSWZ21] Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. Faster dynamic matrix inverse for faster lps. In STOC, 2021. [Kap16] Michael Kapralov. Sparse Fourier transform in any constant dimension with nearly- optimal sample complexity in sublinear time. In STOC, 2016. [Kap17] Michael Kapralov. Sample efficient estimation and recovery in sparse fft via isolation on average. In FOCS, 2017. [Kar84] Narendra Karmarkar. A new polynomial-time algorithm for linear programming. In STOC, pages 302–311, 1984. [KMY+16] Jakub Koneˇ cn` y, H Brendan McMahan, Felix X Yu, Peter Richt´ arik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492 , 2016. [LDFU13] Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression via the subsampled randomized hadamard transform. In NIPS, pages 369–377, 2013. [LGD+20] Xiaoxiao Li, Yufeng Gu, Nicha Dvornek, Lawrence H Staib, Pamela Ventola, and James S Duncan. Multi-site fmri analysis using privacy-preserving federated learn- ing and domain adaptation: Abide results. Medical Image Analysis, 65:101765, 2020. [LHM+17] Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression: Reducing the communication bandwidth for distributed training. arXiv preprint arXiv:1712.01887, 2017. [LJZ+21] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. Fedbn: Feder- ated learning on non-iid features via local batch normalization. ., 2021. [LLC+19] Xinle Liang, Yang Liu, Tianjian Chen, Ming Liu, and Qiang Yang. Federated transfer reinforcement learning for autonomous driving. ., 2019. 32[LLMY17] Jason D Lee, Qihang Lin, Tengyu Ma, and Tianbao Yang. Distributed stochastic variance reduced gradient methods by sampling extra data with replacement. JMLR, 18(1):4404–4446, 2017. [LM00] Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics , pages 1302–1338, 2000. [LSX+23] Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning and weight shifting for softmax regression. arXiv preprint arXiv:2304.13276 , 2023. [LSY23] Xiaoxiao Li, Zhao Song, and Jiaming Yang. Federated adversarial learning: A frame- work with convergence analysis. In ICML, 2023. [LSZ19] Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix multiplication time. In COLT, pages 2140–2157, 2019. [LSZ+20] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. Proceedings of Machine Learning and Systems , 2:429–450, 2020. [LSZ23] Zhihang Li, Zhao Song, and Tianyi Zhou. Solving regularized exp, cosh and sinh regression problems. arXiv preprint arXiv:2303.15725 , 2023. [MKSB13] Dhruv Mahajan, S Sathiya Keerthi, S Sundararajan, and L´ eon Bottou. A parallel sgd method with strong convergence. ., 2013. [MM13] Xiangrui Meng and Michael W Mahoney. Low-distortion subspace embeddings in input- sparsity time and applications to robust linear regression. In STOC, pages 91–100, 2013. [MMR+17] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, 2017. [MSJ+15] Chenxin Ma, Virginia Smith, Martin Jaggi, Michael Jordan, Peter Richt´ arik, and Mar- tin Tak´ ac. Adding vs. averaging in distributed primal-dual optimization. In ICML, pages 1973–1982, 2015. [Nad64] Elizbar A Nadaraya. On estimating regression. Theory of Probability & Its Applications, 9(1):141–142, 1964. [Nes98] Yurii Nesterov. Introductory lectures on convex programming volume i: Basic course. Lecture notes, 1998. [NN13] Jelani Nelson and Huy L Nguyˆ en. Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings. In FOCS, pages 117–126, 2013. [NRRW11] Feng Niu, Benjamin Recht, Christopher R´ e, and Stephen J Wright. Hogwild!: A lock- free approach to parallelizing stochastic gradient descent. ., 2011. [NSW19] Vasileios Nakos, Zhao Song, and Zhengyu Wang. (nearly) sample-optimal sparse fourier transform in any dimension; ripless and filterless. In FOCS, pages 1568–1577, 2019. 33[NW06] Jorge Nocedal and Stephen Wright. Numerical optimization. ., 2006. [Pri13] Eric C. Price. Sparse recovery and Fourier sampling. PhD thesis, Massachusetts Insti- tute of Technology, 2013. [PS15] Eric Price and Zhao Song. A robust sparse Fourier transform in the continuous setting. In FOCS, pages 583–600, 2015. [PSW17] Eric Price, Zhao Song, and David P. Woodruff. Fast regression with an ℓ∞ guarantee. In ICALP, 2017. [QJS+22] Lianke Qin, Rajesh Jayaram, Elaine Shi, Zhao Song, Danyang Zhuo, and Shumo Chu. Adore: Differentially oblivious relational database operators. In VLDB, 2022. [QRS+22] Lianke Qin, Aravind Reddy, Zhao Song, Zhaozhuo Xu, and Danyang Zhuo. Adaptive and dynamic multi-resolution hashing for pairwise summations. In BigData, 2022. [QSW23] Lianke Qin, Zhao Song, and Yitan Wang. Fast submodular function maximization. arXiv preprint arXiv:2305.08367 , 2023. [QSZ23] Lianke Qin, Zhao Song, and Ruizhe Zhang. A general algorithm for solving rank-one matrix sensing. arXiv preprint arXiv:2303.12298 , 2023. [QSZZ23] Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 101–156. PMLR, 2023. [RHL+20] Nicola Rieke, Jonny Hancox, Wenqi Li, Fausto Milletari, Holger R Roth, Shadi Albar- qouni, Spyridon Bakas, Mathieu N Galtier, Bennett A Landman, Klaus Maier-Hein, et al. The future of digital health with federated learning. NPJ digital medicine , 3(1):1–7, 2020. [RHS+15] Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. On variance reduction in stochastic gradient descent and its asynchronous variants.., 2015. [RKR+16] Sashank J Reddi, Jakub Koneˇ cn` y, Peter Richt´ arik, Barnab´ as P´ ocz´ os, and Alex Smola. Aide: Fast and communication efficient distributed optimization. ., 2016. [RPU+20] Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman, Joseph Gonzalez, and Raman Arora. Fetchsgd: Communication-efficient federated learning with sketching. In ICML, pages 8253–8265, 2020. [RT16] Peter Richt´ arik and Martin Tak´ aˇ c. Distributed coordinate descent method for learning with big data. JMLR, 17(1):2657–2681, 2016. [SFC+18] Virginia Smith, Simone Forte, Ma Chenxin, Martin Tak´ aˇ c, Michael I Jordan, and Martin Jaggi. Cocoa: A general framework for communication-efficient distributed optimization. IMLR, 18:230, 2018. [Son19] Zhao Song. Matrix theory: optimization, concentration, and algorithms. The University of Texas at Austin, 2019. 34[SS14] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In 2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton), pages 850–857, 2014. [SSWZ22a] Zhao Song, Baocheng Sun, Omri Weinstein, and Ruizhe Zhang. Quartic samples suffice for fourier interpolation. arXiv preprint arXiv:2210.12495 , 2022. [SSWZ22b] Zhao Song, Baocheng Sun, Omri Weinstein, and Ruizhe Zhang. Sparse fourier transform over lattices: A unified approach to signal reconstruction. arXiv preprint arXiv:2205.00658, 2022. [SSZ14] Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed op- timization using an approximate newton-type method. In ICML, 2014. [SWY23] Zhao Song, Weixin Wang, and Chenbo Yin. Fast and efficient matching algorithm with deadline instances. arXiv preprint arXiv:2305.08353 , 2023. [SWYZ23] Zhao Song, Yitan Wang, Zheng Yu, and Lichen Zhang. Sketching for first order method: Efficient algorithm for low-bandwidth channel and vulnerability. In ICML, 2023. [SWZ17] Zhao Song, David P Woodruff, and Peilin Zhong. Low rank approximation with entry- wise ℓ1-norm error. In STOC, 2017. [SWZ19] Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In SODA, 2019. [SY21] Zhao Song and Zheng Yu. Oblivious sketching-based central path method for linear programming. In ICML, 2021. [SYYZ23] Zhao Song, Mingquan Ye, Junze Yin, and Lichen Zhang. A nearly-optimal bound for fast regression with ℓ∞ guarantee. In ICML, 2023. [SYZ21] Zhao Song, Shuo Yang, and Ruizhe Zhang. Does preprocessing help training over- parameterized neural networks? Advances in Neural Information Processing Systems (NeurIPS), 34, 2021. [SZZ21] Zhao Song, Lichen Zhang, and Ruizhe Zhang. Training multi-layer over-parametrized neural network in subquadratic time. arXiv preprint arXiv:2112.07628 , 2021. [Tib96] Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological) , pages 267–288, 1996. [Wat64] Geoffrey S Watson. Smooth regression analysis. Sankhy¯ a: The Indian Journal of Statistics, Series A , pages 359–372, 1964. [WZ16] David P Woodruff and Peilin Zhong. Distributed low rank approximation of implicit functions of a matrix. In ICDE, 2016. [WZD+20] Ruosong Wang, Peilin Zhong, Simon S Du, Russ R Salakhutdinov, and Lin F Yang. Planning with general objective functions: Going beyond total rewards. In NeurIPS, 2020. [XZZ18] Chang Xiao, Peilin Zhong, and Changxi Zheng. Bourgan: generative networks with metric embeddings. In NeurIPS, 2018. 35[Yan13] Tianbao Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In NIPS, 2013. [YZY+19] Wensi Yang, Yuhang Zhang, Kejiang Ye, Li Li, and Cheng-Zhong Xu. Ffd: a federated learning based method for credit card fraud detection. In International Conference on Big Data, pages 18–32, 2019. [Zha22] Lichen Zhang. Speeding up optimizations via data structures: Faster search, sample and maintenance. Master’s thesis, Carnegie Mellon University, 2022. [ZL15] Yuchen Zhang and Xiao Lin. Disco: Distributed optimization for self-concordant em- pirical loss. In ICML, pages 362–370, 2015. [ZWSL10] Martin Zinkevich, Markus Weimer, Alexander J Smola, and Lihong Li. Parallelized stochastic gradient descent. In NIPS, 2010. [ZWX+17] Shun Zheng, Jialei Wang, Fen Xia, Wei Xu, and Tong Zhang. A general distributed dual coordinate optimization framework for regularized loss minimization. JMLR, 18(1):4096–4117, 2017. 36",
      "references": [
        "Regression shrinkage and selection via the lasso",
        "The regression analysis of binary sequences",
        "Applied logistic regression",
        "Support-vector networks",
        "A decision-theoretic generalization of on-line learning and an application to boosting",
        "On estimating regression",
        "Smooth regression analysis",
        "A new polynomial-time algorithm for linear programming",
        "Solving linear programs in the current matrix multiplication time",
        "Faster dynamic matrix inverse for faster lps",
        "Oblivious sketching-based central path method for linear programming",
        "A faster interior point method for semidefinite programming",
        "An improved cutting plane method for convex optimization, convex-concave games and its applications",
        "Solving empirical risk minimization in the current matrix multiplication time",
        "Communication-efficient distributed sgd with sketching",
        "Fetchsgd: Communication-efficient federated learning with sketching",
        "Communication-efficient distributed dual coordinate ascent",
        "Federated learning: Strategies for improving communication efficiency",
        "Deep gradient compression: Reducing the communication bandwidth for distributed training",
        "Federated optimization in heterogeneous networks",
        "Localnewton: Reducing communication bottleneck for distributed learning.",
        "Parallelized stochastic gradient descent",
        "Hogwild!: A lock-free approach to parallelizing stochastic gradient descent",
        "A parallel sgd method with strong convergence",
        "Distributed stochastic optimization and learning",
        "Distributed stochastic variance reduced gradient methods by sampling extra data with replacement",
        "On variance reduction in stochastic gradient descent and its asynchronous variants.",
        "Trading computation for communication: Distributed stochastic dual coordinate ascent",
        "Distributed coordinate descent method for learning with big data",
        "A general distributed dual coordinate optimization framework for regularized loss minimization",
        "Optimization in high dimensions via accelerated, parallel, and proximal coordinate descent",
        "Communication-efficient distributed op- timization using an approximate newton-type method",
        "Aide: Fast and communication efficient distributed optimization",
        "Disco: Distributed optimization for self-concordant em- pirical loss",
        "Adding vs. averaging in distributed primal-dual optimization",
        "Cocoa: A general framework for communication-efficient distributed optimization",
        "Multi-site fmri analysis using privacy-preserving federated learn- ing and domain adaptation: Abide results",
        "The future of digital health with federated learning",
        "Ffd: a federated learning based method for credit card fraud detection",
        "Federated transfer reinforcement learning for autonomous driving",
        "Communication-efficient learning of deep networks from decentralized data",
        "Fedbn: Feder- ated learning on non-iid features via local batch normalization",
        "Fl-ntk: A neural tangent kernel- based framework for federated learning analysis",
        "Sketching for first order method: Efficient algorithm for low-bandwidth channel and vulnerability",
        "Federated adversarial learning: A frame- work with convergence analysis",
        "Instahide: Instance-hiding schemes for private distributed learning",
        "Texthide: Tack- ling data privacy in language understanding tasks",
        "On instahide, phase retrieval, and sparse matrix factorization",
        "Symmetric sparse boolean matrix factorization and applications",
        "Low rank approximation and regression in input- sparsity time",
        "Osnap: Faster numerical linear algebra algorithms via sparser subspace embeddings",
        "Low-distortion subspace embeddings in input- sparsity time and applications to robust linear regression",
        "Optimal cur matrix decompositions",
        "Low rank approximation with entry- wise ℓ1-norm error",
        "Subspace embedding and linear regression with orlicz norm",
        "Distributed low rank approximation of implicit functions of a matrix",
        "Optimal principal component analysis in distributed and streaming models",
        "Planning with general objective functions: Going beyond total rewards",
        "Relative error tensor low rank approximation",
        "Randomized and deterministic attention sparsification algorithms for over-parameterized feature dimension",
        "Discrepancy minimization in input- sparsity time",
        "Almost linear time density level set estimation via dbscan",
        "Fast and efficient matching algorithm with deadline instances",
        "Sublinear time algorithm for online weighted bipartite matching",
        "Solving regularized exp, cosh and sinh regression problems",
        "Attention scheme inspired softmax regres- sion",
        "The closeness of in-context learning and weight shifting for softmax regression",
        "An iterative algorithm for rescaled hyperbolic functions regression",
        "Convex minimization with integer minima in eO(n4) time",
        "Fast submodular function maximization",
        "Bourgan: generative networks with metric embeddings",
        "Fast distance oracles for any symmetric norm",
        "Training (over- parametrized) neural networks in near-linear time",
        "Does preprocessing help training over- parameterized neural networks?",
        "Training multi-layer over-parametrized neural network in subquadratic time",
        "Training overparametrized neural networks in sublinear time",
        "Speeding up optimizations via data structures: Faster search, sample and maintenance",
        "A sublinear adversarial training algorithm",
        "Bypass ex- ponential time preprocessing: Fast neural network training via weight-data correlation preprocessing",
        "Adore: Differentially oblivious relational database operators",
        "Fast attention requires bounded entries",
        "Adaptive and dynamic multi-resolution hashing for pairwise summations",
        "Dynamic kernel sparsifiers",
        "Low rank matrix completion via robust alternating minimization in nearly linear time",
        "An improved sample complexity for rank-1 matrix sensing",
        "A general algorithm for solving rank-one matrix sensing",
        "Finding frequent items in data streams",
        "The space complexity of approximating the frequency moments",
        "Intro- duction to algorithms",
        "Adaptive estimation of a quadratic functional by model selection",
        "Faster ridge regression via the subsampled randomized hadamard transform",
        "Fast regression with an ℓ∞ guarantee",
        "A nearly-optimal bound for fast regression with ℓ∞ guarantee",
        "An algorithm for the machine calculation of complex Fourier series",
        "Nearly optimal sparse fourier transform",
        "Simple and practical algorithm for sparse Fourier transform",
        "Sparse recovery and Fourier sampling",
        "(Nearly) Sample-optimal sparse Fourier transform",
        "Sample-optimal fourier sampling in any constant dimension",
        "A robust sparse Fourier transform in the continuous setting",
        "Fourier-sparse interpolation without a frequency gap",
        "Sparse Fourier transform in any constant dimension with nearly-optimal sample complexity in sublinear time",
        "Sample efficient estimation and recovery in sparse fft via isolation on average",
        "(nearly) sample-optimal sparse fourier transform in any dimension; ripless and filterless",
        "An o(k log n) time Fourier set query algo- rithm",
        "Quartic samples suffice for fourier interpolation",
        "Sparse fourier transform over lattices: A unified approach to signal reconstruction",
        "Matrix theory: optimization, concentration, and algorithms",
        "A deterministic linear program solver in current matrix multi- plication time",
        "A nearly-linear time algorithm for linear programs with small treewidth: A multiscale representation of robust central path",
        "A faster small treewidth sdp solver",
        "An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization",
        "Solving sdp faster: A robust ipm framework and efficient implementation",
        "A faster quantum algorithm for semidefinite programming via robust ipm framework",
        "Introductory lectures on convex programming volume i: Basic course"
      ],
      "meta_data": {
        "arxiv_id": "2305.17482v1",
        "authors": [
          "Song Bian",
          "Zhao Song",
          "Junze Yin"
        ],
        "published_date": "2023-05-27T14:23:14Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper introduces the first distributed Interior Point Method (IPM) algorithm, named FL-IPM, to solve Empirical Risk Minimization (ERM) problems within a Federated Learning (FL) setting. The main contributions include being the first to apply an IPM framework to ERM under FL and the first to use sketching techniques to compress Hessian information in this context, distinguishing it from prior works that either sketched gradients in FL or Hessians in classical distributed computing. The authors also provide convergence guarantees for their IPM algorithm under FL, which compresses Hessian information via sketching methods to reduce communication costs, without relying on strong, unrealistic assumptions.",
        "methodology": "The proposed methodology centers on an Interior Point Method (IPM) framework adapted for Federated Learning (FL), termed FERM. It employs AMS sketch matrices to significantly reduce communication costs by compressing Hessian information. The algorithm proceeds in several steps: 1. Setup: Server and clients are initialized with the same random seed for generating sketching matrices (R1, R2, R3, R4). 2. Local Update: Each client updates its local parameters (x, s), computes local Hessian inverses (Wt_i), gradient-related terms (µt_i, γt_i, ct_i), and a scaled gradient (ht_i). Clients then send specific sketched components of (Wt_i)^1/2 * A_i^T, R2 * A_i * Wt_i * A_i^T * R3^T, R4 * A_i * (Wt_i)^1/2, and ht_i to the server. 3. Global Update: The server constructs an approximate projection matrix (eP) using the received sketched information, computes global update steps (δt_x, δt_s), and sends these back to the clients. The theoretical analysis focuses on bounding the error introduced by these sketching techniques.",
        "experimental_setup": "The paper primarily presents a theoretical analysis of the proposed FL-IPM algorithm. It does not describe practical experimental setups involving specific datasets or benchmarks. Instead, the validation is based on theoretical convergence guarantees. The main result (Theorem 1.1 and D.1) states that for a convex problem `min Ax=b, x∈Πm i=1Ki c⊤x` under FL, where `Ki` are compact convex sets and `ϕi` are `νi`-self-concordant barrier functions, the algorithm runs in `O(√ν log^2 m log(ν/δ))` iterations. Each iteration sends `O(bn)` words, where `b` is related to the size of sketch matrices (`bmax = O(√n)`), `d` is the dimension, `n` is the total number of data points, `m` is the number of clients, and `ν = Σνi`. The analysis ensures an approximate solution `x` within `δ` error with respect to the objective and constraint violation, assuming boundedness of `∥x∥2` and `∥c∥2`.",
        "limitations": "The paper implicitly addresses limitations of existing methods by proposing a communication-efficient second-order algorithm for FL, contrasting with first-order methods that depend polynomially on Lipschitz constants and strong convexity, and previous distributed second-order methods that made strong, unrealistic assumptions. While the proposed method offers theoretical advantages over naive approaches (Models 1, 2, and 3 described in Section 6), Model 3, for instance, would require sending O(n^2) words per iteration and client data sharing, which is impractical and violates FL's privacy principles. The paper's primary focus is theoretical, and it acknowledges that practical deployment considerations like stragglers, device heterogeneity, network scalability (latency and throughput), and computational cost on devices and servers are not fully addressed within this work.",
        "future_research_directions": "The authors identify several key areas for future research and practical application of their FL-IPM algorithm: 1. Address stragglers and device heterogeneity in real-world system environments by designing more robust algorithms. 2. Improve scalability for large networks, specifically considering network latency and throughput. 3. Account for the computational cost on individual client devices and the central server. The current work provides theoretical results, and these future directions aim to bridge the gap towards practical implementation in complex, real-world federated learning systems.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Can Transformers Smell Like Humans?",
      "full_text": "Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation SHUTONG QIAO, School of Big Data and Software Engineering, Chongqing University, Chongqing, China WEI ZHOU, School of Big Data and Software Engineering, Chongqing University, Chongqing, China JUNHAO WEN∗, School of Big Data and Software Engineering, Chongqing University, Chongqing, China CHEN GAO∗, BNRist, Tsinghua University, Beijing, China QUN LUO, Tencent Inc., Beijing, China PEIXUAN CHEN, Tencent Inc., Beijing, China YONG LI, Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, China Session-based recommendation (SBR) methods often rely on user behavior data, which can struggle with the sparsity of session data, limiting performance. Researchers have identified that beyond behavioral signals, rich semantic information in item descriptions is crucial for capturing hidden user intent. While large language models (LLMs) offer new ways to leverage this semantic data, the challenges of session anonymity, short-sequence nature, and high LLM training costs have hindered the development of a lightweight, efficient LLM framework for SBR. To address the above challenges, we propose an LLM-enhanced SBR framework that integrates semantic and behavioral signals from multiple views. This two-stage framework leverages the strengths of both LLMs and traditional SBR models while minimizing training costs. In the first stage, we use multi-view prompts to infer latent user intentions at the session semantic level, supported by an intent localization module to alleviate LLM hallucinations. In the second stage, we align and unify these semantic inferences with behavioral representations, effectively merging insights from both large and small models. Extensive experiments on two real datasets demonstrate that the LLM4SBR framework can effectively improve model performance. We release our codes along with the baselines at https://github.com/tsinghua-fib-lab/LLM4SBR. CCS Concepts: • Information systems →Recommender systems. Additional Key Words and Phrases: Recommender System; Session-based Recommendation; Large Language Models; Data Augmentation ACM Reference Format: Shutong Qiao, Wei Zhou, Junhao Wen, Chen Gao, Qun Luo, Peixuan Chen, and Yong Li. 2025. Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation.ACM Trans. Inf. Syst. 1, 1, Article 1 (January 2025), 25 pages. https://doi.org/10.1145/3719344 ∗Corresponding author. Authors’ Contact Information: Shutong Qiao, School of Big Data and Software Engineering, Chongqing University, Chongqing, Chongqing, China, qiaoshutong@cqu.edu.cn; Wei Zhou, School of Big Data and Software Engineering, Chongqing University, Chongqing, Chongqing, China, zhouwei@cqu.edu.cn; Junhao Wen, School of Big Data and Software Engineering, Chongqing University, Chongqing, Chongqing, China, jhwen@cqu.edu.cn; Chen Gao, BNRist, Tsinghua University, Beijing, Beijing, China, chgao96@gmail.com; Qun Luo, Tencent Inc., Beijing, Beijing, China, prisluo@tencent.com; Peixuan Chen, Tencent Inc., Beijing, Beijing, China, noahchen@tencent.com; Yong Li, Department of Electronic Engineering, BNRist, Tsinghua University, Beijing, Beijing, China, liyong07@tsinghua.edu.cn. Please use nonacm option or ACM Engage class to enable CC licenses This work is licensed under a Creative Commons Attribution 4.0 International License. © 2025 Copyright held by the owner/author(s). ACM 1558-2868/2025/1-ART1 https://doi.org/10.1145/3719344 ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025. arXiv:2402.13840v2  [cs.IR]  14 Apr 20251:2 • Shutong Qiao et.al. 1 Introduction With the widespread application of Recommender systems (RS) [18, 37, 55, 56] in multiple fields, traditional RS methods are no longer sufficient to meet users’ actual needs in real-world scenarios. Specifically, on the one hand, for the new-coming user, it is difficult to collect enough user behavior data and preference information to make accurate recommendations and encounter the \"cold start\" problem. On the other hand, with the enhancement of awareness of data privacy protection globally, such as California’s California Consumer Privacy Act (CCPA) [23], which reinforces the protection of individual privacy rights, placing compliance pressure on traditional recommendation strategies that rely on user personal information. Therefore, against this backdrop, research on Session-based Recommendation (SBR) [50] technology is gradually becoming a hotspot, which aims to provide recommendations based on users’ transient anonymous session behavioral data with less risk to user privacy. Traditional research in SBR relies heavily on deep neural networks to model user interaction sequences. This includes the use of Recurrent Neural Networks (RNNs) [38] with Attention Mechanisms to capture long- term dependencies between items in a session, as well as the use of Graph Neural Networks (GNNs) [41] and Hypergraph Neural Networks (HGNNs) [13] to model complex item transformation relationships in a session. Although these deep learning-based SBR models [2, 19, 40, 46, 50] show high effectiveness in modeling behavioral information, they are still faced with the challenge of high sparsity of session data. Specifically, the SBR model uses one-hot encoding to represent item IDs, greatly weakening the correlation between items. Therefore, it is difficult for the model to deeply analyze and grasp the user’s fundamental interests and potential intentions when only analyzing sparse behavior records. On the other hand, unlike interactive data, semantic data revolves around items’ inherent similarity and relevance, which may bring opportunities for the above challenge. For instance, assuming a user sequentially interacts with “ iPhone 15, running shoes, iPhone 14, milk, skirt \", traditional SBR models may interpret recent interactions with “milk\" and “skirt\" as indicative of the user’s prevailing preferences. However, through a deeper semantic lens, it becomes apparent that the selection of “iPhone 15 ” and “iPhone 14 ” underscores a more enduring and profound interest in Apple-branded products. This underscores the potential of semantic information integration within RSs to more accurately discern and capture the underlying needs and users’ interests. Currently, the rapid development of large language models (LLMs) [1, 3, 43, 58] has made remarkable achieve- ments in the depth and breadth of language understanding and shows a strong logical inference ability, opening up a new path for recommendation algorithms. Recent work [9, 15] has begun to utilize pre-training and fine-tuning techniques to integrate core tasks in recommendation systems, transforming them into language understanding and generation problems in natural language processing (NLP). Through this approach, LLMs can generate personalized recommendation lists based on a comprehensive understanding of users’ past behaviors and personal preferences, providing detailed recommendations and significantly enhancing both user experience and recom- mendation accuracy. However, the high performance of LLM is often accompanied by high computational costs and latency issues, which largely limits their widespread application in real-time industrial scenarios. Therefore, ID-based collaborative information modeling methods still maintain their unique advantages and indispensable position. In addition, due to the high sparsity and anonymity of session data, research on incorporating LLM into SBR scenarios is still in its infancy. Current research mainly utilizes the capabilities of LLM by fine-tuning parameters[4, 5] or prompt optimization[42]. The above work has conducted a preliminary exploration, with unresolved key challenges as follows: •Behavioral information based on the ID paradigm is the core of personalized recommender systems, which is even ignored by some existing methods solely based on semantic information. •Session data is often augmented using sequence segmentation, but this approach may result in a large number of similar session sequences in the generated dataset, which affects the fine-tuned LLMs based on this data to produce repetitive sentences when generating responses. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:3 •Fine-tuning and pre-training often require a large number of computational resources, which is difficult to deploy in real-world scenarios practically. •The high sparsity of the session data makes LLMs fail to generate valid answers or may generate incorrect items that are outside the candidate set. To address these challenges, we propose a lightweight and effective LLM-enhanced framework (LLM4SBR), which comprises two distinct stages: intent inference and representation enhancement. In the intent inference stage, we employ LLM as the inference engine by guiding LLM to infer through carefully designed prompts from different views of user intents. The intent localization module is crafted to alleviate hallucinations and semantically enhance the inference results. In the representation enhancement stage, we introduce the traditional SBR model to load interaction behaviors simultaneously, combined with textual intention inference results. On the one hand, traditional models generate conversation representations from different views based on interaction data, while on the other hand, it parses text data into embedded forms. Subsequently, each view performs alignment and uniformity of session and inference embeddings separately. Finally, we fuse all embeddings from all views as the session representation for prediction. We summarize the contributions of this work as follows: •We take the pioneering step to propose a general framework for introducing large language models to the session-based recommendation, aiming to leverage LLM’s powerful logical-reasoning capability to facilitate deep semantic information integration. The framework ingeniously separates LLM’s inference process from the training of the traditional SBR model, forming a two-stage strategy, thereby achieving a balanced optimization of efficiency and effectiveness. •We first propose an intent localization module, which can alleviate LLM hallucination and enhance semantic- level intent in preliminary results of LLM inference. We then achieve a finer-grained modal alignment by performing alignment and uniformity between embeddings from different views, facilitating the effective integration of interaction ID information and semantic information. •We conduct extensive experiments on two real-world datasets, and the results show that our proposed LLMSBR framework can be a plug-in solution that can steadily improve the performance of almost all the existing SBR models. Further studies verify the effectiveness of each component in our proposed LLMSBR framework. 2 Related Work 2.1 Session-based Recommendation The available information in the field of SBR is very limited, consisting only of interaction data within the session. Therefore, SBR research focuses on how to effectively model interaction behavior and learn session preferences. Based on different modeling emphases, we can broadly categorize SBR methods into two types: traditional SBR methods and methods focusing on modeling item transition relationships. In traditional SBR methods, S-POP [ 2] recommends based on the most popular items, and Item-KNN [ 11] calculates item similarity based on historical behavior to recommend similar items. As Markov chains exhibit advantages in modeling sequential data, FPMC [40] captures data sequence information and user preferences by combining first-order Markov chains with matrix factorization. In the SBR methods based on deep learning, inspired by the field of NLP, GRU4Rec [19] proposed for the first time to use of the RNN to simulate user preference changes in behavioral sequence data. Based on this research, Stamp improved performance by introducing an attention mechanism to make preferences more targeted. NARM [ 28] uses the attention network to capture users’ short-term interests and long-term dependencies. DSAN[54] adaptively filters the noisy information in the sequences through a two-layer sparse attention network, ensuring that the model can focus on items that truly reflect the user’s interests. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:4 • Shutong Qiao et.al. As GNNs show their prowess in various fields, SBR researchers have found that by constructing session data into the form of graphs, they can better capture the complex transformation relationships between items and greatly improve recommendation performance. SR-GNN[50] is the first model to represent sequences in the form of session graphs, utilizing gated graph neural networks as encoders. GC-SAN [52], an upgraded version of SR-GNN, incorporates attention mechanisms to make session representations more targeted. GCE-GNN [48], MSGAT [39], MEGAN [45] and KMVG[7] construct multiple graphs with different structures, simultaneously considering both local item collaborations and global session collaboration relationships. In addition, DHCN [51], HL[47], and HIDE [33] captures the complex high-order miscellaneous information of the items by building the hypergraph. In recent research, GSNIR [ 24] adopts a dual intent network to learn user intent from the attention mechanism and historical data distribution respectively. MiaSRec [8] represents various session intents by deriving multiple session representations centered around each item and dynamically selecting important representations. Although the aforementioned SBR methods have achieved good performance, they mainly focus on modeling the interaction information in the session but fail to fully explore and utilize the rich semantic information contained in the sequence. 2.2 Recommender System with LLM Generative dialogue models represented by ChatGPT have sparked research in various fields. According to how LLM participates in the RS, we divide it into LLM as Recommender and LLM-enhanced Recommender. 2.2.1 LLM as Recommender. The model of LLM as Recommender realizes the transformation from the ID paradigm to the modal paradigm by converting the recommendation task into a task in natural language processing. The M6-Rec [9] model extends the pre-trained language model M6 [34] by transforming recommendation tasks into either language understanding or language generation tasks. It establishes a unified foundational recommendation model to reduce downstream tasks’ dependence on data. Geng et al. [15] proposed the P5 paradigm, which enables predictions in a zero-shot or few-shot manner by providing adaptive personalized prompts tailored to different users. This approach reduces the need for fine-tuning. Kang et al. [26] evaluated the performance of LLMs of different sizes (250M - 540B parameters) in zero-shot, few-shot, and fine-tuning scenarios to explore the extent to which LLM understands user preferences based on the user’s previous behavior. Sunhao Dai et al. [10] enhance the recommendation capabilities of ChatGPT by combining ChatGPT with traditional information retrieval (IR) ranking functions. GPT4Rec [29] first generates queries based on a language model, and then optimizes product retrieval separately through a search engine, addressing optimization from two aspects. VIP5 [16] explores a multi-modal base model of the P5 recommendation paradigm that considers both visual and textual modalities. Zhu Sun et al. [42] proposed the PO4ISR model of SBR, which promotes LLM to continuously reflect and update the results from the view of real-time optimization prompts to improve the accuracy of recommendations. Agent4Rec [59] utilizes a generative agent empowered by LLM to simulate and infer personalized user preferences and behavioral patterns. ToolRec [60] uses LLM as a proxy user to guide the recommendation process and call external tools to generate recommendation lists that are closely related to the user’s subtle preferences. Lin et al. [ 35] achieve the goal of efficiently fine-tuning LLM-based RS by introducing a data-pruning task. Although these methods have made breakthrough progress in zero-shot, few-shot, and interpretability aspects, the core of the above method is to enhance recommendation performance by improving LLM’s adaptability to recommended data and inference capabilities. Therefore, they suffer from drawbacks such as high fine-tuning costs and difficulty capturing specific fine-grained behavioral patterns. 2.2.2 LLM-enhanced Recommender. LLM-enhanced RS treats LLM as a tool to enhance the performance of recommendation models. The large-model recommendation framework proposed by Weiwei et al. [49] leverages ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:5 graph-enhanced strategies based on LLM to enhance RS, addressing challenges posed by data sparsity and low-quality side information in RS. Chat-Rec [14] integrates traditional RS with conversational AI like ChatGPT, eliminating the need for training to gain a deep understanding of user preferences through LLM’s comprehension of dialogue context. CTRL [31] regards the original table data and the corresponding text data as two different modalities, uses the collaborative CTR model and the pre-trained language model, respectively, for feature extraction, and adjusts the knowledge of the two modalities through comparative learning. E4SRec [ 32] is a solution that combines sequence recommendation with LLMs. It takes only ID sequences as input and ensures efficient controllable generation by predicting all candidate sequences at each forward pass. The above method has made us realize the potential of integrating LLM with RS and how a two-stage framework can better balance efficiency and performance compared to an end-to-end framework. Jesse Harte et al. [17] devised three strategies for leveraging LLM, and found that using embeddings initialized with LLM significantly enhances the performance of sequential recommendation models. This inspires us about the importance of textual semantics. The SAID [20] shows how to effectively use LLM to convert item IDs into semantically rich embedding vectors for downstream recommendation task models. The above methods explore the effectiveness of LLM in RS from different views, allowing us to realize the potential of integrating LLM with RS and how the two-stage framework can better balance efficiency and performance compared with the end-to-end framework. However, these methods fail to fully integrate and fuse semantic information and behavioral data, and therefore perform poorly when dealing with SBR scenarios with high sparsity and lack of user information. Table 1. Description of notations. Notations Descriptions 𝑆 The set of all session sequences 𝑠𝑡 The session sequence of clicks at timestamp 𝑡 𝐼 The set of all items 𝑖𝑡,𝑘 The 𝑘-th item clicked in the session at timestamp 𝑡 𝑌, 𝑦, ˆ𝑦 The label set of all sessions, the ground truth and predicted label of a single session Textinfer, Textitem LLM inference results and item names in text form 𝐸, 𝑒 Text embeddings obtained using a pre-trained language model 𝐻, ℎ The set of latent representations for all sessions and the latent repre- sentation for a single session 𝑑,𝑊,𝑏 Dimension, weight matrix and biases 𝑇 Alignment transformation matrix 𝐾 The number of items that users are actually interested in 𝑟 The number of items with similar semantics 𝜏 Weight of alignment and uniformity loss functions L Loss Function 3 Problem Formulation The objective of SBR is to predict the next interaction item expected to occur in the current session history of an anonymous user. Here, we provide the problem definition in mathematical terms. Each data entry in the dataset represents a session sequence. Let the collection of all sessions be denoted as S= {𝑠1,𝑠2,··· ,𝑠𝑚}, where 𝑚is ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:6 • Shutong Qiao et.al. Stage 2.  Representation Enhancement Stage 1.  Intent Inference  Long-term  perspective promptLong-term  perspective promptLong-term view  prompt Short-term  perspective promptShort-term  perspective promptShort-term view  prompt Large Language Model Long-term  perspective promptLong-term  perspective promptLong-term view  response Short-term  perspective promptShort-term  perspective promptShort-term view  response Intent Localization ModuleItem name  table Short-term view IntentLong-term view Intent ID interaction sequence (Small) SBR model DirectAU DirectAULinear Transformation Long-term inference  intent embedding Short-term inference  intent embedding Local  representation  embedding Global  representation  embedding Softmax item embedding ResultMulti-view Intents Fig. 1. LLM4SBR framework diagram. LLM4SBR is a two-stage framework: (a) In the intent inference stage, LLM makes initial inferences based on prompts from different views (long-term and short-term). Subsequently, the intent localization module is utilized to alleviate hallucinations and enhance semantics in the inference results. (b) In the representation enhancement stage, interaction data and text data are synchronously loaded into the model. Traditional SBR models are used to model the interaction data to obtain local and global session representations. After aligning and uniforming session representations and inference representations of the same view, all representations are fused into the final session representation for prediction. the total number of sessions. The item set is the summary of items that have appeared in all sessions, which we define as I= {𝑖1,𝑖2,··· ,𝑖𝑛}, where 𝑛is the total number of items in the set. We represent the t-th session 𝑠𝑡 in the dataset as 𝑠𝑡 = {𝑖𝑡,1,𝑖𝑡,2,··· ,𝑖𝑡,𝑘,··· ,𝑖𝑡,|𝑠𝑡 |}, where |𝑠𝑡|is the length of the current session, and 𝑖𝑡,𝑘 ∈I represents the 𝑘-th clicked item in the current session 𝑠𝑡. Based on the above symbols and descriptions, we define the modeling goal of session 𝑠𝑡 as predicting the click of the |𝑠𝑡|+ 1th item based on the historical behavior records of 𝑠𝑡. The symbols and explanations involved in this article are listed in Table 1. In the following text, different symbols are distinguished by subscripts. 4 METHODOLOGY The overall architecture of LLM4SBR is depicted in Figure 1, and the framework process is shown in Algorithm 1. This section will introduce the problem definition and the specific implementation details of each module in turn. 4.1 Intent Inference Stage 4.1.1 Prompt Design. To more effectively utilize the inference capabilities of LLM and better integrate the inferential results of LLM with the representations learned by the SBR model, we introduce view constraint qualifiers as an auxiliary tool. Specifically, in our prompt design, we utilize the view-limiting qualifiers based on ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:7 \"The order in which users click on items is as follows:  1. ItemName_ItemID \\n  <Number [Order relationship] + Name [Text mode] + ID [Unique identifier]> 2. ItemName_ItemID \\n ... N. ItemName_ItemID \\n <Background Description> Please guess an item that the user is interested in in the [View-limiting qualifier].  <Task Definition> Fig. 2. Illustration of the design of prompts. LLM will perform inference based on short-term and long-term prompts respectively to obtain inference results from different views. \"The order in which users click on items is as follows: 1.Beauty Flawless Makeup Blender Sponge Puff (size 1)_13  2.Hot Tools Big Bumper Spring Curling Iron, 1-1/2_14  3.Macadamia Oil Nourishing Leave-In Cream, 10 ounces Bottle_15  4.Macadamia Oil Healing Oil Treatment, 4.2 ounces Glass Bottle_16  5.10 Color Makeup Cosmetic Blush Blusher_17  Please guess an item that the user is interested in in the long-term. (Only  output the item name without any explanation.)\" \"The order in which users click on items is as follows: 1.Beauty Flawless Makeup Blender Sponge Puff (size 1)_13  2.Hot Tools Big Bumper Spring Curling Iron, 1-1/2_14  3.Macadamia Oil Nourishing Leave-In Cream, 10 ounces Bottle_15  4.Macadamia Oil Healing Oil Treatment, 4.2 ounces Glass Bottle_16  5.10 Color Makeup Cosmetic Blush Blusher_17  Please guess an item that the user is interested in in the short-term. (Only  output the item name without any explanation.)\" Long-term Prompt: Short-term Prompt: Inference Macadamia Oil  Nourishing Leave-In Cream,  10 ounces Bottle LLM Response (Accurate inference) Blush Blusher LLM Response (Vague or Keyword inference) Inference Fig. 3. An example of LLM inference from different views. commonly used behavioral modeling views in SBR (long-term and short-term). By artificially setting them, we decompose the text inference task into finer-grained view inference subtasks, thereby maximizing the utilization of LLM’s inference capabilities. It is worth noting that the view settings are not fixed and can be freely added or removed, endowing the framework with scalability. The specific prompt template is shown in figure 2, where we mark the template’s necessary components in blue, the view-limiting qualifier in red, and the composition of the sequence items in green. A prompt consists of two parts: [Background Description and Task Definition]. Each item in the sequence of the background description consists of three components: < Number +Name +ID >. The \"Number\" indicates the order of the items, while the \" Name\" represents the textual modality information. Some studies [ 21, 30] suggest that ID information helps LLM distinguish between different items more accurately. Inspired by this, we incorporate corresponding ID information after the item names in the prompt design. 4.1.2 LLM Inference. To enhance the effective utilization of semantic information and understand the genuine intent of sessions, we leverage the contextual understanding and logical inference capabilities of LLMs to achieve ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:8 • Shutong Qiao et.al. Macadamia Oil Nourishing  Leave-In Cream,  10 ounces Bottle 1. Macadamia Oil Nourishing Leave-In  Cream, 10 ounces Bottle 2. Nourishing Leave-In Cream by  Macadamia for Unisex Cream, 2 Ounce 3. Macadamia Oil Rejuvenating Shampoo,  10 ounces Bottle 4. 100% Pure Pink Grapefruit Nourishing  Body Cream (8 oz Tube) 5. Mizani Moisturizing Leave-In Milk,  D'tangle, 8.5 Ounce Results before  intent localization Results after  intent localization Fig. 4. The result of intent localization module. intent inference from different views. Figure 3 shows an example of LLM reasoning based on prompts from different views. It is worth noting that the LLM is interchangeable here. LLMs with more parameters and stronger inference capabilities can produce more accurate inference results. We adopt the question-and-answer format, input different view prompts as questions to the LLM, and then the LLM returns its inferring results according to the prompts. Textinfer = LLM(promptp) (1) 4.1.3 Intent Localization. To assist LLM in alleviating hallucinations and achieving semantic enhancement, we designed the intent localization module. Although in most cases, the LLM inference result is an accurate item name, sometimes it may be just a vague item category or key item term. In rare cases, a reasonable inference result may not be obtained. The red portion in Figure 4 illustrates the initial inference results of LLM. Inspired by the RAG retrieval model [27], alleviating hallucinations in LLM requires providing relevant external knowledge to LLM. The text retrieval scheme of the RAG model is usually based on the similarity of text embeddings, so we first encode all inference results and the text of the item set into embedding forms using a pre-trained BERT model [12]1. 𝐸infer = Bert(Textinfer), (2) 𝐸item = Bert(Textitem), (3) where 𝑒infer,𝑒item ∈R𝑑text . Then, we compute the cosine similarity scores between each inference result and all item embeddings. Utilizing text embedding similarity, we select the Top-𝑟 most similar actual items from the item set, where 𝑟 is a hyperpa- rameter that controls the number of semantically similar items to be filtered. We multiply the embeddings of selected items by their corresponding similarity scores and then sum them up to obtain the inference result of the LLM. Figure 4 illustrates the comparison of inference results before and after using the intent localization module. Finally, the inference results of each view are passed through this module to alleviate hallucinations and enhance semantics. Similarity𝑖 = 𝑒𝑖 infer ·𝑒𝑗 item ∥𝑒𝑖 infer ∥∥𝑒𝑗 item ∥ , (4) ℎinfer = ∑︁ 𝑖𝑟 ∈𝐼𝑟 Similarity𝑖𝑟 𝑒𝑖𝑟 item, (5) 1https://huggingface.co/bert-base-uncased ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:9 Algorithm 1 Process details of the LLM4SBR framework. Input: prompts(Short & Long views), 𝐼𝑡, Textitem, Output: ˆ𝑦𝑖 1: 𝐸item = BERT(Textitem) 2: for each prompt 𝑝 ∈{𝑃𝑠ℎ𝑜𝑟𝑡,𝑃𝑙𝑜𝑛𝑔}do 3: Textinfer = LLM(𝑝) 4: 𝐸infer = BERT(Textinfer) 5: Similarity𝑖 = 𝑒𝑖 infer ·𝑒𝑗 item ∥𝑒𝑖 infer ∥∥𝑒𝑗 item ∥ 6: ℎinfer = Í 𝑖𝑟 ∈𝐼𝑟 Similarity𝑖𝑟 𝑒𝑖𝑟 item, where 𝐼𝑟 is the set of the most similar 𝑟 items. 7: end for 8: 𝐻𝑙 𝑡,𝐻𝑔 𝑡 = SBR-Model(I𝑡) 9: Initial L𝑎 = 0,L𝑢 = 0 10: for each views p (long-term & short-term) do 11: ˜ℎp infer = Tℎp infer 12: Lp 𝑎 = E (infer,t)∼𝑠𝑒𝑠𝑠 || ˜ℎp infer −ℎp t ||2 13: Lp 𝑢 = log 𝑒 −2|| ˜ℎp infer − ˜ℎp infer′|| 2 /2 +log 𝑒 −2||ℎp 𝑡 −ℎp 𝑡′||2 /2 14: L𝑎+= Lp 𝑎, L𝑢+= Lp 𝑢 15: end for 16: L𝑎 = ¯L𝑎,L𝑢 = ¯L𝑢 17: 𝛼lt infer = QT 1 (sigmoid(𝐻𝑔 t +𝐻lt infer)) 18: 𝛼st infer = QT 2 (sigmoid(𝐻𝑙 t +𝐻st infer)) 19: 𝐻sess = W[𝐻𝑙 t ∗𝛼st infer; 𝐻𝑔 t ∗𝛼lt infer], 20: ˆ𝑦𝑖 = softmax(ℎT sess𝑣𝑖) 21: L𝑟 = −Í𝑛 𝑖=1 𝑦𝑖𝑙𝑜𝑔(ˆ𝑦𝑖)+( 1 −𝑦𝑖)𝑙𝑜𝑔(1 −ˆ𝑦𝑖) 22: L= L𝑟 +𝜏(𝐿𝑎 +𝐿𝑢) 23: return ˆ𝑦𝑖 where 𝑒𝑖 infer ∈𝐸infer is the text embedding of the inference result and 𝑒𝑗 item ∈𝐸item is the text embedding of the item name. The formula (4) represents the calculation of the cosine similarity scores between each inference result and all item names. 𝐼𝑟 denotes the set of the Top-𝑟 item indices with the highest similarity scores, in which 𝑟 is a hyperparameter. 4.2 Representation Enhancement Stage After the intent inference stage, we move into the representation enhancement phase. In this stage, the SBR model processes behavioral modeling data and parsed inference data. Subsequently, the alignment and uniformity of session embeddings and inference embeddings are conducted separately for each view. Ultimately, all view inference embeddings are fused with session embeddings to form the final session representation used for prediction. Most of the state-of-the-art (SOTA) models in RS are currently based on the item-ID paradigm. Although this paradigm may sacrifice semantic information, its performance and efficiency are undeniably superior. There is still a long way to go to subvert the ID paradigm. [ 57] Therefore, we opt to model user behavior based on the item-ID paradigm while simultaneously injecting multimodal information for supplementary enhancement. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:10 • Shutong Qiao et.al. The SBR model in the framework is interchangeable. In the subsequent experimental section, we also test the performance after replacing SR-GNN with other SBR models. 4.2.1 SBR Modeling. In this section, we use the SBR model to model interactive information in conversation sequences and learn user behavior preferences. The SBR model here can be replaced arbitrarily. Given that SR- GNN [50] stands as one of the classic models in SBR, and the state-of-the-art (STOA) models in SBR predominantly rely on GNN, this model holds significant importance. Therefore, we primarily select it as the prototype SBR model within the framework for the experimental segment. Specifically, SR-GNN constructs session data into a session graph, where each node in the graph represents a unique item in the session. It utilizes GGNN to learn node features, then takes the last clicked item in the session as the local embedding of the session. It aggregates all node information and utilizes a soft attention mechanism to represent global preferences. 𝐻𝑙 𝑡,𝐻𝑔 𝑡 = SBR-Model(I𝑡), (6) where I𝑡 ⊆I represents the set of items interacted with in session at time 𝑡. 𝐻𝑙 𝑡, and 𝐻𝑔 𝑡 represent the local embedding and global embedding of session 𝑡 respectively. 4.2.2 Representation Alignment and Fusion. The SBR model models the interaction information in a session, while the LLM uses its knowledge to infer the semantic content corresponding to the session. Although both have the same goal, they are not in a unified embedding space. Here we draw on the approach of DirectAU [44] to unify the SBR and LLM representations into a common space to achieve alignment and unification of the representations in different viewpoints. ˜ℎp infer = Tℎp infer, (7) L𝑎 = E (infer,t)∼𝑠𝑒𝑠𝑠 || ˜ℎp infer −ℎp t ||2, (8) L𝑢 = log 𝑒 −2|| ˜ℎp infer − ˜ℎp infer′|| 2 /2 +log 𝑒 −2||ℎp 𝑡 −ℎp 𝑡′||2 /2, (9) where the matrix T ∈R𝑑×𝑑text transforms the embedding vectors of the text modal into the latent space R𝑑, L𝑎 denotes alignment loss function and L𝑢 denotes uniformity loss function. For each view (long-term, short-term), we separately compute the alignment loss between the inference representation and session representation under that view and the uniform loss within each inference representation and each session representation. We then fuse session representations from different views and modalities into a final session representation through a soft-attention mechanism. 𝛼lt infer = QT 1 (sigmoid(𝐻𝑔 t +𝐻lt infer)), (10) 𝛼st infer = QT 2 (sigmoid(𝐻𝑙 t +𝐻st infer)), (11) 𝐻sess = W[𝐻𝑙 t ∗𝛼st infer; 𝐻𝑔 t ∗𝛼lt infer], (12) where parameters Q1,Q2 ∈R𝑑 and W ∈R𝑑×2𝑑 transforms the concatenated embedding vectors into a latent space R𝑑. 𝐻𝑙 t is the local preference representation obtained in the SBR model, where the local preference embedding is simply defined as the last clicked item. 𝐻𝑔 t is the global embedding obtained by the SBR model, which is obtained by the soft attention mechanism. For details, please see SR-GNN [50]. Additionally, 𝐻st infer and 𝐻lt infer represent the short-term and long-term view text embeddings of LLM inference, respectively. Finally, we concatenate the weighted local and global representations into the final session representation 𝐻sess. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:11 4.2.3 Prediction and Optimization. By taking the item of the session representation and the item representation, scores for each candidate item are obtained. Then, the softmax function is applied to obtain the model’s predicted values 𝑌. ˆ𝑦𝑖 = softmax(ℎT sess𝑣𝑖), (13) where ˆ𝑦𝑖 represents the probability that each item in the itemset becomes the next item in the current session. The loss function for SBR tasks is defined as the cross-entropy between the predicted values and the ground truth, as shown below: L𝑟 = − 𝑛∑︁ 𝑖=1 𝑦𝑖𝑙𝑜𝑔(ˆ𝑦𝑖)+( 1 −𝑦𝑖)𝑙𝑜𝑔(1 −ˆ𝑦𝑖), (14) where 𝑦is the one-hot encoding vector of the ground truth item. Ultimately, the joint learning loss function is composed of both the recommendation loss function and the auxiliary task (alignment and uniformity) loss function. L= L𝑟 +𝜏(L𝑎 +L𝑢), (15) where 𝜏 controls the proportion of auxiliary tasks. 4.3 Discussion Ease-of-Use: One key aspect that makes LLM4SBR easy to use is its modular design. The separation of Intent Inference and Representation Enhancement stages allows developers to integrate or modify individual components without necessarily disrupting the entire system. Moreover, LLM4SBR retains the data interface of the traditional SBR model, which means that you only need to prepare the data required for LLM inference to seamlessly transition to the training phase without making large-scale changes to the existing workflow. This makes the migration from the old system to the new model smoother and more efficient. Plug-and-Play Compatibility: The LLM4SBR framework utilizes SBR models to process interaction data, ensuring that advances in session modeling can be easily adopted without the need for radical modifications. This compatibility encourages researchers to experiment with different SBR models as “plug-ins, ” choosing the most appropriate model based on their specific application requirements or performance benchmarks. Additionally, the LLM in the LLM4SBR framework can also be updated and does not require dedicated pre-training and fine-tuning. As research in the LLMs continues to advance, the LLMs in the framework can be replaced by newer and more powerful models. This feature enables it to adapt to the ever-changing research environment and technological advances. 5 Experiments In this section, we design a series of experiments to answer the following four questions: •RQ1: Can the LLM4SBR framework improve the performance of the SBR model? •RQ2: Is each component in the LLM4SBR framework necessary and what is its impact on the overall performance? •RQ3: How do hyperparameter settings affect performance in LLM4SBR? •RQ4: Is the LLMSBR framework lightweight? 5.1 Experimental Settings 5.1.1 Datasets. We initially hoped to use datasets commonly used in SBR, such as Diginetica, Nowplaying, and RetailRocket, etc., to validate performance as they are more representative. Unfortunately, none of these datasets provide both interaction ID sequences and item name information. Consequently, we selected the classic datasets Beauty and MovieLens-1M (Ml-1M) in the Sequential Recommendation, and adapted them to a session format. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:12 • Shutong Qiao et.al. Table 2. Statistics of the utilized datasets. Datasets Train Test Clicks Items Avg.len. Beauty 158,139 18,000 198,502 12,101 8.66 Ml-1M 47,808 5,313 987,610 3,416 17.59 The details of these two datasets are shown in Table 2. For both datasets, we adhere [ 28, 50] to removing sessions with a length of 1 and items that appear fewer than 5 times across all sessions. •Beauty 2 dataset comprises evaluations and ratings from users on various beauty items. We treat all ratings sequences from a single user as a session sequence. We enhance the dataset using the commonly employed se- quence segmentation method [28, 36, 50] in SBR. For instance, consider an original session𝑠 = [𝑖𝑡,1,𝑖𝑡,2,··· ,𝑖𝑡,𝑛]. After segmentation by sequence, we obtain ([𝑖𝑡,1],𝑖𝑡,2),([𝑖𝑡,1,𝑖𝑡,2],𝑖𝑡,3), ··· ,([𝑖𝑡,1,𝑖𝑡,2,··· ,𝑖𝑡,𝑛−1],𝑖𝑡,𝑛). •Ml-1M 3 dataset consists of over1 million ratings from more than6,000 users on over4,000 movies. Considering that our research problem is SBR, we have performed special processing on the sequence dataset by using 10- minute intervals as segmentation points to divide the user sequence into multiple session sequences. 5.1.2 Evaluation metrics. In terms of the evaluation indicators used in the experiment, We chose the most commonly used ones in SBR tasks: Precision (P)@𝐾, Mean reciprocal rank (MRR)@𝐾and Normalized Discounted cumulative gain (NDCG) @𝐾. After referring to the classic work [ 22, 48, 50, 51] 4 in recent years, we set the length of the candidate set @𝐾 to 5, 10, and 20, which is the most meaningful for comparison. 5.1.3 Implementation details and Hyper-parameter settings. All experiments were conducted on NVIDIA A100 GPUs. In the aspect of selecting large models, we have chosen the Qwen-7B-Chat 5 model as the inference model after careful consideration of LLM’s inference capability, adaptability to both Chinese and English languages and model parameter count. For fairness in performance comparison, the optimizer used throughout the experiments was unified as Adam with a learning rate of 0.001, decayed by 0.1 every three epochs, and an 𝐿2 penalty set to 10−5. For the SBR model involved in the experiments, the batch size is100 and the dimension size is 100. 𝜏is set to 0.1. We initially set the hyperparameter 𝑟 in the intent localization module to 5, and subsequent hyperparameter experiments 5.4 will discuss the optimal value. We followed the optimal parameter settings as published in their paper for the remaining parameters. 5.2 Performance Experiment and Analysis (RQ1) In this section, we mainly compare the performance of the SBR model and the corresponding SBR model applying the LLM framework under different Top-𝐾. 5.2.1 Backbones. To validate the effectiveness of the framework, we carefully selected six classic models in the field of Session-based Recommendation/Sequential Recommendation, with the SBR model as its core component. Note that here we have not chosen to compare performance with a model that incorporates LLM. This is because there is very little work on SBR incorporating LLM, and no open-source code exists. Second, the architecture of our work is different from other work, focusing on enhancements to the traditional SBR model rather than from the view of replacing SBR with LLM. Among them, the GRU4Rec and SASRec models are particularly good at mining sequences’ temporal dynamics and patterns. In contrast, SRGNN and TAGNN focus more on revealing the transfer or transformation relationships between items. The GCE-GNN and DHCN models, based on effectively capturing 2https://jmcauley.ucsd.edu/data/amazon/links.html 3https://grouplens.org/datasets/movielens/ 4The calculation method of 𝑃@20 and 𝑀𝑅𝑅@20 refer to the [50], and 𝑁𝐷𝐶𝐺@𝐾 refers to [22]. 5https://github.com/QwenLM/Qwen ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:13 Table 3. Performance comparison experimental results (%). Dataset Beauty Ml-1M Model 𝑃@5 𝑃@10 𝑃@20 𝑀𝑅𝑅@5 𝑀𝑅𝑅@10 𝑀𝑅𝑅@20 𝑃@5 𝑃@10 𝑃@20 𝑀𝑅𝑅@5 𝑀𝑅𝑅@10 𝑀𝑅𝑅@20 GRU4Rec 4.99 8.18 12.71 2.71 3.06 3.23 5.48 9.52 14.72 2.98 3.40 3.57 LLM4SBR(GRU4Rec) 7.50 11.68 16.74 4.51 4.89 4.99 6.66 10.91 17.20 3.97 4.38 4.52 GRU4Rec Improv. 50.30% 42.96% 31.68% 66.42% 60.32% 54.60% 21.53% 14.70% 16.88% 33.22% 28.71% 26.72% SASRec 3.93 6.40 10.11 2.09 2.37 2.54 2.07 3.67 6.47 1.07 1.27 1.42 LLM4SBR(SASRec) 5.29 8.50 13.20 2.98 3.33 3.49 3.44 6.06 10.05 1.72 2.02 2.21 SASRec Improv. 34.6% 32.81% 30.56% 42.58% 40.51% 37.94% 66.18% 65.12% 55.33% 60.74% 59.06% 55.63% SR-GNN 6.23 10.06 15.06 3.45 3.84 3.99 4.42 7.53 12.27 2.44 2.80 2.98 LLM4SBR(SR-GNN) 7.78 11.73 16.98 4.71 5.03 5.12 8.13 11.80 18.35 4.85 5.16 5.27 SR-GNN Improv. 24.87% 16.60% 12.74% 36.52% 30.98% 28.32% 83.93% 56.70% 49.55% 98.77% 84.28% 76.84% TAGNN 6.12 10.06 15.23 3.10 3.63 3.97 3.60 6.19 10.28 1.77 2.15 2.23 LLM4SBR(TAGNN)7.79 11.79 16.76 4.39 4.78 5.05 7.47 12.33 18.60 4.03 4.79 4.87 TAGNN Improv. 27.28% 17.19% 10.04% 41.61% 31.68% 27.20% 107.5% 99.19% 80.93% 127.68% 122.79% 118.38% GCE-GNN 6.39 8.93 12.38 3.97 4.30 4.54 5.16 6.85 9.67 3.18 3.41 3.60 LLM4SBR(GCE-GNN) 7.7512.48 18.08 3.91 4.41 4.80 7.10 13.44 22.10 3.14 3.63 4.21 GCE-GNN Improv. 21.28% 39.75% 46.04% -1.51% 2.56% 5.73% 37.59% 96.20% 128.54% -1.25% 6.45% 16.94% 𝑆2-DHCN 7.14 11.97 17.54 2.97 3.61 3.99 8.35 14.55 23.38 3.66 4.51 5.09 LLM4SBR(𝑆2-DHCN) 7.77 11.85 17.48 4.26 4.79 5.15 9.54 15.31 22.67 5.13 5.91 6.40 𝑆2-DHCN Improv. 8.82% -1.00% -0.34% 43.43% 32.68% 29.07% 14.25% 5.22% -3.03% 40.16% 31.04% 25.73% * We highlight the best performance values for each metric in bold and underscore the best values within the backbones. * The calculation formulas for 𝑃@𝐾, 𝐻𝑅@𝐾 and 𝑅𝑒𝑐𝑎𝑙𝑙@𝐾 are the same in SBR. item transformation relationships, further incorporate the concept of multi-graph structure and introduce the use of inter-session information to enhance the model’s ability to understand complex interaction patterns. In the experimental section, we assess the performance of each model both before and after enhancement with the framework and systematically compare and analyze the performance disparities among the enhanced models. The introduction of the SBR models is as follows: •GRU4Rec [19] utilizes GRU to learn dependencies between sequences to predict the next likely item of interest. •SASRec [25] uses a self-attention mechanism to solve the long-term dependency problem existing in traditional sequence models. •SR-GNN [50] is the first model to construct data into session graphs, utilizing GGNN to capture complex transition relationships among items. •TAGNN [53] adds a target-sensitive attention mechanism based on SR-GNN. •GCE-GNN [48] constructs session graphs and global graphs respectively, and learns relevant information from the item level and session level. •𝑆2-DHCN [51] uses hypergraph convolution to learn high-order relationships in item sequences, and uses self-supervised learning to alleviate the data sparse problem of hypergraphs. The comparison results of the overall performance experiments are shown in Table 3. We record the performance with K set to 5, 10, 20. It is worth noting that smaller𝐾 values are more significant in the evaluation system of RS. From the results displayed in Table 3, we draw the following observations: •LLM4SBR significantly improves backbone performance. In the models enhanced through the LLM framework, both sequence and graph-structured models show significant performance improvements. For example, the 𝑃@5 of SASRec and SR-GNN on the Ml-1M dataset have increased by 66.18% and 83.93%, respectively. This confirms that the text representations derived from LLM inference contain rich and valuable information, which can greatly help the SBR model understand the potential intention of the conversation data. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:14 • Shutong Qiao et.al. •LLM4SBR has a greater improvement for smaller 𝐾 values. Almost all backbone models achieve larger improvements at smaller 𝐾 values when combined with the framework. For example, LLM4SBR (TAGNN) improved the 𝑃@5 index of the two data sets by 27.28% and 107.5% respectively. We believe this is due to the semantic enhancement achieved by LLM4SBR during the intent localization stage, where it utilizes 𝑟 similar semantic items. Consequently, it results in more accurate predictions for the top few items in the predicted candidate set. We also observe slight decreases in performance for 𝑆2-DHCN and GCE-GNN on a few metrics (𝑃@20 and 𝑀𝑅𝑅@20) after integrating with the framework. We posit that when the original SBR model already effectively models the data, enhancing the inference information through the intent localization module may introduce noise. Compared to the improvement magnitude, the decrease is very slight. Moreover, since noise issues can be effectively controlled by adjusting the hyperparameter 𝑟 in the intent localization module, the negative impact can be almost negligible. •The LLM4SBR framework can effectively alleviate modeling defects caused by scene mismatch and data sparseness and fully activate the inherent potential of the model. Due to the short sequence characteristics of session data, although SASRec is good at capturing long-term dependencies, this becomes a limitation in short sequence scenarios, resulting in poor performance. However, after being integrated into the LLM4SBR framework, its shortcomings in short-sequence processing have been significantly compensated, and its performance has been greatly improved. This achievement not only reflects the efficiency of framework optimization and collaborative model capabilities but also proves that framework integration can effectively enhance the model potential in specific scenarios. GCE-GNN captures effective information at both the item and session levels by constructing global graphs and session graphs simultaneously, due to the model’s complex computations, in scenarios with limited data volume, it becomes challenging for this model to learn effective session representations. LLM4SBR (GCE-GNN) showed the greatest improvement, especially on the Ml-1M dataset, 𝑃@5, 𝑃@10, and 𝑃@20 increased by 37.59%, 96.2%, and 128.54% respectively. We attribute this to the effective text information obtained from LLM inference, which compensates for the information scarcity in GCE-GNN’s session modeling, allowing it to achieve better performance. In conclusion, the effectiveness of the LLM4SBR framework is undeniable. As a plug-and-play framework, it significantly enhances the prediction accuracy of traditional SBR models. 5.2.2 Comparison with SBR Models Combining LLMs. In this subsection, we further compare the performance of the LLM4SBR framework with other frameworks that combined with LLM to explore the enhancement capability of LLM in LLM4SBR for SBR models. Note that here we use the same backbone, SRGNN. the specific model is described below: •LLM-Infer directly predicts the next click using LLM inference of long-term and short-term interests after intent localization. •LLMSeqSim[17] retrieves semantically rich embeddings for each item in the session from existing LLMs and computes aggregated session embeddings to recommend products with similar embeddings. •LLM2SRGNN [6]6 initializes the SBR model with the item embeddings obtained from LLM. In Figure 5, we show the original SR-GNN model in yellow. It can be clearly seen from the figure that LLM-Infer represented in gray and LLMSeqSim represented in blue are significantly lower in performance than SR-GNN, which effectively models collaborative information. This observation demonstrates that although LLMs perform well on many NLP tasks, they are not suitable for direct application in recommendation tasks. At the same time, the other two frameworks (LLM2SRGNN and LLM4SBR) that use LLM to enhance the collaborative model in the figure have achieved significant performance improvements compared to the backbone (SR-GNN). This experimental result confirms that combining LLM with the collaborative model is a wise direction. LLM can 6We used the LLM2Sequential framework but replaced the SR model backbone with SR-GNN to ensure the fairness of the experiments. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:15 /uni00000033/uni00000023/uni00000018/uni00000033/uni00000023/uni00000014/uni00000013/uni00000033/uni00000023/uni00000015/uni00000013 /uni00000033/uni00000023/uni0000002e /uni00000013/uni00000011/uni00000013 /uni00000015/uni00000011/uni00000018 /uni00000018/uni00000011/uni00000013 /uni0000001a/uni00000011/uni00000018 /uni00000014/uni00000013/uni00000011/uni00000013 /uni00000014/uni00000015/uni00000011/uni00000018 /uni00000014/uni00000018/uni00000011/uni00000013 /uni00000014/uni0000001a/uni00000011/uni00000018/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni0000002f/uni0000002f/uni00000030/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004c/uni00000050/uni0000002f/uni0000002f/uni00000030/uni00000010/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000036/uni00000035/uni00000010/uni0000002a/uni00000031/uni00000031/uni0000002f/uni0000002f/uni00000030/uni00000015/uni00000036/uni00000035/uni0000002a/uni00000031/uni00000031/uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni0000000b/uni00000036/uni00000035/uni00000010/uni0000002a/uni00000031/uni00000031/uni0000000c /uni00000030/uni00000035/uni00000035/uni00000023/uni00000018/uni00000030/uni00000035/uni00000035/uni00000023/uni00000014/uni00000013/uni00000030/uni00000035/uni00000035/uni00000023/uni00000015/uni00000013 /uni00000030/uni00000035/uni00000035/uni00000023/uni0000002e /uni00000013 /uni00000014 /uni00000015 /uni00000016 /uni00000017 /uni00000018/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni0000002f/uni0000002f/uni00000030/uni00000036/uni00000048/uni00000054/uni00000036/uni0000004c/uni00000050/uni0000002f/uni0000002f/uni00000030/uni00000010/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000036/uni00000035/uni00000010/uni0000002a/uni00000031/uni00000031/uni0000002f/uni0000002f/uni00000030/uni00000015/uni00000036/uni00000035/uni0000002a/uni00000031/uni00000031/uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni0000000b/uni00000036/uni00000035/uni00000010/uni0000002a/uni00000031/uni00000031/uni0000000c Fig. 5. Performance Comparison with SBR Models Combining LLMs. provide rich semantic modality information for the collaborative model that is limited by data sparsity. It is particularly noteworthy that LLM4SBR achieved the best results among all models, and achieved a significant performance improvement compared to the second-best SR-GNN, which proves the rationality and effectiveness of our framework design. We conclude that it is not enough to simply introduce knowledge from LLM, but also to effectively integrate information from different modalities and improve the effective utilization of information to achieve more accurate recommendation results. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:16 • Shutong Qiao et.al. Table 4. The ablation results of LLM and SBR models in the LLM4SBR framework. Dataset Beauty Ml-1M Model w/o SBR w/o LLM w/o AU LLM4SBR w/o SBR w/o LLM w/o AU LLM4SBR 𝑃@5 1.92 6.23 7.12 7.78 1.64 4.42 6.31 8.13 𝑃@10 3.44 10.06 10.75 11.73 2.90 7.53 10.07 11.80 𝑃@20 5.76 15.06 15.74 16.98 5.38 12.27 15.11 18.35 𝑀𝑅𝑅@5 0.95 3.45 4.39 4.71 0.83 2.44 3.85 4.85 𝑀𝑅𝑅@10 1.14 3.84 4.69 5.03 0.99 2.80 4.20 5.16 𝑀𝑅𝑅@20 1.27 3.99 4.79 5.12 1.13 2.98 4.34 5.27 𝑁𝐷𝐶𝐺@5 1.21 4.27 5.21 5.64 1.04 3.02 4.54 5.89 𝑁𝐷𝐶𝐺@10 1.69 5.38 6.22 6.67 1.44 3.98 5.60 6.90 𝑁𝐷𝐶𝐺@20 2.23 6.39 7.16 7.64 2.03 4.97 6.65 8.10 5.3 Ablation Study (RQ2) 5.3.1 The impact of LLMSBR framework integration on performance. To test the integration effect of the LLM4SBR framework, we design three variants as follows: •LLM4SBR w/o SBR - using the long-term and short-term interest results of LLM inference after intention localization for direct prediction. •LLM4SBR w/o LLM - using the SBR model (here we use SR-GNN) for direct prediction. •LLM4SBR w/o AU - removing the representation alignment operation, and directly fuses the LLM inference representation with the long and short-term session representation constructed by SBR. We added 𝑁𝐷𝐶𝐺@𝐾 as an evaluation criterion on top of the 𝑃@𝐾 and 𝑀𝑅𝑅@𝐾 evaluation metrics. Then we compared the performance of these three variants with the full version of LLM4SBR on the Beauty and Ml-1M datasets. The experimental results are shown in Table 4, where we marked the best-performing model in bold. By observing and analyzing the results in Table 4, we draw the following conclusions: •LLM inference results are ineffective when directly used for recommendation tasks. In the experi- mental results shown in Table 4, the LLM4SBR w/o SBR variant performed poorly, ranking last across all evaluation metrics. This finding is consistent with previous research [5, 14], which suggests that there is a significant difference between pre-trained LLMs and the tasks required for the RS, leading to their limited effectiveness when directly applied to SBR scenarios. RSs require models to understand user historical behavior patterns, capture real-time changes in user interests, and effectively match associations within a high-dimensional item space. In contrast, LLMs primarily focus on coherence and meaning expression within language structures, lacking the in-depth understanding of domain-specific knowledge and context required for the recommendation field. Therefore, the core of this study is the innovative integration strategy, which aims to combine the powerful language understanding of LLM with the targetedness of traditional recommendation algorithms to build a more efficient and intelligent RS. •The LLM4SBR framework effectively combines the advantages of SBR and LLM. The SBR model using the LLM4SBR framework has the best performance in Table 4, in which the 𝑁𝐷𝐶𝐺@𝐾 metrics are significantly enhanced. This improvement demonstrates the advantages of LLM4SBR in accurately capturing user interests and proves that the LLM4SBR framework can skillfully integrate the expertise of LLM and SBR to achieve high efficiency in collaborative modeling. The second best performance of all metrics is the LLM4SBR w/o AU, which firstly proves the validity of the operation of split-view representation alignment in the framework, which is more scientifically sound to align and uniform the representations of different modalities in separate views before fusing them, than to directly fuse the different modal representations, ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:17 P@5 P@10 P@20 Metrics 0 2 4 6 8 10 12 14 16Performance /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035 /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni0000004b/uni00000052/uni00000055/uni00000057 /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000052/uni00000051/uni0000004a /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000010/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f (a) 𝑃@𝐾 on Beauty. MRR@5 MRR@10 MRR@20 Metrics 0 1 2 3 4 5Performance /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035 /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni0000004b/uni00000052/uni00000055/uni00000057 /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000052/uni00000051/uni0000004a /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000010/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f (b) 𝑀𝑅𝑅@𝐾 on Beauty. P@5 P@10 P@20 Metrics 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5Performance /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035 /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni0000004b/uni00000052/uni00000055/uni00000057 /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000052/uni00000051/uni0000004a /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000010/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f (c) 𝑃@𝐾 on Ml-1M. MRR@5 MRR@10 MRR@20 Metrics 0 1 2 3 4 5Performance /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035 /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni0000004b/uni00000052/uni00000055/uni00000057 /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000052/uni00000003/uni0000002f/uni00000052/uni00000051/uni0000004a /uni0000002f/uni0000002f/uni00000030/uni00000017/uni00000036/uni00000025/uni00000035/uni00000010/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f (d) 𝑀𝑅𝑅@𝐾 on Ml-1M. Fig. 6. The ablation results from different inference views. and secondly, this variant proves once again that the results of the LLM inference can indeed significantly enhance the performance of the SBR, even without the representation alignment operation, it also performs better than the modeling approach using unimodal information. Through the LLM4SBR framework, the SBR model can not only deeply understand the text context and extract more detailed and comprehensive user preferences, but also effectively handle sequence dependencies in conversational recommendation scenarios, thereby generating more personalized recommendation results. 5.3.2 The influence of LLM inference results from different views. To examine the necessity and relative importance of the long-term and short-term inference views, we designed three variants as follows: •LLM4SBR w/o Long indicates inference without considering the long-term view, retaining only the short-term view. •LLM4SBR w/o Short retains only the long-term view and removes the short-term view during inference. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:18 • Shutong Qiao et.al. /uni00000013/uni00000014/uni00000016/uni00000018 /uni00000055 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni0000001a/uni00000011/uni00000017/uni00000017/uni0000001a/uni00000011/uni00000016/uni00000015/uni0000001a/uni00000011/uni0000001a/uni00000013/uni0000001b/uni00000011/uni00000014/uni00000016 /uni00000017/uni00000011/uni00000018/uni00000013/uni00000017/uni00000011/uni00000018/uni0000001a/uni00000017/uni00000011/uni00000018/uni00000019/uni00000017/uni00000011/uni0000001b/uni00000018 /uni00000033/uni00000023/uni00000018 /uni00000030/uni00000035/uni00000035/uni00000023/uni00000018 (a) The result of 𝑃@5 and 𝑀𝑅𝑅@5 /uni00000013/uni00000014/uni00000016/uni00000018 /uni00000055 /uni00000013 /uni00000015 /uni00000017 /uni00000019 /uni0000001b /uni00000014/uni00000013 /uni00000014/uni00000015 /uni00000014/uni00000017/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni00000014/uni00000014/uni00000011/uni0000001a/uni0000001b/uni00000014/uni00000015/uni00000011/uni00000014/uni0000001a/uni00000014/uni00000014/uni00000011/uni0000001b/uni00000017/uni00000014/uni00000014/uni00000011/uni0000001b/uni00000013 /uni00000017/uni00000011/uni0000001b/uni0000001a/uni00000018/uni00000011/uni00000013/uni00000015/uni00000017/uni00000011/uni0000001c/uni00000019/uni00000018/uni00000011/uni00000014/uni00000019 /uni00000033/uni00000023/uni00000014/uni00000013 /uni00000030/uni00000035/uni00000035/uni00000023/uni00000014/uni00000013 (b) The result of 𝑃@10 and 𝑀𝑅𝑅@10 /uni00000013/uni00000014/uni00000016/uni00000018 /uni00000055 /uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048 /uni00000014/uni0000001b/uni00000011/uni00000014/uni00000016/uni00000014/uni0000001b/uni00000011/uni00000019/uni00000016/uni00000014/uni0000001b/uni00000011/uni0000001a/uni0000001b/uni00000014/uni0000001b/uni00000011/uni00000016/uni00000018 /uni00000017/uni00000011/uni0000001c/uni0000001a/uni00000018/uni00000011/uni00000014/uni00000016/uni00000018/uni00000011/uni00000013/uni0000001c/uni00000018/uni00000011/uni00000015/uni0000001a /uni00000033/uni00000023/uni00000015/uni00000013 /uni00000030/uni00000035/uni00000035/uni00000023/uni00000015/uni00000013 (c) The result of 𝑃@20 and 𝑀𝑅𝑅@20 Fig. 7. Hyperparameter experimental results of different 𝑟 settings of the intent localization module on Ml-1M. •LLM4SBR-Normal removes the view-limiting qualifier in prompt, and LLM directly infers the overall interest, which is then aligned and uniformed with the comprehensive session embedding learned by the traditional SBR model. We compared the performance of these three variants with the whole performance and visualized the comparison as a bar chart to clearly illustrate the differences between them. Through observation and analysis of Figure 6, we summarized the following conclusions: •Combining prompts from different views can improve the accuracy of LLM in inferring user interests, both long-term and short-term views are necessary. Because the whole framework represented by the blue column in the figure shows the best performance on both datasets. For example, on the Beauty, the 𝑃@5, 𝑃@10, and 𝑃@20 of LLM4SBR are 7.78, 11.73, and 16.98, respectively, while the corresponding w/o Long and w/o Short variants show varying degrees of decreases in all three of the above metrics(𝑃@5, 𝑃@10, 𝑃@20), with values of 7.32, 10.80, and 15.98, as well as 6.92, 10.59, and 15.90. This justifies the necessity for multi-view inference, where deleting any view results in a performance loss. In addition, the second best performing model is LLM-Normal, with 𝑃@5, 𝑃@10, and 𝑃@20 metrics of 7.50, 11.38, and 16.72 on the Beauty dataset, which indicates that inference from any single view is inferior to direct comprehensive inference, but combining prompts from different views can improve the accuracy of LLM’s inference of user interests and effectively improve the recommendation metrics. •The contribution of long-term and short-term view inference varies across the two datasets. Specifi- cally, on the Beauty dataset, the 𝑃@5 of LLM w/o Long and LLM w/o Short are 7.32 and 6.92 respectively, and the former performs better than the latter; on the Ml-1M dataset, the𝑃@5 of LLM w/o Long and LLM w/o Short are 6.90 and 7.22 respectively, and the former performs worse than the latter. In Beauty, the framework relies more on the information provided by the short-term view, as discarding the inference results of the short-term view would lead to a greater performance drop. Conversely, in Ml-1M, it’s the opposite; the framework relies more on the inference results of the long-term view. Through discussion and analysis, we attribute this perfor- mance difference to the length of the dataset sessions. Session intent in short sequences is usually relatively stable, and the intent is mainly reflected in the last few clicks. This underscores the increased importance of accurately modeling short-term interests in short-session scenarios. However, as the session length increases, the session intent is influenced by various factors, thereby increasing the importance of long-term dependency relationships within the session. Finally, we believe that simultaneously considering the inference results of multiple views can enhance the stability of the framework’s performance, making it adaptable to datasets with varying session lengths. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:19 •The contribution of view splitting is to enhance the directionality of LLM. The performance of LLM4SBR-Normal is the second best on both datasets, and the performance degradation is not significant. We analyzed the experimental results and believe that there are two main reasons. First, LLM itself has strong reasoning ability. Without clear guidance, LLM can combine global context information and summarize the user’s overall interests on its own. Second, the long-term and short-term view reasoning is mainly used to enhance LLM’s fine-grained understanding of user interests, rather than completely changing its reasoning method. By introducing different view qualifiers in the prompts, the framework can more accurately guide LLM to infer users’ short-term preferences and long-term interests, supplement the missing long-tail items, and thus give full play to its reasoning ability. In conclusion, within the LLM4SBR framework, each module is indispensable. The LLM4SBR framework successfully integrates the strengths of LLM and SBR models by leveraging LLM for multi-view interest infer- ence and aligning semantic and collaborative information from corresponding views, significantly enhancing recommendation performance. 5.4 Hyperparameter Experiment and Analysis (RQ3) In this section, we discuss the hyperparameter 𝑟 set within the intent localization module. This hyperparameter is designed to alleviate hallucination and enhance semantics in the preliminary inference results of LLM, using a candidate set of items with similar semantics. The reason we chose to conduct this experiment on the Ml-1M dataset is that this dataset does not use the data enhancement method of sequence segmentation, which is more reflective of the actual effect of precise localization and semantic enhancement in real scenarios using items with similar semantics. The hyperparameter 𝑟 is configured to control the range of selecting items with similar semantics. The value of 𝑟 is set to 0, 1, 3, and 5, and we discuss four scenarios accordingly: (1) directly utilizing the inference results of LLM; (2) using the most similar 1 items to alleviate hallucination and enhance semantics; (3) using the most similar 3 items; (4) using the most similar 5 items. The experimental results are shown in Figure 7. Firstly, across all three subfigures, although the optimal hyperparameter values differ for each subfigure, it can be seen that in most cases, the performance is the worst when 𝑟 = 0. We believe this is logical and demonstrates the necessity of the intent localization module in the framework. If the results of LLM inference are not processed, hallucinations occurring in some session data may decrease the overall framework performance. Moving on to each of the figures, in Figure 7a, the performance enhances as 𝑟 increases, peaking at 𝑃@5 and 𝑀𝑅𝑅@5 when 𝑟 = 5, with 𝑃@5 and 𝑀𝑅𝑅@5 at 8.13 and 4.85, respectively. In Figures 7b and 7c, the peaks are achieved when 𝑟 = 1 or 𝑟 = 3, respectively. However, 𝑟 has little effect on those metrics. We believe that this is because the 𝑟-value range set by the framework is small (no more than 5), so the intent Localization module only screens the top few items in text similarity for semantic enhancement. Therefore, when evaluating the Top-𝐾 metrics, it has a positive impact on smaller 𝐾 values but has no obvious impact on larger 𝐾 values. In summary, 𝑟 values from 1 to 5 are valid. A smaller value of 𝑟 can play the efficacy of pinpointing, and a larger value of 𝑟 can improve the accuracy of the whole recommendation list through semantic enhancement. Depending on the actual situation, choosing different 𝑟 values can better utilize the effectiveness of the module. 5.5 Model training space occupancy experiment and Time complexity analysis (RQ4) Training LLM-based recommendation models typically require a significant amount of GPU resources and longer training time. To explore the spatial and temporal effectiveness of the LLM4SBR framework, We record the GPU usage during training, the training time for a single epoch, and the training time required to achieve the best performance for SR-GNN and LLM4SBR (SR-GNN). ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:20 • Shutong Qiao et.al. Table 5. The results of GPU usage and training time of model training on the Beauty and Ml-1M datasets. Dataset Beauty Ml-1M Model SR-GNN LLM4SBR (SR-GNN) SR-GNN LLM4SBR (SR-GNN) GPU Usage (MB) 1,262 1,282 1,290 1,324 Single Epoch Time (s) 398.08 1490.49 124.89 350.80 Training Time (s) 2085.67 2987.54 932.10 1406.79 The results are shown in Table 5. First, we can observe that the GPU occupancy rates of the original SR-GNN and the model using the LLM4SBR framework are very close. We believe that this is because the LLM4SBR framework only requires model training in the second stage, and this process does not require the participation of LLM. The SBR model only needs to parse the LLM inference results into tensor form to participate in training, thus saving a lot of training time and GPU resources. Second, looking at the combined single epoch training time and the time for the model training to reach optimal performance, it can be observed that although the training time of the model using the LLMSBR framework for a single epoch maybe 3-4 times longer than that of the original model, the training time to reach the optimal performance is not much different in comparison with the original model, which illustrates the fact that although the model using the LLM4SBR framework will increase the training time for a single epoch, the model can converge faster and reach optimal performance. In addition, we analyze the time complexity of the model. In the first stage of LLM4SBR, each prompt corresponds to a conversation sequence. It is assumed that there are 𝑛such prompts, and the length of each prompt is quite short compared to n, which means that the time for a single inference can be regarded as constant time 𝑂(1). Given that there are 𝑛prompts, each of which requires two inferences, the total time complexity of the entire stage is 𝑂(2×𝑛×1)= 𝑂(2𝑛)= 𝑂(𝑛). In the second stage of SBR model training, since the second stage only needs to parse the stored inference embedding into tensors, its added time complexity is 𝑂(𝑛). The time complexity of this phase is 𝑂(𝑆𝐵𝑅 +𝑛), and 𝑂(𝑆𝐵𝑅)usually lies between [𝑛,𝑛2], so the time complexity of the LLM4SBR mainly depends on the original SBR model. Therefore, the total time complexity of the LLMSBR framework is 𝑂(𝐿𝐿𝑀4𝑆𝐵𝑅)= 𝑂(𝑆𝐵𝑅 +𝑛)≈ 𝑂(𝑆𝐵𝑅), which depends on the time complexity of SBR and the size of the dataset. In summary, the added space and time costs are small compared to the huge performance gains of the LLM4SBR framework. 5.6 Case Study In this section, we analyze a specific case and examine the level of understanding and inference results of prompts by LLMs with different parameter quantities. As shown in Figure 8, we generated prompts from both short-term and long-term views for the text of the same session sequence and used LLMs ( 7B, 32B, 72B, and 110B ) for inference. The results are shown on the right of the figure, where \"S\" represents the inference result of short-term interest and \"L\" represents the inference result of long-term interest. We did not remove the text explanations generated by LLMs here. We summarize the observed phenomena and analysis results as follows: •All LLMs can respond to different view prompts, even those with the smallest parameter size. In Figure 8, we highlight in red the view-limiting qualifier in the prompt and the fields that respond to the view in the reply. We can see that all four LLMs in the figure understand the view-limiting qualifier in the prompt and make corresponding inferences in the response. In the inference results of the same LLM, the inference results of long-term interests and short-term interests are not the same. These phenomena indirectly prove that the inferencing ability of current general-purpose LLMs has been greatly improved and that they can infer reasonably within the original knowledge even without the fine-tuning of domain-specific knowledge. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:21 Qwen1.5-7B-Chat Qwen1.5-32B-Chat Qwen1.5-110B-Chat Qwen1.5-72B-Chat The order in which users click on items is as follows: 1.Misery (1990) -- Horror_316  2.Addams Family, The (1991) -- Comedy_675  3.Star Wars: Episode VI - Return of the Jedi (1983) --  Action|Adventure|Romance|Sci-Fi|War_55  4.Bodyguard, The (1992) --  Action|Drama|Romance|Thriller_166  Please guess an item that the user is interested in in the  [ short-term | long-term ].  Based on the order of the items, a possible short-term interest item could be \\\"Bodyguard, The  (1992)\\\" since it's the most recent and falls under multiple genres like Action, Drama, Romance, and  Thriller, which might indicate a seeker looking for a thrilling or drama-driven movie to watch. Based on the user's order, it is difficult to determine a single item that represents their long-term  interest with certainty. However, \\\"Star Wars: Episode VI - Return of the Jedi\\\" seems to be a  popular choice, given its diverse genre classification (Action|Adventure|Romance|Sci-Fi|War). Users  might have an interest in the Star Wars franchise or enjoy a mix of those genres. S L Considering the variety of genres the user has engaged with, I would suggest:\\n\"Indiana Jones  and the Raiders of the Lost Ark\\\" (1981) -- Action|Adventure|Fantasy\"\\nThis recommendation  combines elements from several genres the user has shown interest in, such as action, adventure,  and a hint of romance with its thrilling plot, which might appeal to their short-term interests. It's difficult to determine a single item with absolute certainty based on just four clicks. However,  considering the variety of genres the user has engaged with, I would guess the user might have a  broad interest in \\\"Action\\\" genre. Therefore, a potential long-term interest could be:\\n\\nDie Hard  (1988) -- Action|Thriller_24\" S L Based on the recent clicks, it seems the user has an interest in horror, comedy, science fiction, and  action-romance genres. A short-term interest could be a film that combines a few of these elements  or is from a popular franchise. However, since we don't have enough data to precisely predict the  next choice, it's challenging to give an accurate recommendation. If I had to suggest one item that  might pique their interest based on recent trends, it could be:\\n\\n\\\"Ghostbusters\\\" (1984) -  Comedy|Fantasy|Sci-Fi_222\\nThis movie combines comedy and science fiction, which aligns with  the user's recent choices. Keep in mind that this is a speculative suggestion, and actual  preferences may vary. Based on the given user's clicks, it's difficult to definitively determine their long-term interest since  the genres vary from horror to comedy, action, and romance. However, considering the recurrence  of a genre they seem to enjoy, I would guess:\\nStar Wars: Episode VI - Return of the Jedi  (1983)\\nThis choice is based on the fact that \\\"Action|Adventure\\\" is present in the genre list, and it's  a popular film from a well-known franchise, suggesting a potential long-term interest. However, this  is just a speculative guess, and the user's actual long-term interests could be any of the genres  they've explored or something completely different. S L Based on the user's recent clicks on diverse genres such as Horror, Comedy, Action/Adventure, Sci- Fi, Romance, and Drama/Thriller, a short-term interest item could be a film that combines elements  from these preferences, leaning towards popular or well-received titles in crossover genres.  However, without specific data on how to narrow it down to one exact title or a detailed algorithm for  prediction, this is an educated guess. A potential suggestion might be \\\"Guardians of the Galaxy\\\"  (2014), which is a blend of Action, Adventure, Sci-Fi, and has comedic elements, but remember this  is a general guess and not based on the user's exact pattern beyond the given four films. Based on the user's click history, they seem to have a diverse taste in movies, ranging from horror  to comedy, classic sci-fi, and romantic thrillers. Given this variety, it's challenging to pinpoint an  exact genre they may consistently favor. However, if we consider the popularity and the blend of  genres they've enjoyed, a movie that could potentially align with their interests might be \\\"Ghost\\\"  (1990), which combines Drama, Romance, and Supernatural elements. S L The red words highlight the view qualifier  in the Prompt and the understanding of  the perspective qualifier in the LLM  response. S: Short-term L: Long-term The orange words indicate that the  LLM inference results are related to the  words given in the Prompt. The blue words indicate that the LLM  inference result is independent of the  words given in the Prompt, and is the  result of LLM's associative inference  based on its own knowledge. Note: Prompt: Fig. 8. The inference results of LLM with different parameter quantities (7B, 32B, 72B, 110B) for short-term and long-term views. •The number of parameters in an LLM affects the inference ability and scope of a generalized LLM. In the figure, we use orange to indicate that the inference item exists in the user sequence, and blue to indicate that the inference item does not exist in the user sequence. We can observe that LLMs of 7B tend to infer item representatives of users’ long-term/short-term interests from the current sequence item set. In contrast, larger LLMs may generate inference results beyond the scope due to having more built-in knowledge. However, the LLM4SBR framework we proposed has already considered this situation and designed an intent localization module that can effectively constrain this LLM hallucination phenomenon. Furthermore, as shown in Table 3, even the 7B model can still bring significant performance improvements to the SBR model. The LLM4SBR framework allows free substitution of LLMs so that the appropriate LLM can be selected for inference depending on the actual requirements or computational resources, which provides a very high degree of flexibility and compatibility. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:22 • Shutong Qiao et.al. 6 Conclusions and Future work In this paper, we propose a scalable two-stage LLM enhancement framework (LLM4SBR) tailored for SBR. This approach is more efficient in utilizing information compared to encoding text data into embeddings for training, and it allows us to place LLM and SBR in separate stages, greatly reducing training costs. Specifically, in the semantic inference phase, we utilize LLM as the inference engine, designing prompt-guided inference processes from different views and leveraging an intent localization module to alleviate LLM hallucinations and enhance semantic. In the representation enhancement stage, we perform fine-grained alignment and uniformity of text embeddings and session embeddings from different views. This effectively facilitates the fusion of representations from different modalities, thereby enhancing the final session representation. Extensive experiments have demonstrated the effectiveness of the LLM4SBR framework, which significantly enhances most SBR models while also improving model interpretability and enhancing the diversity of candidate selection. For future work, we will continue exploring whether adding additional LLM inference views can yield greater benefits, as well as assessing the effectiveness of utilizing LLM Agent for logical inference. In addition, we also want to explore the application of other downstream tasks combined with LLM. Finally, we hope for this work to open up new avenues in SBR research, accelerating deeper exploration into the integration of LLM with RS. Acknowledgments This work is supported by the National Key Research and Development Program of China under 2022YFB3104702. This work is also supported by the National Natural Science Foundation of China under grants 62272262, 72442026, 72342032, 72074036, and 62072060. References [1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [2] Gediminas Adomavicius and Alexander Tuzhilin. 2005. Toward the next generation of recommender systems: A survey of the state-of- the-art and possible extensions. IEEE transactions on knowledge and data engineering 17, 6 (2005), 734–749. [3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. arXiv preprint arXiv:2309.16609 (2023). [4] Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023. A bi-step grounding paradigm for large language models in recommendation systems. arXiv preprint arXiv:2308.08434 (2023). [5] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. Tallrec: An effective and efficient tuning framework to align large language model with recommendation. arXiv preprint arXiv:2305.00447 (2023). [6] Artun Boz, Wouter Zorgdrager, Zoe Kotti, Jesse Harte, Panos Louridas, Dietmar Jannach, and Marios Fragkoulis. 2024. Improving sequential recommendations with llms. arXiv preprint arXiv:2402.01339 (2024). [7] Qian Chen, Zhiqiang Guo, Jianjun Li, and Guohui Li. 2023. Knowledge-enhanced multi-view graph neural networks for session-based recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval . 352–361. [8] Minjin Choi, Hye-young Kim, Hyunsouk Cho, and Jongwuk Lee. 2024. Multi-intent-aware Session-based Recommendation. InProceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 2532–2536. [9] Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. M6-rec: Generative pretrained language models are open-ended recommender systems. arXiv preprint arXiv:2205.08084 (2022). [10] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT’s Capabilities in Recommender Systems. arXiv preprint arXiv:2305.02182 (2023). [11] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, et al. 2010. The YouTube video recommendation system. In Proceedings of the fourth ACM conference on Recommender systems. 293–296. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:23 [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018). [13] Yifan Feng, Haoxuan You, Zizhao Zhang, Rongrong Ji, and Yue Gao. 2019. Hypergraph neural networks. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 3558–3565. [14] Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023. Chat-rec: Towards interactive and explainable llms-augmented recommender system. arXiv preprint arXiv:2303.14524 (2023). [15] Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5). In Proceedings of the 16th ACM Conference on Recommender Systems . 299–315. [16] Shijie Geng, Juntao Tan, Shuchang Liu, Zuohui Fu, and Yongfeng Zhang. 2023. VIP5: Towards Multimodal Foundation Models for Recommendation. arXiv preprint arXiv:2305.14302 (2023). [17] Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023. Leveraging large language models for sequential recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems . 1096–1102. [18] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web . 173–182. [19] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939 (2015). [20] Jun Hu, Wenwen Xia, Xiaolu Zhang, Chilin Fu, Weichang Wu, Zhaoxin Huan, Ang Li, Zuoli Tang, and Jun Zhou. 2024. Enhancing sequential recommendation via llm-based semantic embedding learning. In Companion Proceedings of the ACM on Web Conference 2024 . 103–111. [21] Wenyue Hua, Shuyuan Xu, Yingqiang Ge, and Yongfeng Zhang. 2023. How to Index Item IDs for Recommendation Foundation Models. arXiv preprint arXiv:2305.06569 (2023). [22] Chao Huang, Jiahui Chen, Lianghao Xia, Yong Xu, Peng Dai, Yanqing Chen, Liefeng Bo, Jiashu Zhao, and Jimmy Xiangji Huang. 2021. Graph-enhanced multi-task learning of multi-level transition dynamics for session-based recommendation. In Proceedings of the AAAI conference on artificial intelligence , Vol. 35. 4123–4130. [23] Erin Illman and Paul Temple. 2019. California Consumer Privacy Act. The Business Lawyer 75, 1 (2019), 1637–1646. [24] Di Jin, Luzhi Wang, Yizhen Zheng, Guojie Song, Fei Jiang, Xiang Li, Wei Lin, and Shirui Pan. 2023. Dual intent enhanced graph neural network for session-based new item recommendation. In Proceedings of the ACM Web Conference 2023 . 684–693. [25] Wang-Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In 2018 IEEE international conference on data mining (ICDM) . IEEE, 197–206. [26] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023. Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. arXiv preprint arXiv:2305.06474 (2023). [27] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems 33 (2020), 9459–9474. [28] Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Tao Lian, and Jun Ma. 2017. Neural attentive session-based recommendation. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management . 1419–1428. [29] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023. GPT4Rec: A generative framework for personalized recommendation and user interests interpretation. arXiv preprint arXiv:2304.03879 (2023). [30] Lei Li, Yongfeng Zhang, and Li Chen. 2023. Prompt distillation for efficient llm-based recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 1348–1357. [31] Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023. CTRL: Connect Tabular and Language Model for CTR Prediction. arXiv preprint arXiv:2306.02841 (2023). [32] Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. 2023. E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation. arXiv preprint arXiv:2312.02443 (2023). [33] Yinfeng Li, Chen Gao, Hengliang Luo, Depeng Jin, and Yong Li. 2022. Enhancing Hypergraph Neural Networks with Intent Disentangle- ment for Session-based Recommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1997–2002. [34] Junyang Lin, Rui Men, An Yang, Chang Zhou, Yichang Zhang, Peng Wang, Jingren Zhou, Jie Tang, and Hongxia Yang. 2021. M6: Multi-modality-to-multi-modality multitask mega-transformer for unified pretraining. InProceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining . 3251–3261. [35] Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-Seng Chua. 2024. Data-efficient Fine-tuning for LLM-based Recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 365–374. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.1:24 • Shutong Qiao et.al. [36] Qiao Liu, Yifu Zeng, Refuoe Mokhosi, and Haibin Zhang. 2018. STAMP: short-term attention/memory priority model for session-based recommendation. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining . 1831–1839. [37] Jie Lu, Dianshuang Wu, Mingsong Mao, Wei Wang, and Guangquan Zhang. 2015. Recommender system application developments: a survey. Decision support systems 74 (2015), 12–32. [38] Danilo Mandic and Jonathon Chambers. 2001. Recurrent neural networks for prediction: learning algorithms, architectures and stability . Wiley. [39] Shutong Qiao, Wei Zhou, Junhao Wen, Hongyu Zhang, and Min Gao. 2023. Bi-channel Multiple Sparse Graph Attention Networks for Session-based Recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management . 2075–2084. [40] Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt-Thieme. 2010. Factorizing personalized markov chains for next-basket recommendation. In Proceedings of the 19th international conference on World wide web . 811–820. [41] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. The graph neural network model. IEEE transactions on neural networks 20, 1 (2008), 61–80. [42] Zhu Sun, Hongyang Liu, Xinghua Qu, Kaidong Feng, Yan Wang, and Yew-Soon Ong. 2023. Large Language Models for Intent-Driven Session Recommendations. arXiv preprint arXiv:2312.07552 (2023). [43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023). [44] Chenyang Wang, Yuanqing Yu, Weizhi Ma, Min Zhang, Chong Chen, Yiqun Liu, and Shaoping Ma. 2022. Towards representation alignment and uniformity in collaborative filtering. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1816–1825. [45] Dongjing Wang, Xin Zhang, Yuyu Yin, Dongjin Yu, Guandong Xu, and Shuiguang Deng. 2023. Multi-view enhanced graph attention network for session-based music recommendation. ACM Transactions on Information Systems 42, 1 (2023), 1–30. [46] Meirui Wang, Pengjie Ren, Lei Mei, Zhumin Chen, Jun Ma, and Maarten de Rijke. 2019. A collaborative session-based recommendation approach with parallel memory modules. In Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval. 345–354. [47] Nan Wang, Shoujin Wang, Yan Wang, Quan Z Sheng, and Mehmet A Orgun. 2022. Exploiting intra-and inter-session dependencies for session-based recommendations. World Wide Web 25, 1 (2022), 425–443. [48] Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, and Minghui Qiu. 2020. Global context enhanced graph neural networks for session-based recommendation. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 169–178. [49] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models with graph augmentation for recommendation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining . 806–815. [50] Shu Wu, Yuyuan Tang, Yanqiao Zhu, Liang Wang, Xing Xie, and Tieniu Tan. 2019. Session-based recommendation with graph neural networks. In Proceedings of the AAAI conference on artificial intelligence , Vol. 33. 346–353. [51] Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Lizhen Cui, and Xiangliang Zhang. 2021. Self-supervised hypergraph convolutional networks for session-based recommendation. In Proceedings of the AAAI conference on artificial intelligence , Vol. 35. 4503–4511. [52] Chengfeng Xu, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Fuzhen Zhuang, Junhua Fang, and Xiaofang Zhou. 2019. Graph contextualized self-attention network for session-based recommendation.. In IJCAI, Vol. 19. 3940–3946. [53] Feng Yu, Yanqiao Zhu, Qiang Liu, Shu Wu, Liang Wang, and Tieniu Tan. 2020. TAGNN: target attentive graph neural networks for session-based recommendation. In Proceedings of the 43rd international ACM SIGIR conference on research and development in information retrieval. 1921–1924. [54] Jiahao Yuan, Zihan Song, Mingyou Sun, Xiaoling Wang, and Wayne Xin Zhao. 2021. Dual sparse attention network for session-based recommendation. In Proceedings of the AAAI conference on artificial intelligence , Vol. 35. 4635–4643. [55] Wei Yuan, Chaoqun Yang, Liang Qu, Nguyen Quoc Viet Hung, Guanhua Ye, and Hongzhi Yin. 2024. PTF-FSR: A Parameter Transmission- Free Federated Sequential Recommender System. ACM Transactions on Information Systems (2024). [56] Wei Yuan, Chaoqun Yang, Guanhua Ye, Tong Chen, Nguyen Quoc Viet Hung, and Hongzhi Yin. 2024. FELLAS: Enhancing Federated Sequential Recommendation with LLM as External Services. ACM Transactions on Information Systems (2024). [57] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to go next for recommender systems? id-vs. modality-based recommender models revisited. arXiv preprint arXiv:2303.13835 (2023). [58] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414 (2022). [59] An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. 2023. On generative agents in recommen- dation. arXiv preprint arXiv:2310.10108 (2023). ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation • 1:25 [60] Yuyue Zhao, Jiancan Wu, Xiang Wang, Wei Tang, Dingxian Wang, and Maarten de Rijke. 2024. Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval . 1796–1806. ACM Trans. Inf. Syst., Vol. 1, No. 1, Article 1. Publication date: January 2025.",
      "references": [
        "Multi-view Intent Learning and Alignment with Large Language Models for Session-based Recommendation",
        "Gpt-4 technical report",
        "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions",
        "Qwen Technical Report",
        "A bi-step grounding paradigm for large language models in recommendation systems",
        "Tallrec: An effective and efficient tuning framework to align large language model with recommendation",
        "Improving sequential recommendations with llms",
        "Knowledge-enhanced multi-view graph neural networks for session-based recommendation",
        "Multi-intent-aware Session-based Recommendation",
        "M6-rec: Generative pretrained language models are open-ended recommender systems",
        "Uncovering ChatGPT’s Capabilities in Recommender Systems",
        "The YouTube video recommendation system",
        "Bert: Pre-training of deep bidirectional transformers for language understanding",
        "Hypergraph neural networks",
        "Chat-rec: Towards interactive and explainable llms-augmented recommender system",
        "Recommendation as language processing (rlp): A unified pretrain, personalized prompt & predict paradigm (p5)",
        "VIP5: Towards Multimodal Foundation Models for Recommendation",
        "Leveraging large language models for sequential recommendation",
        "Neural collaborative filtering",
        "Session-based recommendations with recurrent neural networks",
        "Enhancing sequential recommendation via llm-based semantic embedding learning",
        "How to Index Item IDs for Recommendation Foundation Models",
        "Graph-enhanced multi-task learning of multi-level transition dynamics for session-based recommendation",
        "California Consumer Privacy Act",
        "Dual intent enhanced graph neural network for session-based new item recommendation",
        "Self-attentive sequential recommendation",
        "Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction",
        "Retrieval-augmented generation for knowledge-intensive nlp tasks",
        "Neural attentive session-based recommendation",
        "GPT4Rec: A generative framework for personalized recommendation and user interests interpretation",
        "Prompt distillation for efficient llm-based recommendation",
        "CTRL: Connect Tabular and Language Model for CTR Prediction",
        "E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation",
        "Enhancing Hypergraph Neural Networks with Intent Disentanglement for Session-based Recommendation",
        "M6: Multi-modality-to-multi-modality multitask mega-transformer for unified pretraining",
        "Data-efficient Fine-tuning for LLM-based Recommendation",
        "STAMP: short-term attention/memory priority model for session-based recommendation",
        "Recommender system application developments: a survey",
        "Recurrent neural networks for prediction: learning algorithms, architectures and stability",
        "Bi-channel Multiple Sparse Graph Attention Networks for Session-based Recommendation",
        "Factorizing personalized markov chains for next-basket recommendation",
        "The graph neural network model",
        "Large Language Models for Intent-Driven Session Recommendations",
        "Llama: Open and efficient foundation language models",
        "Towards representation alignment and uniformity in collaborative filtering",
        "Multi-view enhanced graph attention network for session-based music recommendation",
        "A collaborative session-based recommendation approach with parallel memory modules",
        "Exploiting intra-and inter-session dependencies for session-based recommendations",
        "Global context enhanced graph neural networks for session-based recommendation",
        "Llmrec: Large language models with graph augmentation for recommendation",
        "Session-based recommendation with graph neural networks",
        "Self-supervised hypergraph convolutional networks for session-based recommendation",
        "Graph contextualized self-attention network for session-based recommendation.",
        "TAGNN: target attentive graph neural networks for session-based recommendation",
        "Dual sparse attention network for session-based recommendation",
        "PTF-FSR: A Parameter Transmission-Free Federated Sequential Recommender System",
        "FELLAS: Enhancing Federated Sequential Recommendation with LLM as External Services",
        "Where to go next for recommender systems? id-vs. modality-based recommender models revisited",
        "Glm-130b: An open bilingual pre-trained model",
        "On generative agents in recommendation",
        "Let Me Do It For You: Towards LLM Empowered Recommendation via Tool Learning"
      ],
      "meta_data": {
        "arxiv_id": "2402.13840v2",
        "doi": "10.1145/3719344",
        "authors": [
          "Shutong Qiao",
          "Wei Zhou",
          "Junhao Wen",
          "Chen Gao",
          "Qun Luo",
          "Peixuan Chen",
          "Yong Li"
        ],
        "published_date": "2024-02-21T14:38:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses challenges in Session-based Recommendation (SBR), such as data sparsity and high LLM training costs, by proposing LLM4SBR, a lightweight, efficient LLM-enhanced SBR framework. It integrates semantic and behavioral signals from multiple views through a two-stage process: intent inference using LLMs with multi-view prompts and an intent localization module to mitigate hallucinations, followed by representation enhancement where traditional SBR models are aligned and fused with LLM-inferred semantic representations. The framework significantly enhances the performance of various SBR models, acting as a plug-in solution, and improves model interpretability and candidate diversity.",
        "methodology": "LLM4SBR is a two-stage framework. The first stage, **Intent Inference**, employs LLMs guided by multi-view prompts (long-term and short-term) to infer latent user intentions. An **Intent Localization Module** then alleviates LLM hallucinations and enhances semantics by encoding inference results and item names into BERT embeddings, computing cosine similarity, and selecting/weighting the Top-r most similar actual items. The second stage, **Representation Enhancement**, uses a traditional **SBR Model** (e.g., SR-GNN) to model interaction data and generate local and global session representations. These are then **aligned and unified** with the LLM-inferred semantic representations from corresponding views using linear transformations and alignment/uniformity loss functions. Finally, a soft-attention mechanism **fuses** all representations into a final session representation for next-item prediction, optimized via a joint loss function combining recommendation loss and auxiliary alignment/uniformity losses.",
        "experimental_setup": "Experiments were conducted on two real-world datasets: **Beauty** (Amazon product reviews, segmented into sessions) and **MovieLens-1M (Ml-1M)** (movie ratings, segmented into sessions by 10-minute intervals). Datasets were preprocessed by removing sessions of length 1 and items appearing fewer than 5 times. Evaluation metrics included **Precision (P@K), Mean Reciprocal Rank (MRR@K), and Normalized Discounted Cumulative Gain (NDCG@K)**, with K values of 5, 10, and 20. The inference LLM used was **Qwen-7B-Chat**. The optimizer was Adam, with a learning rate of 0.001, L2 penalty of 10^-5, batch size of 100, and dimension size of 100 for the SBR backbone. The auxiliary loss weight (τ) was 0.1, and the `r` hyperparameter for intent localization was initially 5, with further analysis for values 0, 1, 3, and 5. Six classic SBR models (GRU4Rec, SASRec, SR-GNN, TAGNN, GCE-GNN, S2-DHCN) were used as backbones, and comparisons were made against LLM-combined SBR models (LLM-Infer, LLMSeqSim, LLM2SRGNN).",
        "limitations": "The intent localization module, while designed to mitigate hallucinations, might introduce noise if the original SBR model is already highly effective, potentially leading to slight performance drops on certain metrics for some SBR models (e.g., P@20, MRR@20 for S2-DHCN and GCE-GNN). Larger LLMs, due to their extensive built-in knowledge, may generate inference results that are outside the candidate item set (hallucinations), although the intent localization module is specifically designed to constrain this behavior. The framework's overall performance is still inherently dependent on the capabilities of the underlying traditional SBR model it enhances.",
        "future_research_directions": "Future work includes exploring the benefits of adding more LLM inference views, assessing the effectiveness of using LLM Agents for logical inference in SBR, and investigating the application of this framework or similar LLM-integrated approaches to other downstream tasks in recommender systems. The authors also hope this work will foster deeper exploration into LLM integration within SBR research.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Chain-of-Thought Reasoning Without Prompting",
      "full_text": "A Weak Supervision Approach for Few-Shot Aspect Based Sentiment Analysis Robert Vacareanu2,3∗ Siddharth Varia1 Kishaloy Halder1 Shuai Wang1 Giovanni Paolini1 Neha Anna John1 Miguel Ballesteros1 Smaranda Muresan1 1AWS AI Labs 2University of Arizona, Tucson, AZ, USA 3Technical University of Cluj-Napoca, Romania {siddhvar, kishaloh, wshui, paoling, nehajohn, ballemig, smaranm}@amazon.com rvacareanu@arizona.edu Abstract We explore how weak supervision on abun- dant unlabeled data can be leveraged to im- prove few-shot performance in aspect-based sentiment analysis (ABSA) tasks. We pro- pose a pipeline approach to construct a noisy ABSA dataset, and we use it to adapt a pre-trained sequence-to-sequence model to the ABSA tasks. We test the resulting model on three widely used ABSA datasets, before and after ﬁne-tuning. Our proposed method pre- serves the full ﬁne-tuning performance while showing signiﬁcant improvements ( 15.84% absolute F1) in the few-shot learning scenario for the harder tasks. In zero-shot (i.e., with- out ﬁne-tuning), our method outperforms the previous state of the art on the aspect ex- traction sentiment classiﬁcation (AESC) task and is, additionally, capable of performing the harder aspect sentiment triplet extraction (ASTE) task. 1 Introduction Aspect Based Sentiment Analysis (ABSA) is a ﬁne- grained variant of sentiment analysis (Hu and Liu, 2004; Pontiki et al., 2014, 2015, 2016; Zhang et al., 2021a; Shu et al., 2022; Zhang et al., 2022), where the task is to predict the sentiment expressed to- wards an entity or a certain aspect of an entity, instead of just the sentence-level sentiment ( e.g., traditional sentiment analysis tasks (Socher et al., 2013; dos Santos and de C. Gatti, 2014)). For illustration, for a reviewThe pizza was great, but the service was terrible, a sentence-level senti- ment analysis model might identify the sentiment as neutral. The need for ABSA stems from such complex interactions between the target and the polarity of the sentiment (Pontiki et al., 2014). An ABSA model has to identify the sentiment towards pizza as positive, and service as negative, for a holistic understanding of the text. Furthermore, ∗Work done during internship at AWS AI Labs ABSA tasks can include the identiﬁcation of the opinion terms (i.e. great, terrible), and the as- pect categories (i.e. FOOD, SERVICE) (Zhang et al., 2021a). Although traditionally considered as a struc- tured prediction task in the ABSA literature, recent works have shown how sequence-to-sequence (seq- to-seq) models can be effective in these tasks with a generative approach (Yan et al., 2021; Zhang et al., 2021a). Such approaches leverage the knowledge gained from one task to seamlessly perform well in another. As such, we build upon the Instruc- tion Tuning with Multi-Task Learning approach (Varia et al., 2022) and address the following ﬁve ABSA tasks: (i) Aspect-term Extraction (AE), (ii) Aspect-term Extraction and Sentiment Classiﬁca- tion (AESC), (iii) Target Aspect Sentiment Detec- tion (TASD), (iv) Aspect Sentiment Triplet Extrac- tion (ASTE), and (v) Aspect Sentiment Quadruple Prediction (ASQP). Sentence-level sentiment annotations are com- paratively cheaper and are available at scale through automated proxies ( e.g., /uni2600or /uni2600/uni2600be- come negative, /uni2600/uni2600/uni2600/uni2600or /uni2600/uni2600/uni2600/uni2600/uni2600be- come positive, in the Amazon/Yelp review corpus (Zhang et al., 2015b)). On the contrary, ABSA requires understanding at sub-sentence level with multiple words or phrases being related to each other, making it prohibitively costly to annotate at scale.1 However, the abundance of generic review data presents a promising opportunity to improve the performance of a pre-trained language model (PLM) beyond simply ﬁne-tuning it on the small annotated ABSA corpora. Towards this end, we ﬁrst construct a noisily annotated ABSA corpus out of generic customer review data without any direct supervision. We utilize this noisy corpus to pre-train a seq-to-seq model on multiple ABSA tasks. We show that such 1This is evident from the corpus size of 2.1k vs 700k for REST 16 and Y ELP -FULL , respectively. arXiv:2305.11979v1  [cs.CL]  19 May 2023models are capable of learning in zero/few-shot in ﬁnal downstream ABSA tasks. Our contributions are the following: (i) We propose a weakly super- vised method to obtain annotations for three out of the ﬁve ABSA tasks explored in the literature; (ii) We introduce a pre-training step to improve the few-shot performance on the downstream task of PLMs; (iii) We comprehensively evaluate our pro- posed method in three scenarios (full ﬁne-tuning, few-shot, and zero-shot learning), yielding as much as 15.84% F1 improvement over the SOTA base- lines. We release the sources to create the few-shot benchmarking datasets2. 2 Related Work Aspect-Based Sentiment Analysis has received tremendous attention in the past years (Tulkens and van Cranenburgh, 2020; Zhang et al., 2021a; Shu et al., 2022; Zhang et al., 2022), either handling sin- gle tasks, such as aspect term extraction (He et al., 2017; Liu et al., 2015; Tulkens and van Cranen- burgh, 2020), aspect category detection (Tulkens and van Cranenburgh, 2020), aspect sentiment clas- siﬁcation (V o and Zhang, 2015; Xu et al., 2019; Li et al., 2021; Wang et al., 2021), or handling compound tasks (Zhang et al., 2015a; Yu et al., 2021; Xu et al., 2020; Zhang et al., 2021a). For the latter group, it typically includes either a pipeline approach (Peng et al., 2020; Yan et al., 2021) or an end-to-end (E2E) approach (Xu et al., 2020; Zhang et al., 2021a,b). In the pipeline approach the ﬁnal prediction is constructed using the output of multiple compo- nents. The disadvantage of such models is that the error is propagated throughout the system (Zhang et al., 2022). In the E2E approach, the model learns the in- teractions jointly between the multiple prediction tasks, which is believed to improve the ﬁnal perfor- mance (Xu et al., 2020; Zhang et al., 2022). Our proposed approach falls in this category. Typical E2E approaches include: (i) treating it as a token classiﬁcation task (Xu et al., 2019; Shu et al., 2019; Xu et al., 2020), (ii) framing it as a machine reading comprehension task (Chen et al., 2021; Liu et al., 2022), natural language inference task (Shu et al., 2022), or as a language generation task (Zhang et al., 2021b; Yan et al., 2021; Zhang et al., 2021a; Varia et al., 2022). 2https://github.com/robertvacareanu/ NoisyABSAPreTraining Our proposed approach treats the ABSA tasks as a generation task, similar to (Zhang et al., 2021a; Varia et al., 2022). We build upon the paradigm called Instruction Tuning with in Multi-Task Learn- ing (IT-MTL), introduced in (Varia et al., 2022), resulting in a single model capable of handling dif- ferent ABSA tasks. However, none of these meth- ods takes advantage of the vast amount of review data available, other than just pre-training on them with some generic language modeling objectives. 3 Method We introduce an additional step in the classical pretrain → finetune approach (Howard and Ruder, 2018; Devlin et al., 2019; Raffel et al., 2020), transforming it into pretrain → Noisy ABSA Pre-Training (NAPT) → finetune for ABSA. We propose an approach for building a weakly annotated dataset for the intermediate NAPT step. We use this noisy dataset to enhance the knowledge of a pretrained model with the in- tuition that exposing the model to tasks which are well aligned with the ﬁnal downstream task, im- proves the performance. We then consider this as the backbone base model, and ﬁnetune it on the downstream task as usual. Our proposed approach is applicable to any generic seq-to-seq model. 3.1 Dataset Construction The ﬁrst step in our proposed method is to weakly annotated a dataset without any direct supervision.3 Our proposed approach annotates a dataset with tu- ples of the form aspect-terms, opinion-terms, and sentiment polarity. We follow a pipeline approach as shown in Table 1(Xu et al., 2013; Zhang et al., 2022), but without using any direct ABSA supervi- sion. We describe each step in greater detail next. 3.1.1 Aspect-term Extraction The ﬁrst step in our proposed dataset creation pro- cedure is aspect-term extraction. We use spacy to- kenizer to obtain POS tags and then consider 20% of the most frequent nouns in the text. These nouns serve as candidate aspect terms. We note that this method implicitly assumes that dataset Dconsists of a single domain. Nevertheless, this is a small assumption as the reviews are typically directed towards a product of a known category (He and McAuley, 2016; Zhang et al., 2015b). We extend 3We use models which were trained on different tasks, but no model has seen any aspect-based sentiment analysis data.Sentence:The pizza was great, but the service was terrible. Step Heuristic / Method Resulting Annotations #1 Extract frequent nouns as Aspect-terms pizza, service #2 Extract matches with an opinion lexicon as Opinion-termsgreat, terrible #3 Predict entailment of form{aspect} is {opinion}for every aspect, opinion combinations using a pre-trained NLI model<pizza, great>, <service, terrible> #4 Classify documents of form{aspect} is {opinion}with a pre-trained sentiment analysis model <pizza, great, positive>, <service, terrible, negative> Table 1: A step-by-step illustration of our noisy dataset construction pipeline. It follows a pipeline approach, and yields <aspect, opinion, sentiment> triplets in the end for each sentence in a generic review corpus. this method to multi-word aspect terms by consid- ering collocations of length ≤ 4 ﬁltered by their POS tags. For example, we allow bigrams of the form NN-NN like chicken breast(cf Table 16 for all patterns used). Finally, we ﬁlter out the sentences from which no aspect term was extracted. 3.1.2 Opinion-term Extraction The second step in our proposed algorithm is opin- ion term extraction. We take a lexicon-based ap- proach to opinion extraction (Ding et al., 2008; Kanayama and Nasukawa, 2006; Hu and Liu, 2004). In particular, we use the opinion lexicon from (Hu and Liu, 2004) and perform word match- ing on the target text. If negations e.g., noor not appear before the opinion word, we include it in the ﬁnal extraction as well. We ﬁlter out the sentences from which no opinion term was extracted. 3.1.3 Linking Opinion-terms with Aspect-terms So far the resulting dataset consists of noisy aspect, and opinion terms, but without the association be- tween them. For example, for a sentence such as The pizza was great , but the service was terri- ble., the proposed algorithm would extract pizza and service as the aspect terms and great and terri- ble as the opinion terms, respectively. But at this point we do not know that great refers to pizza and terrible refers to service. We reformulate this problem as a natural language inference problem (Dagan et al., 2005; Shu et al., 2022). We use an MPNet4 model (Song et al., 2020) and construct ar- tiﬁcial sentences to determine which opinion-term refers to which aspect-term. More precisely, we construct sentences such as <aspect-term> is <opinion-term>, for each aspect- and opinion- term.5 Then, we use the original sentence (e.g. 4huggingface.co/symanto/mpnet-base-snli-mnli 5We relax strict grammatical correctness e.g., the formu- lation might result in burgers is greatinstead of burgers are great). The pizza was great , but the service was terrible.) as the premise and our artiﬁcially constructed sen- tence as the hypothesis (e.g. pizza is great). We interpret a high entailment score ( ≥ 0.75) as evi- dence that the opinion term refers to that particular aspect term. We discard aspect- and opinion-term pairs where the entailment score was below the threshold. Alternative Approach: We consider an alter- nate approach where the linking is based on constituency-parse rules which turns out disad- vantageous. Constituency parsing is considerably slower and the rules are non-trivial to formulate. 3.1.4 Sentiment Extraction The last step in our proposed dataset creation method is to add the sentiment (Hu and Liu, 2004) to each <aspect-term, opinion-term> tuple. We use a sentence-level classiﬁer on top of arti- ﬁcially constructed sentences (Sanh et al., 2019). For example, for a tuple such as <pizza, great>, we feed the sentence pizza is greatthrough a sentence- level sentiment classiﬁer.6 Then, we label the <as- pect term, opinion term> tuple with the sentiment prediction if the model’s conﬁdence is above a certain threshold ( ≥ 0.75), otherwise we discard the tuple. At the end of this step, for the sen- tence The pizza was great , but the service was terrible. we have the following <aspect-term, opinion-term, sentiment> noisy annotations: <pizza, great, positive>, <service, terrible, nega- tive>. We consider an alternative for this step using the sentiments associated in the opinion lexicon, but a classiﬁer allows for conﬁdence ﬁltering. Throughout our proposed dataset creation pro- cess we use external resources, such an opinion lexicon, an NLI model and a sentence-level senti- ment classiﬁer. However, these resources do not consume any annotated ABSA data by any means. 6huggingface.co/distilbert-base-uncased-ﬁnetuned-sst-2- english3.2 Noisy ABSA Pre-training (NAPT) The phase consists of exposing the model to tasks that are more aligned with the ﬁnal downstream task, i.e., ABSA in our case. We factorize the triplets from the noisy dataset into ﬁve separate but overlapping tasks: (i) aspect-term extraction, (ii) opinion-term extraction, (iii) aspect-term and opinion-term extraction, (iv) aspect-term extrac- tion and sentiment prediction, and (v) aspect-term extraction, opinion-term extraction and sentiment prediction. Note that there exists a correspondence between our NAPT tasks and classical ABSA tasks: tasks (i), (iv) and (v) correspond to Aspect Extrac- tion (AE), Aspect Extraction Sentiment Classiﬁca- tion (AESC), and Aspect Sentiment Triplet Extrac- tion (ASTE), respectively. We use the noisy ABSA dataset to pre-train the base model. We train the model parameters in a multi-task learning frame- work (cf Figure 1) using instruction tuning with a diverse set of instructions (Sanh et al., 2022). At the end of NAPT, the resulting model is imbued with the capability of performing multiple ABSA tasks. This can serve as a drop-in replacement to the off-the-shelf pre-trained checkpoints that are widely used in the generative ABSA literature. 3.2.1 Addressing Overﬁtting The primary goal of our proposed NAPT phase is to enhance the pre-trained model while retaining existing knowledge from pre-training objectives, in other words, avoiding catastrophic forgetting and overﬁtting. We achieve this in a few different ways. First, instead of just randomly splitting the data into train/validation, we split the extracted aspect- and opinion-terms into two disjoint sets, favoring novel aspect- and opinion term constructions in the validation partition. We observe this split deﬁni- tion to be necessary to prevent overﬁtting of the base model. Additionally, we invoke three types of regularization: • Standard weight decay: we add a standard ℓ2 regularization term to the loss function. • Tuple Dropout: we apply dropout over the tu- ples that the model is trained to extract to prevent it from overﬁtting to the noisy annotations. We randomly dropped 50% of the tuples from pre- diction targets of the seq-to-seq model. • Biased weight decay: we use a biased variant of weight decay to prevent the parameters from diverging considerably from the initialization point, akin to (Kirkpatrick et al., 2017). Towards this, we use the ℓ2 norm over the difference be- tween the current (θ) and the initial weights of the model (θinit), and add it to the loss. Our ﬁnal loss function (L) is: L =CEloss +α⋅ℓ2(θ−θinit) +β⋅ℓ2(θ). (1) where αand βare hyperparameters, and CEloss denotes the standard cross-entropy loss. 4 Experiments We compare against state-of-the-art methods on three widely used ABSA datasets. We evaluate in three scenarios: (i) k-shot learning: where the model has access to at least k examples of each class, (ii) zero-shot evaluation: where the model has not seen any example at all from the gold- annotated ABSA data, and (iii) full-training: where the model has access to the complete gold-standard training data, 4.1 Experimental Setup In all our experiments, we use T5 (Raffel et al., 2020), particularly t5-base as the pre-trained seq- to-sed model, which has ∼220M parameters. We experiment with t5-large as well to explore the impact of model size on the downstream perfor- mance (cf Appendix B). We use the standard eval- uation metrics as previous work, which is F1 score over the exact match of the tuples. For zero-shot, we use the same evaluation procedure as (Shu et al., 2022), which is token-level F1 score. We use a random subset of Amazon Electron- ics (He and McAuley, 2016), and Yelp reviews (Zhang et al., 2015b) to create our noisy-annotated dataset.7 We split the reviews with≥3 sentences us- ing a sentence tokenizer. We split the noisy dataset into train/validation split. We enforce that there is no overlap in terms of aspect-terms between the train/validation splits. This results in approx- imately 190k examples for training and 12.5k ex- amples for validation. We repeat each experiment with 5 different ran- dom seeds. Additionally, we repeat the noisy ABSA pre-training step with 3 different random seeds. As a result, the numbers corresponding to our proposed method (i.e. the ones with -APT) rep- resent an average of 5 ×3 = 15 runs, and all the 7100K reviews from Amazon, and YELP each are used.Figure 1: Overview of our proposed Noisy ABSA Pre-Training (NAPT). We start from a pretrained language model and extend its capabilities by instruction tuning it in a multi-task learning fashion. We use 5 different yet related tasks for the proposed NAPT step. The tasks we use are: (i) aspect-term extraction, (ii) opinion-term extraction, (iii) aspect-term extraction and opinion-term extraction, (iv) aspect term extraction and sentiment classiﬁcation, and (v) aspect-term extraction, opinion-term extraction, and sentiment classiﬁcation. This step results in a model capable of performing multiple ABSA tasks. other numbers represent an average of 5 runs. We report the mean and (sample) standard deviation. We present the results on the Aspect Sentiment Triplet Extraction (ASTE) and Aspect-term Extrac- tion and Sentiment Classiﬁcation (AESC) tasks available in all the datasets we use for evaluation.8 4.2 Datasets We use three popular datasets for aspect-based sen- timent analysis: REST 15, REST 16 and LAP14 (Pontiki et al., 2014, 2015, 2016), which cover two domains: restaurant and laptop, respectively. In particular, we use the version released by Zhang et al.. For k-shot, we use the same splits as (Varia et al., 2022) to ensure a fair comparison. Speciﬁ- cally, the k-shot datasets were created by sampling k examples for each attribute. The attributes are aspect category, and sentiment for restaurant, and laptop respectively. 4.3 Baselines Since we introduce the NAPT step and build upon the existing Instruction Tuning with Multi-Task Learning (IT-MTL) paradigm, we refer to our pro- posed method as IT-MTL-NAPT. We compare this with standard ﬁne-tuning based approaches that generally show strong performance in ABSA tasks i.e., (i) text-only (Text), where we give the model the text review and train it to predict the gold text (Zhang et al., 2021a), (ii) instruction tuning ( IT) 8Results for all tasks are in Tables 10,11,12, and 7,8,9 for k-shot and full training respectively. and (iii) instruction tuning + multi-task learning, as per (Varia et al., 2022) (IT-MTL). To succinctly show the effectiveness of proposed NAPT, we keep another baseline where a seq-to- seq model is further pre-trained with in-domain data using the same objective as that oft5 i.e., span prediction. We call it IT-MTL-ID.9 The in-domain data is essentially the same as that of the NAPT corpus, but without the noisy annotations. 4.4 K-Shot Learning Next, we compare between the two approaches in k-shot learning scenarios. We summarize our re- sults in Figure 2. IT, and IT-MTL-ID perform simi- larly with the other baselines, so we skip them for clarity. We include all our results in Appendix B.2. First we observe that, our proposed method outper- forms the baselines across all datasets in all k-shot scenarios, yielding as much as 15.84% F1 points (i.e. from 13.04%F1 to 28.88%F1) of improve- ment. Second, the performance improvement in- creases as the number of examples decrease, with the biggest improvement being in the k=5 case. This is expected because with the growing number of examples, all models are able to learn the task better. When using the full dataset, as we see in Table 3, both the proposed model and the baseline performances converge. Additionally, we observe that our proposed method brings the larger improve- ments on the harder tasks, as it gets difﬁcult for the 9As in, In-Domain (ID) pre-training occurs along with IT-MTL.(a) LAP14 on ASTE Task  (b) REST 15 on ASTE Task  (c) REST 16 on ASTE Task (d) LAP14 on AESC Task  (e) REST 15 on AESC Task  (f) REST 16 on AESC Task Figure 2: Performance Comparison between our proposed method ( IT-MTL-NAPT) and two baselines over 3 datasets on on the Aspect Sentiment Triplet Extraction (ASTE), Aspect-term Extraction and Sentiment Classiﬁ- cation (AESC) tasks in top, and bottom rows respectively. We note that our proposed method helps in all the k splits. (larger is better) baselines to learn from only a few of examples. 4.5 Zero-Shot Evaluation Our proposed NAPT step enables the model to per- form the following ABSA tasks in zero-shot i.e., without any gold-standard supervision: (i) Aspect- term Extraction (AE), (ii) Aspect-term Extraction and Sentiment Classiﬁcation (AESC), and (iii) As- pect Sentiment Triplet Extraction (ASTE). We per- form two experiments in the zero-shot setting. First, we investigate how much data does a baseline need to reach the performance obtained by our proposed model in the zero-shot setting. Second, we com- pare against previous work in the ASTE task (Shu et al., 2022). 4.5.1 Dataset Size Equivalence We compare our proposed method in zero-shot setting against a baseline model trained on gold- annotated data, where we vary the number of train- ing data points. This experiment shows how many annotated data points, on average, is the noisy ABSA pre-training phase equivalent of. We ob- served that the improvement depends on the dif- ﬁculty of the task and of the dataset, respectively. For example, Figure 3 shows that for the ASTE task, one would need∼15,25 annotated data points to obtain a comparable performance with our pro- (a) LAP14  (b) REST 15 Figure 3: Data size equivalence comparison between t5 models that are finetuned on downstream corpus vs our proposed NAPT for ASTE task in (a) L AP14, (b) R EST 15 respectively. The ﬁnetuned models need ∼15 −25 completely annotated data points to equalize our proposed method. posed method for REST 15 and LAP14 respectively. We remark that the number of data points vary ac- cording to the difﬁculty of the task and with the difﬁculty of the dataset, ranging between ∼6 −25 data points for AE, and ASTE task for LAP14 re- spectively. 4.5.2 Performance Comparison with Baselines We compare the zero-shot performance of our pro- posed method with previous work on ABSA (Shu et al., 2022), summarized in Table 2. Our proposed model outperforms the previous state-of-the-art re- sults for AESC by as much as 6.94%F1 points inthe restaurant domain. The improvement for the laptop domain is smaller, we attribute this to the NAPT dataset being biased towards the restaurant domain in terms of size. It is interesting to note that our model’s backbonei.e., t5-base is able to outperform CORN altough it has almost half the number of parameters as that of its counterpart i.e., bart-large. Model R EST LAP CORN 37.20±0.50 40.30±0.60 IT-MTL-NAPT44.14±0.30 40.51±0.43 Table 2: Comparison of our proposed method with pre- vious work on zero-shot Aspect Extraction Sentiment Classiﬁcation (AESC). Our proposed method outper- forms the previous work on both datasets. Metric is token-level F1 score. 4.6 Full-Training We compare the performance of our proposed method (i.e. pretrain → NAPT → finetune) with the standard method ofpretrain → finetune and report the result in Table 3, for all the datasets. Overall in the full-training scenario, our proposed method performs comparably with or better than the baseline. We observe during our preliminary experiments that the training dynamics change dras- tically between thepretrain → NAPT → finetune and pretrain → finetune. 5 Discussion In this section, we would like to discuss a few important aspects of our approach apart from the main experiments. 5.1 Ablation To better understand how different components of our NAPT strategy inﬂuence the ﬁnal downstream performance, we conduct the following ablation studies. Model Dataset LAP14 R EST15 R EST16 Text 59.50±1.35 51.74±0.84 62.95±0.61 IT 60.47±1.36 52.78±0.81 63.77±0.82 IT-MTL 60.17±1.19 53.17±0.67 62.69±0.69 IT-MTL-ID 58.24±1.03 53.42±1.27 62.38±0.69 IT-MTL-NAPT59.97±1.28 53.57±1.42 61.67±0.65 Table 3: F1 scores of our proposed method (IT-MTL-NAPT) and 4 competitive baselines on the As- pect Sentiment Triplet Extraction task over 3 datasets under training on full dataset. We observe similar lev- els of performance. Regarding NAPT Tasks: We analyze the impor- tance of NAPT with multiple tasks and their im- pact on the downstream performance. Our anal- ysis shows that there exists a positive correlation between the NAPT complexity and downstream performance. We average the downstream perfor- mance across every task and every k-shot split and train on the downstream task in a multi-task learn- ing fashion. We summarize our results in Table 4. Our experiments show that it helps in general to align the NAPT and ﬁnetuning objectives. If the NAPT phase is done in a multi-task learning fash- ion, it is beneﬁcial for the model if the same is done for ﬁnetuning on the downstream task as well. Additionally, we observe that that harder NAPT tasks are beneﬁcial for the downstream task regard- less of the way the training on the downstream task is performed, as the F1 scores reﬂect the relative order in difﬁculty of the tasks (i.e., ASTE > AESC > AE). NAPT Task Dataset L AP 14 R EST 15 R EST 16 AE 43 .47 46 .72 50 .76 AESC 44 .94 46 .99 50 .75 ASTE 46 .30 47 .14 51 .17 MTL 47 .45 47 .32 51 .65 Table 4: Ablation study over NAPT tasks in terms of macro F1 scores averaged across all the tasks and 4 k- shot settings. It shows that having all the tasks during NAPT achieves the best scores. Regarding NAPT Regularization: We analyze the importance on the downstream performance of each regularization technique used during the NAPT phase. We report the performance in Table 6. We analyze the inﬂuence of: (i) Tuple Dropout, (ii) Biased weight decay, and (iii) Weight decay. We observe that our proposed approach is robust to hyperparameters, obtaining similar performance with various combinations of the 3 regularization techniques. We attribute this to the way the NAPT dataset is split into train and validation: enforcing disjoint sets of aspect-terms. This allows us to detect when the model starts to overﬁt.10 5.2 Sentiment Prediction: Error Analysis Quantitative: We ﬁrst compare the percentage of correct predictions over each sentiment class, 10Preliminary experiments shows that regularization was needed, but the training and testing splits contained overlap- ping aspect terms and opinion terms.Task : Input Gold w/o NAPT w/ NAPTASTE:Given the text: Finally, the biggest problem has been techsupport., what are the aspect terms and their sentiments?<tech support,negative> <support,negative> <tech support,negative> ASTE:What are the aspect terms and their sentiments in the text:Of course, for a student, weight is always an issue.?<weight,neutral> <weight,neutral> <weight,negative> AESC:Given the text: the mouse buttons are hard to push.,what are the aspect term, opinion term, and sentiment triplets?<mouse buttons, hard,negative> <, ,> <mouse buttons, hard,negative> AESC:Given the text: The resolution is even higher then any otherlaptop on the market., what are the aspect term,opinion term and sentiment triplets?<resolution, higher,positive> <resolution, higher,positive> <laptop, higher,positive> Table 5: Predictions made by an instruction tuned model with and without NAPT in low-shot scenarios. Ablation Conﬁg. Dataset Tuple Dropout Weight Decay Biased Weight LAP14 R EST15 R EST16 ✓ ✓ ✓ 47.45 47 .32 51 .65 ✓ ✓ × 47.57 47 .10 51 .39 ✓ × ✓ 47.62 47 .26 51 .65 ✓ × × 47.39 47 .17 51 .37 × ✓ ✓ 47.55 47 .65 51 .80 × ✓ × 46.43 47 .44 51 .49 × × ✓ 46.78 47 .12 51 .11 × × × 46.90 47 .27 51 .49 Table 6: Ablation study over different regularization techniques in terms of macro F1 scores averaged across all tasks and 4 k-shot settings. Figure 4: Comparison on the percentage of correct pre- dictions over each sentiment class for an instructiontuned model with vs without the proposed NAPT on the LAP14 dataset and k=10. With NAPT, it performs bet- ter on each sentiment class, even though neutral class does not appear in the noisy dataset (larger is better). namely positive, negative, and neutral. We com- pare instruction tuning with and without our pro- posed NAPT step. We highlight the results in Fig- ure 4. We observe that our proposed method per- forms better for every sentiment class. Moreover, we note that our proposed method outperforms the baseline even for the neutral sentiment class, a class which has not been seen during the NAPT phase. This suggests that NAPT can help the model learn faster even unseen tasks. Qualitative: We present examples of the predic- tions made by an instruction tuned model with and without our proposed NAPT in Table 5. We show4 predictions, 2 for ASTE (ﬁrst two rows) and 2 for AESC (bottom two) on LAP14, in low-shot scenar- ios. We observe that the baseline has difﬁculties extracting the full aspect term (ﬁrst row), while our proposed method is able extract the complete triple. The metric used does not reward partial matching. In the second row, the baseline correctly generates the gold output, while our proposed method pre- dicts a negative sentiment. In this case, the input can be considered ambiguous, as there is no explicit sentiment expressed in it. Also, for more complex tasks, such as aspect sentiment triplet extraction (AESC), the baseline has difﬁculties generating a valid prediction, while our proposed method is able to generate the correct prediction (third row). Lastly, we observe that although with NAPT we predict incorrectly (last row), it rather falls back to a term relevant to the domain (i.e., laptop). 6 Conclusion In this paper, we proposed to add an intermediate step in the pretrain→ finetune paradigm, called Noisy ABSA Pre-Training. We motivate this newly introduced step with the hypothesis that exposing the model to tasks more aligned with the down- stream task will improve its performance, espe- cially in low-data regimes such as in few-shot or complete zero-shot. We constructed a noisy dataset with a heuristic based pipeline approach consist- ing of three steps that utilize well-studied NLP re- sources and models. It serves as the training dataset for the noisy pre-training phase. We then evaluated with customer reviews from three datasets covering two domains, laptop and restaurant, and obtained large improvements in the zero/few-shot cases while achieving similar performance under ﬁne- tuning on full dataset. We also discussed caveats around introducing catastrophic forgetting of gen- eral purpose pre-trained language models through such noisy pre-training, and introduced a few regu- larization techniques to help alleviate it.Limitations We believe our proposed noisy pre-training step should apply to other structured prediction tasks, however we have not evaluated the approach on anything other than ABSA related tasks. Addi- tionally, the noisy corpus construction process is heavily dependent on English based resources and pre-trained models. It might be non-trivial to ex- tend the approach to other languages. Finally, we presented some extrinsic evaluation regarding the quality of the noisy corpus we create e.g., equiva- lence in terms of gold-annotated data size (Section 4.5.1). We leave any intrinsic evaluation of it by means of human supervision or otherwise for future work. References Shaowei Chen, Yu Wang, Jie Liu, and Yuelin Wang. 2021. Bidirectional machine reading comprehen- sion for aspect sentiment triplet extraction. ArXiv, abs/2103.07665. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In MLCW. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Pa- pers), pages 4171–4186. Association for Computa- tional Linguistics. Xiaowen Ding, B. Liu, and Philip S. Yu. 2008. A holis- tic lexicon-based approach to opinion mining. In WSDM ’08. Cícero Nogueira dos Santos and Maíra A. de C. Gatti. 2014. Deep convolutional neural networks for senti- ment analysis of short texts. In COLING. Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2017. An unsupervised neural attention model for aspect extraction. In ACL. Ruining He and Julian McAuley. 2016. Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative ﬁltering. Proceedings of the 25th International Conference on World Wide Web. Jeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In ACL. Minqing Hu and Bing Liu. 2004. Mining and summa- rizing customer reviews. Proceedings of the tenth ACM SIGKDD international conference on Knowl- edge discovery and data mining. Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully automatic lexicon expansion for domain-oriented sentiment analysis. In EMNLP. James Kirkpatrick, Razvan Pascanu, Neil C. Rabi- nowitz, Joel Veness, Guillaume Desjardins, An- drei A. Rusu, Kieran Milan, John Quan, Tiago Ra- malho, Agnieszka Grabska-Barwinska, Demis Hass- abis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017. Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 114:3521 – 3526. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario vSavsko, Gun- jan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Th’eo Ma- tussiere, Lysandre Debut, Stas Bekman, Pierric Cis- tac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander M. Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. ArXiv, abs/2109.02846. Ruifan Li, Hao Chen, Fangxiang Feng, Zhanyu Ma, Xi- aojie Wang, and Eduard H. Hovy. 2021. Dual graph convolutional networks for aspect-based sentiment analysis. In ACL. Pengfei Liu, Shaﬁq R. Joty, and Helen M. Meng. 2015. Fine-grained opinion mining with recurrent neural networks and word embeddings. In EMNLP. Shu Liu, Kai-Wen Li, and Zuhe Li. 2022. A robustly optimized bmrc for aspect sentiment triplet extrac- tion. In NAACL. Haiyun Peng, Lu Xu, Lidong Bing, Fei Huang, Wei Lu, and Luo Si. 2020. Knowing what, how and why: A near complete solution for aspect-based sentiment analysis. In AAAI. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Moham- mad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orphée De Clercq, Véronique Hoste, Marianna Apidianaki, Xavier Tannier, Na- talia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel, Salud María Jiménez-Zafra, and Gül¸ sen Eryi ˘git. 2016. SemEval-2016 task 5: Aspect based senti- ment analysis. In Proceedings of the 10th Interna- tional Workshop on Semantic Evaluation (SemEval- 2016), pages 19–30, San Diego, California. Associa- tion for Computational Linguistics. Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015.SemEval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 486–495, Denver, Colorado. Association for Computational Linguistics. Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 task 4: As- pect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27–35, Dublin, Ireland. As- sociation for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a uniﬁed text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1–67. Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang A. Sutawika, Zaid Alyafeai, An- toine Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Ur- mish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Ab- heesht Sharma, Andrea Santilli, Thibault Févry, Ja- son Alan Fries, Ryan Teehan, Stella Rose Bider- man, Leo Gao, Tali Bers, Thomas Wolf, and Alexan- der M. Rush. 2022. Multitask prompted train- ing enables zero-shot task generalization. ArXiv, abs/2110.08207. Lei Shu, Jiahua Chen, Bing Liu, and Hu Xu. 2022. Zero-shot aspect-based sentiment analysis. ArXiv, abs/2202.01924. Lei Shu, Hu Xu, and Bing Liu. 2019. Controlled cnn-based sequence labeling for aspect extraction. ArXiv, abs/1905.06407. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, A. Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In EMNLP. Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie- Yan Liu. 2020. Mpnet: Masked and permuted pre-training for language understanding. ArXiv, abs/2004.09297. Stéphan Tulkens and Andreas van Cranenburgh. 2020. Embarrassingly simple unsupervised aspect extrac- tion. In ACL. Siddharth Varia, Shuai Wang, Kishaloy Halder, Robert Vacareanu, Miguel Ballesteros, Yassine Benajiba, Neha Anna John, Rishita Anubhai, Smaranda Mure- san, and Dan Roth. 2022. Instruction tuning for few- shot aspect-based sentiment analysis. ArXiv. Duy-Tin V o and Yue Zhang. 2015. Target-dependent twitter sentiment classiﬁcation with rich automatic features. In IJCAI. Bo Wang, Tao Shen, Guodong Long, Tianyi Zhou, and Yi Chang. 2021. Eliminating sentiment bias for aspect-level sentiment classiﬁcation with unsu- pervised opinion extraction. In EMNLP. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow- icz, and Jamie Brew. 2020. Transformers: State-of- the-art natural language processing. In EMNLP. Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2019. Bert post-training for review reading comprehen- sion and aspect-based sentiment analysis. ArXiv, abs/1904.02232. Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao. 2013. Mining opinion words and opinion tar- gets in a two-stage framework. InProceedings of the 51st Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 1764–1773, Soﬁa, Bulgaria. Association for Compu- tational Linguistics. Lu Xu, Hao Li, Wei Lu, and Lidong Bing. 2020. Position-aware tagging for aspect sentiment triplet extraction. In EMNLP. Hang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng Zhang. 2021. A uniﬁed generative frame- work for aspect-based sentiment analysis. ArXiv, abs/2106.04300. Guoxin Yu, Jiwei Li, Ling Luo, Yuxian Meng, Xiang Ao, and Qing He. 2021. Self question-answering: Aspect-based sentiment analysis by role ﬂipped ma- chine reading comprehension. In EMNLP. Meishan Zhang, Yue Zhang, and Duy-Tin V o. 2015a. Neural networks for open domain targeted sentiment. In EMNLP. Wenxuan Zhang, Yang Deng, Xin Li, Yifei Yuan, Li- dong Bing, and Wai Lam. 2021a. Aspect sentiment quad prediction as paraphrase generation. ArXiv, abs/2110.00796. Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and Wai Lam. 2021b. Towards generative aspect-based sentiment analysis. In ACL. Wenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and Wai Lam. 2022. A survey on aspect-based sentiment analysis: Tasks, methods, and challenges. ArXiv, abs/2203.01054.Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015b. Character-level convolutional networks for text clas- siﬁcation. Advances in neural information process- ing systems, 28. A Implementation details We use HuggingFace’s implementation of trans- formers (Wolf et al., 2020; Lhoest et al., 2021). We use similar parameters as (Varia et al., 2022). We run our experiments on NVIDIA Tesla V100 GPUs. B All Experiments For completeness, we include here all the models investigated over the 3 datasets, LAP14, REST 15, and REST 16, respectively. B.1 Full-Training We report the results (test) on Full Training in Ta- bles 7, 8, 9. B.2 K-Shot Learning We report the results (test) on K-Shot Learning in Tables 10, 11, 12. B.3 Cross Domain We experiment with pre-training on a different do- main than the domain of the downstream task. Con- cretely, we perform two experiments: (i) we per- form NAPT on restaurant domain, then ﬁnetune on the laptop domain, and (ii) we perform NAPT on the laptop domain, then ﬁnetune on the restau- rant domain. We include the results with our pro- posed model trained with NAPT on restaurant data and ﬁnetuned on LAP14 in Table 13. We include the results with our proposed model trained with NAPT on laptop data and ﬁnetuned on REST 15 and REST 16 in Table 14 and in Table 15, respec- tively. We observed that our proposed model is still able to transfer the knowledge learned during the NAPT phase. Our proposed model still outper- forms the baseline, brining as much as 11.49% F1 points for the ASTE task in the laptop domain. In some cases we noticed a slight increase in the ﬁ- nal performance compared to the model trained with NAPT on the full dataset. This suggests that the model trained on the full dataset overﬁts to the noisy data. C Multi-word Patterns In Table 16 we list all the patterns that were used to ﬁlter 2-grams, 3-grams and 4-grams during aspect term extraction.Model NAPT Task (F1↑) AverageAE AESC ASTE Text (t5-base) No 76.13±1.06 66.57 ±1.01 59.50 ±1.35 67.40±7.13 IT (t5-base) No 77.09±0.68 66.25 ±0.45 60.47 ±1.36 67.94±7.18 Yes 76.96±1.17 66.08 ±0.80 60.03 ±1.23 67.69±7.16 IT-MTL (t5-base) No 77.64±0.75 66.54 ±1.09 60.17 ±1.19 68.11±7.53 Yes 77.67±1.04 66.66 ±0.69 59.97 ±1.28 68.10±7.45 IT (t5-large) No 77.18±1.64 67.20 ±1.23 60.24 ±0.61 68.21±7.28 Yes 76.79±1.05 66.66 ±1.16 60.98 ±1.78 68.14±6.78 IT-MTL (t5-large) No 77.89±0.53 66.44 ±1.06 59.83 ±2.32 68.05±7.85 Yes 77.95±1.00 65.62 ±1.23 59.34 ±1.42 67.64±7.95 IT No 75.77±0.71 65.99 ±0.98 59.28 ±0.64 67.01±7.05 (continued pre-training) (t5-base) Yes 76.19±1.33 66.28 ±1.36 59.38 ±1.25 67.28±7.09 IT-MTL No 76.37±0.82 65.85 ±1.03 58.24 ±1.03 66.82±7.74 (continued pre-training) (t5-base) Yes 76.68±0.88 65.95 ±1.06 58.44 ±1.26 67.03±7.64 Table 7: Comparison of full dataset training performances on all 3 ABSA tasks for LAP14. Model NAPT Task (F1↑) AverageAE AESC TASD ASTE ASQP Text (t5-base) No 72.76±0.96 66.43 ±1.45 60.05 ±0.67 51.74 ±0.84 46.66 ±0.67 59.53±9.72 IT (t5-base) No 73.54±1.20 67.09 ±0.53 59.78 ±0.91 52.78 ±0.81 46.79 ±0.59 59.99±9.82 Yes 72.89±1.31 65.98 ±1.29 59.30 ±0.77 52.62 ±1.13 46.49 ±0.71 59.45±9.48 IT-MTL (t5-base) No 73.85±1.14 67.46 ±0.80 59.88 ±1.02 53.17 ±0.67 47.17 ±1.03 60.30±9.81 Yes 74.55±1.26 67.53 ±1.37 59.29 ±1.67 53.57 ±1.42 47.30 ±1.21 60.45±9.86 IT (t5-large) No 74.24±0.74 69.83 ±1.10 62.82 ±0.69 55.96 ±0.41 49.61 ±0.55 62.49±9.16 Yes 74.68±0.72 69.94 ±1.18 62.82 ±0.94 54.72 ±1.53 49.48 ±1.04 62.33±9.47 IT-MTL (t5-large) No 75.79±0.69 70.18 ±1.31 62.84 ±1.37 54.16 ±0.95 48.86 ±1.13 62.37±10.17 Yes 74.80±0.94 68.26 ±0.96 61.11 ±1.10 53.69 ±1.40 48.41 ±1.26 61.25±9.70 IT No 73.05±1.05 67.17 ±1.16 59.09 ±0.91 51.89 ±1.09 46.51 ±0.36 59.54±9.92 (continued pre-training) (t5-base) Yes 72.82±1.11 67.44 ±0.99 60.42 ±0.95 53.07 ±0.88 47.56 ±1.50 60.26±9.31 IT-MTL No 74.14±0.47 68.06 ±0.49 60.97 ±0.59 53.42 ±1.27 47.49 ±0.90 60.82±9.84 (continued pre-training) (t5-base) Yes 74.66±1.06 68.59 ±0.78 61.14 ±0.88 53.42 ±0.75 48.41 ±0.55 61.24±9.69 Table 8: Comparison of full dataset training performances on all 5 ABSA tasks for REST 15.Model NAPT Task (F1↑) AverageAE AESC TASD ASTE ASQP Text (t5-base) No 78.40±1.14 73.64 ±1.30 67.05 ±0.96 62.95 ±0.61 57.77 ±1.13 67.96±7.58 IT (t5-base) No 79.74±0.98 74.24 ±0.54 68.04 ±0.86 63.77 ±0.82 58.41 ±0.73 68.84±7.72 Yes 78.69±1.30 72.90 ±0.98 67.40 ±1.20 61.96 ±0.94 57.57 ±1.25 67.70±7.66 IT-MTL (t5-base) No 79.90±0.62 74.51 ±0.91 67.59 ±0.75 62.69 ±0.69 57.72 ±0.76 68.48±8.15 Yes 78.53±0.75 73.31 ±0.87 66.72 ±0.98 61.67 ±0.65 56.78 ±0.65 67.40±7.90 IT (t5-large) No 79.66±0.98 76.90 ±0.93 70.24 ±1.13 65.15 ±0.20 60.13 ±1.06 70.42±7.42 Yes 78.87±1.11 75.25 ±0.80 70.40 ±0.81 64.61 ±1.11 59.76 ±0.86 69.78±7.06 IT-MTL (t5-large) No 79.67±0.50 75.01 ±0.95 69.12 ±1.04 62.84 ±0.98 58.79 ±0.99 69.09±7.85 Yes 79.33±0.78 74.66 ±0.72 67.11 ±1.66 62.43 ±0.99 57.17 ±1.17 68.14±8.18 IT No 79.22±0.59 74.05 ±0.70 67.58 ±1.61 62.69 ±1.58 57.73 ±0.82 68.25±7.92 (continued pre-training) (t5-base) Yes 79.06±0.92 74.38 ±1.30 68.40 ±1.21 62.33 ±1.25 58.24 ±0.83 68.48±7.74 IT-MTL No 79.25±0.58 74.13 ±0.56 67.72 ±0.80 62.38 ±0.69 58.04 ±0.87 68.30±7.86 (continued pre-training) (t5-base) Yes 78.72±0.73 73.88 ±0.95 67.16 ±1.00 62.00 ±1.15 56.61 ±1.01 67.68±8.05 Table 9: Comparison of full dataset training performances on all 5 ABSA tasks for REST 16.K Model NAPT Task (F1↑) AverageAE AESC ASTE 5 Text (t5-base) No 37.45±2.94 22.91±1.65 12.06±1.83 24.14±10.96 IT (t5-base) No 44.59±1.15 26.81±2.35 13.04±0.91 28.14±13.45 Yes 47.46±2.76 38.85±2.11 28.88±1.58 38.40±7.98 IT-MTL (t5-base) No 36.63±3.03 25.31±2.78 15.96±2.11 25.97±9.09 Yes 47.02±2.60 36.49±1.97 27.53±1.97 37.02±8.34 IT (t5-large) No 43.01±2.09 26.73±2.86 16.14±2.19 28.63±11.66 Yes 46.92±2.71 37.52±2.44 25.81±2.62 36.75±9.13 IT-MTL (t5-large) No 40.88±3.65 27.47±2.72 17.37±2.51 28.57±10.35 Yes 45.30±3.29 32.47±5.05 23.54±5.34 33.77±10.13 IT No 36.59±0.91 22.82±1.20 12.38±0.88 23.93±10.31 (continued pre-training) (t5-base) Yes 45.83±1.80 38.85±1.31 28.15±1.84 37.61±7.53 IT-MTL No 26.25±2.32 22.40±1.26 13.62±1.98 20.76±5.75 (continued pre-training) (t5-base) Yes 45.28±1.27 36.61±1.46 27.33±2.02 36.41±7.58 10 Text (t5-base) No 46.85±2.12 33.67±1.71 18.95±2.91 33.16±11.99 IT (t5-base) No 52.12±2.42 37.49±1.91 25.22±0.83 38.28±11.51 Yes 55.98±2.16 45.02±1.64 36.62±2.61 45.87±8.29 IT-MTL (t5-base) No 48.71±1.89 39.13±2.29 28.00±2.59 38.61±9.01 Yes 55.81±2.14 44.49±1.50 35.15±1.71 45.15±8.72 IT (t5-large) No 49.44±9.70 36.64±3.64 25.10±1.46 37.06±11.71 Yes 53.13±4.59 43.35±2.91 34.94±1.49 43.81±8.19 IT-MTL (t5-large) No 49.23±4.91 36.13±2.07 27.16±3.74 37.51±10.01 Yes 51.99±3.47 41.45±2.28 31.05±4.58 41.50±9.35 IT No 41.61±6.49 33.89±1.69 21.36±2.57 32.29±9.45 (continued pre-training) (t5-base) Yes 55.69±2.27 45.77±1.55 34.51±1.20 45.32±8.91 IT-MTL No 41.65±1.78 34.44±2.71 24.55±1.50 33.55±7.50 (continued pre-training) (t5-base) Yes 56.16±2.60 46.17±1.79 35.25±1.06 45.86±8.84 20 Text (t5-base) No 56.56±1.15 42.64±0.99 29.18±2.23 42.79±11.66 IT (t5-base) No 59.08±1.97 44.82±1.24 33.24±1.53 45.71±11.04 Yes 61.67±1.81 48.88±1.10 41.20±2.01 50.58±8.70 IT-MTL (t5-base) No 57.98±3.72 47.14±2.42 34.55±1.85 46.56±10.24 Yes 61.05±1.62 48.94±1.68 38.17±1.96 49.38±9.60 IT (t5-large) No 59.30±2.38 46.88±2.92 34.44±2.61 46.88±10.79 Yes 61.43±1.44 49.00±3.37 38.52±1.84 49.65±9.79 IT-MTL (t5-large) No 61.02±2.89 46.78±4.32 36.00±1.17 47.93±10.99 Yes 61.16±1.97 49.68±2.13 38.10±2.41 49.65±9.80 IT No 53.92±1.64 43.56±1.02 28.45±1.62 41.98±10.91 (continued pre-training) (t5-base) Yes 60.06±2.47 49.73±1.48 40.19±1.64 49.99±8.42 IT-MTL No 55.64±2.04 45.44±1.97 32.12±1.28 44.40±10.11 (continued pre-training) (t5-base) Yes 60.93±1.36 49.85±1.65 37.96±1.78 49.58±9.61 50 Text (t5-base) No 65.31±1.86 54.35±1.15 40.84±2.53 53.50±10.51 IT (t5-base) No 68.95±1.22 54.92±1.07 44.67±2.12 56.18±10.40 Yes 68.14±1.12 54.67±1.82 46.56±1.38 56.46±9.11 IT-MTL (t5-base) No 67.54±1.62 55.86±1.90 45.10±2.69 56.16±9.69 Yes 68.23±1.34 54.79±1.68 45.85±1.11 56.29±9.40 IT (t5-large) No 68.27±3.17 56.37±1.48 45.26±1.55 56.64±9.94 Yes 68.36±1.15 57.99±2.05 47.23±2.36 57.86±8.97 IT-MTL (t5-large) No 69.92±1.23 56.33±1.24 44.87±2.10 57.04±10.70 Yes 70.07±1.30 55.99±0.95 45.99±2.25 57.35±10.16 IT No 63.36±1.05 48.97±0.84 37.31±1.78 49.88±11.09 (continued pre-training) (t5-base) Yes 68.78±1.42 55.20±1.08 45.50±1.44 56.49±9.74 IT-MTL No 63.72±0.64 53.02±1.08 40.83±1.10 52.53±9.72 (continued pre-training) (t5-base) Yes 69.19±1.31 55.73±1.11 45.44±1.56 56.79±9.92 Table 10: Comparison of k-Shot performances on all 3 ABSA tasks for LAP14.K Model NAPT Task (F1↑) AverageATE AESC TASD ASTE ASQP 5 Text (t5-base) No 44.55±2.55 39.44±2.64 24.62±1.56 20.11±1.05 12.88±0.91 28.32±12.26 IT (t5-base) No 49.33±0.66 42.48±1.84 24.75±0.65 24.44±1.09 15.52±1.47 31.31±12.87 Yes 50.05±2.91 43.95±1.79 30.46±1.87 31.59±1.35 21.72±0.90 35.56±10.37 IT-MTL (t5-base) No 48.14±2.79 41.42±3.28 24.79±2.33 24.49±1.85 15.28±1.64 30.82±12.53 Yes 51.11±1.81 43.51±1.55 27.12±1.97 30.35±1.48 18.98±1.39 34.21±11.76 IT (t5-large) No 46.40±1.56 41.24±0.86 24.73±1.99 22.72±1.95 16.04±3.00 30.23±11.96 Yes 47.87±4.76 43.01±2.77 28.42±7.70 30.49±1.43 20.85±1.79 34.13±10.84 IT-MTL (t5-large) No 44.54±2.84 36.25±1.78 19.08±3.03 18.92±3.92 10.57±2.01 25.87±13.05 Yes 48.47±1.98 40.38±2.76 23.79±3.88 26.97±3.56 16.25±3.37 31.17±12.16 IT No 46.06±2.36 39.34±3.07 24.67±1.17 22.70±0.85 14.47±1.62 29.45±11.92 (continued pre-training) (t5-base) Yes 50.40±1.76 44.06±1.59 29.32±2.16 31.31±2.31 22.20±2.32 35.46±10.53 IT-MTL No 47.78±2.49 39.59±1.24 24.33±1.43 22.93±0.56 14.55±1.32 29.84±12.40 (continued pre-training) (t5-base) Yes 50.87±2.76 44.15±2.18 29.30±2.79 31.60±2.05 20.98±2.28 35.38±11.06 10 Text (t5-base) No 54.71±0.91 49.28±0.46 36.26±1.62 31.99±0.80 24.42±0.68 39.33±11.41 IT (t5-base) No 56.62±1.59 51.03±1.93 37.64±1.50 33.25±1.54 25.76±1.08 40.86±11.71 Yes 57.91±1.29 50.78±1.42 37.37±1.81 37.63±1.26 28.78±1.11 42.49±10.59 IT-MTL (t5-base) No 58.10±0.72 48.27±0.98 37.26±0.29 33.75±0.74 26.48±1.01 40.77±11.41 Yes 58.72±1.23 49.95±1.30 36.77±1.68 37.82±1.70 28.03±1.21 42.26±10.95 IT (t5-large) No 54.58±1.99 48.32±1.27 35.31±1.90 34.55±0.86 25.43±1.79 39.64±10.76 Yes 55.69±1.94 49.52±1.42 38.11±1.76 36.54±1.71 28.10±1.53 41.59±10.04 IT-MTL (t5-large) No 54.14±1.11 45.38±1.09 33.90±2.76 30.95±1.68 23.10±1.47 37.49±11.31 Yes 55.00±3.53 46.91±3.01 35.09±2.65 32.82±2.97 24.79±2.71 38.92±11.19 IT No 56.55±2.35 51.28±0.82 39.02±2.58 33.70±1.41 25.10±0.66 41.13±11.81 (continued pre-training) (t5-base) Yes 57.96±1.36 51.42±1.41 39.33±1.29 37.81±1.68 29.57±1.32 43.22±10.31 IT-MTL No 58.31±0.92 49.57±2.13 39.00±2.28 33.01±1.21 25.58±0.75 41.09±11.98 (continued pre-training) (t5-base) Yes 57.88±1.58 50.34±1.87 38.56±1.47 37.83±1.22 28.73±1.32 42.67±10.42 20 Text (t5-base) No 58.91±1.69 53.77±0.90 42.37±1.55 37.27±1.85 30.45±0.83 44.55±10.76 IT (t5-base) No 62.08±1.85 53.91±2.18 42.89±0.86 38.35±0.83 30.77±1.19 45.60±11.45 Yes 61.84±1.18 53.80±1.19 44.13±1.19 41.93±1.13 34.23±1.30 47.19±9.76 IT-MTL (t5-base) No 63.77±1.86 53.47±2.10 43.27±1.33 40.66±2.07 33.27±0.76 46.89±10.97 Yes 63.77±1.15 55.48±1.55 44.24±1.18 42.77±1.16 34.71±0.91 48.19±10.36 IT (t5-large) No 59.97±1.49 55.11±1.86 45.59±1.00 40.27±1.10 34.40±1.80 47.07±9.67 Yes 62.13±1.32 55.85±1.68 46.35±2.68 41.79±0.71 35.69±1.19 48.36±9.75 IT-MTL (t5-large) No 62.26±1.55 54.59±2.62 45.04±1.44 40.39±2.01 34.23±1.12 47.30±10.35 Yes 63.19±1.70 55.67±2.23 44.23±1.40 41.77±1.48 34.43±1.25 47.86±10.49 IT No 62.30±1.44 55.82±1.49 45.16±1.25 38.23±1.54 31.58±0.96 46.62±11.52 (continued pre-training) (t5-base) Yes 62.85±1.38 56.12±0.90 45.51±1.57 42.07±1.53 34.48±1.13 48.21±10.25 IT-MTL No 63.42±0.89 55.09±0.49 46.43±1.13 40.40±1.45 32.85±0.67 47.64±11.00 (continued pre-training) (t5-base) Yes 63.91±1.21 56.14±1.47 46.40±1.18 42.80±1.34 36.15±0.92 49.08±9.97 50 Text (t5-base) No 62.55±1.74 57.12±1.31 48.50±0.97 43.09±0.91 35.51±0.82 49.35±9.91 IT (t5-base) No 64.74±1.15 59.35±0.91 50.40±0.65 43.79±1.12 37.51±0.72 51.16±10.17 Yes 65.17±0.76 58.96±0.92 49.72±1.24 44.74±1.34 39.10±1.16 51.54±9.56 IT-MTL (t5-base) No 67.51±0.89 58.98±1.52 50.45±1.49 45.27±0.76 37.69±1.04 51.98±10.68 Yes 67.55±1.18 60.19±1.23 50.51±1.09 46.76±0.93 39.94±0.86 52.99±9.91 IT (t5-large) No 64.75±0.94 59.33±0.47 52.19±0.93 45.59±0.75 40.66±1.12 52.50±8.99 Yes 66.82±1.16 61.21±1.40 52.53±1.76 47.19±1.30 42.27±1.41 54.00±9.16 IT-MTL (t5-large) No 67.84±1.16 60.77±1.23 51.70±0.97 46.76±1.45 39.92±1.00 53.40±10.18 Yes 68.15±0.86 61.67±0.94 52.02±1.47 47.33±1.30 41.24±1.18 54.08±9.86 IT No 64.49±0.95 60.23±0.51 51.51±0.81 44.10±1.74 37.56±1.30 51.58±10.20 (continued pre-training) (t5-base) Yes 65.37±1.20 59.64±1.12 51.08±0.65 45.49±1.14 39.37±0.90 52.19±9.49 IT-MTL No 67.46±1.03 61.93±0.70 52.73±1.10 46.06±0.61 39.71±1.70 53.58±10.38 (continued pre-training) (t5-base) Yes 67.37±0.96 60.54±1.29 51.57±1.25 46.96±1.12 40.39±1.03 53.37±9.72 Table 11: Comparison of k-Shot performances on all 5 ABSA tasks for REST 15.K Model NAPT Task (F1↑) AverageAE AESC TASD ASTE ASQP 5 Text (t5-base) No 52.67±0.69 47.87±1.34 31.57±1.74 29.58±1.96 19.76±1.44 36.29±12.52 IT (t5-base) No 55.59±2.74 51.62±1.46 36.26±1.15 34.10±1.17 23.89±2.11 40.29±12.07 Yes 61.54±1.35 55.32±2.05 39.13±2.11 40.18±1.60 28.64±1.82 44.96±12.09 IT-MTL (t5-base) No 59.78±1.32 52.35±0.82 36.88±1.77 36.27±0.90 25.86±1.63 42.23±12.50 Yes 64.25±1.60 55.22±1.35 38.97±2.19 40.95±1.36 29.58±1.76 45.79±12.54 IT (t5-large) No 55.88±1.63 52.90±2.02 38.37±2.79 36.70±0.83 27.70±1.85 42.31±10.91 Yes 62.01±1.48 55.91±2.68 37.09±7.90 41.14±1.78 32.13±2.04 45.66±12.13 IT-MTL (t5-large) No 56.81±2.44 48.65±1.32 32.64±2.56 32.47±1.80 23.36±1.16 38.79±12.52 Yes 60.50±1.91 51.89±2.50 34.94±3.71 37.71±2.08 27.04±2.22 42.42±12.46 IT No 55.87±2.42 50.92±3.05 36.57±1.38 31.41±1.94 20.39±2.38 39.03±13.37 (continued pre-training) (t5-base) Yes 62.22±1.99 56.70±1.43 37.00±2.16 39.19±1.67 27.18±1.64 44.46±13.22 IT-MTL No 55.93±1.71 48.95±2.26 34.71±2.20 32.02±0.88 22.79±1.46 38.88±12.31 (continued pre-training) (t5-base) Yes 62.74±1.21 55.43±0.86 37.13±2.36 39.84±1.37 27.80±1.88 44.59±12.89 10 Text (t5-base) No 59.45±0.89 54.33±1.03 38.85±1.95 36.82±0.91 29.31±1.17 43.75±11.59 IT (t5-base) No 62.14±1.14 57.02±2.17 40.34±2.22 40.37±0.74 29.90±0.94 45.95±12.20 Yes 65.33±1.18 58.84±1.48 42.69±2.83 44.24±1.07 32.30±1.39 48.68±12.07 IT-MTL (t5-base) No 64.03±1.81 56.51±0.97 41.53±1.12 39.66±1.50 31.27±1.37 46.60±12.24 Yes 65.85±1.08 57.96±1.14 41.66±2.32 44.42±1.00 32.77±2.38 48.53±12.04 IT (t5-large) No 59.01±1.07 51.11±3.59 42.76±1.89 39.66±1.81 31.75±2.54 44.86±9.84 Yes 61.41±2.08 57.90±1.18 43.13±2.28 43.26±1.74 35.40±2.29 48.22±10.10 IT-MTL (t5-large) No 59.76±1.11 53.26±2.04 39.01±2.52 37.45±1.46 29.06±1.24 43.71±11.52 Yes 61.85±1.89 54.15±2.23 39.64±2.10 39.74±2.18 31.13±2.01 45.30±11.39 IT No 59.25±2.32 56.57±2.06 39.28±2.12 37.84±1.59 26.17±1.79 43.82±12.78 (continued pre-training) (t5-base) Yes 63.34±2.30 59.95±1.25 42.75±2.43 44.85±2.03 32.25±1.23 48.63±11.73 IT-MTL No 60.50±1.25 55.34±0.67 41.57±2.03 38.22±0.89 30.40±1.30 45.20±11.41 (continued pre-training) (t5-base) Yes 65.10±1.28 57.91±1.32 43.31±1.75 43.55±1.59 34.27±1.53 48.83±11.28 20 Text (t5-base) No 63.34±1.24 57.56±1.21 44.90±1.99 42.11±1.82 35.20±0.58 48.62±10.62 IT (t5-base) No 65.89±1.90 60.52±1.44 47.27±2.49 44.27±0.99 36.39±0.75 50.87±11.14 Yes 66.73±1.49 60.78±1.11 50.49±1.21 47.75±1.07 40.14±1.28 53.18±9.61 IT-MTL (t5-base) No 65.82±0.96 59.66±1.06 49.30±0.99 44.71±0.72 38.71±0.76 51.64±10.10 Yes 67.97±0.97 60.81±1.05 49.82±1.09 47.94±1.11 40.25±1.24 53.36±9.95 IT (t5-large) No 64.63±0.41 61.07±0.94 49.74±2.34 46.02±1.34 40.53±1.04 52.40±9.36 Yes 65.24±1.28 60.14±2.72 51.82±1.85 48.44±0.97 41.14±1.09 53.35±8.76 IT-MTL (t5-large) No 66.26±2.38 59.48±2.01 48.37±2.96 44.70±3.16 37.42±3.04 51.25±10.85 Yes 67.08±1.94 60.17±1.09 49.08±1.99 47.13±1.56 39.76±1.43 52.64±9.96 IT No 63.43±1.03 58.89±1.36 46.15±2.18 44.17±1.96 35.39±0.76 49.61±10.51 (continued pre-training) (t5-base) Yes 65.85±0.78 60.97±0.69 49.82±1.10 47.38±1.23 39.20±1.17 52.64±9.71 IT-MTL No 66.16±1.22 60.56±0.99 49.84±1.06 44.86±2.36 38.42±0.86 51.97±10.43 (continued pre-training) (t5-base) Yes 68.00±1.07 61.54±1.14 50.66±1.09 48.11±1.08 40.35±1.30 53.73±9.97 50 Text (t5-base) No 69.06±0.70 63.97±0.59 55.42±0.70 50.50±0.99 45.91±1.56 56.97±8.73 IT (t5-base) No 70.11±0.84 65.75±1.08 55.06±0.94 51.58±1.23 47.56±1.36 58.01±8.78 Yes 70.14±0.97 65.13±0.82 55.86±0.95 52.63±0.94 47.53±1.02 58.26±8.36 IT-MTL (t5-base) No 72.11±1.36 65.68±1.05 56.92±0.84 52.80±1.07 46.75±1.39 58.85±9.29 Yes 71.92±0.88 65.88±0.70 56.56±0.99 53.83±1.08 47.88±1.37 59.21±8.72 IT (t5-large) No 70.57±0.96 67.34±1.68 58.99±1.29 53.13±0.93 48.87±0.94 59.78±8.46 Yes 71.77±0.77 66.66±1.11 59.59±1.44 55.06±1.45 50.36±0.89 60.69±7.88 IT-MTL (t5-large) No 71.73±0.55 66.65±1.05 57.89±0.76 53.17±2.33 47.69±1.62 59.42±9.02 Yes 72.38±0.83 66.70±0.77 58.48±1.27 53.89±1.56 48.45±1.53 59.98±8.78 IT No 69.80±1.11 65.11±0.51 55.94±1.51 50.75±1.06 45.25±1.11 57.37±9.27 (continued pre-training) (t5-base) Yes 70.06±1.29 64.81±1.12 55.68±0.95 52.12±0.98 46.69±1.49 57.87±8.62 IT-MTL No 72.08±0.79 66.74±0.99 58.02±0.95 52.48±1.77 46.66±1.35 59.19±9.49 (continued pre-training) (t5-base) Yes 71.20±0.87 65.79±1.19 56.68±0.96 53.31±0.87 47.10±0.85 58.82±8.76 Table 12: Comparison of k-Shot performances on all 5 ABSA tasks for REST 16.k AE AESC ASTE Average 5 47.55±2.06 36.55±2.35 24.53±2.25 33.06 10 55.93±2.80 45.55±2.39 35.38±1.80 43.33 20 64.55±1.47 52.18±1.07 41.67±1.97 52.51 50 69.52±0.71 56.25±1.44 46.49±1.97 57.30 Full Dataset77.32±1.18 68.20±0.72 60.93±1.12 68.56 Table 13: Cross-Domain performance of IT-MTL-NAPT on LAP14. The NAPT was done only on Yelp corpus. k AE AESC TASD ASTE ASQP Average 5 53.17±2.79 44.54±1.97 29.26±1.96 32.89±1.58 21.75±1.25 35.80 10 63.07±1.43 53.79±2.13 38.05±1.82 42.22±1.76 29.80±2.15 45.41 20 68.99±1.34 60.20±1.21 44.84±1.23 46.01±1.22 35.18±1.55 52.02 50 74.20±0.89 64.50±0.85 50.67±1.08 50.18±1.65 40.49±1.37 57.54 Full Dataset79.39±1.07 72.37±1.02 62.92±1.11 58.95±1.11 51.38±0.90 65.32 Table 14: Cross-Domain performance of IT-MTL-NAPT on REST 15. The NAPT was done only on Amazon Reviewscorpus. k AE AESC TASD ASTE ASQP Average 5 59.17±1.63 54.07±1.35 38.05±2.04 41.03±1.68 29.26±1.74 43.46 10 62.80±1.54 57.27±1.71 42.65±2.11 43.66±1.44 34.14±1.18 47.74 20 66.06±1.21 60.46±1.58 47.96±1.34 47.10±1.30 38.32±1.02 52.31 50 69.67±1.12 64.61±0.76 54.17±1.40 51.91±1.08 45.29±1.25 57.80 Full Dataset80.72±0.81 75.72±0.89 68.95±0.97 64.04±0.84 58.02±0.97 68.84 Table 15: Cross-Domain performance of IT-MTL-NAPT on REST 16. The NAPT was done only on Amazon Reviewscorpus. Multi-word Patterns NN*-NN* JJ*-NN* VBG-NN* VBN-NN* NN*-NN*-NN* NN*-IN-NN* JJ*-NN*-NN* JJ*-JJ*-NN* VBN-JJ*-NN* NN*-NN*-NN*-NN* NN*-CC-NN*-NN* Table 16: Multi-word Patterns used to ﬁlter 2-grams, 3-grams and 4-grams. ‘*’ denotes any variant of the corre- sponding POS tags. For example, NN* captures NN, NNS, NNP, NNPS.",
      "references": [
        "Mining and summarizing customer reviews",
        "SemEval-2014 task 4: Aspect based sentiment analysis",
        "SemEval-2015 task 12: Aspect based sentiment analysis",
        "SemEval-2016 task 5: Aspect based sentiment analysis",
        "Aspect sentiment quad prediction as paraphrase generation",
        "Zero-shot aspect-based sentiment analysis",
        "A survey on aspect-based sentiment analysis: Tasks, methods, and challenges",
        "Recursive deep models for semantic compositionality over a sentiment treebank",
        "Deep convolutional neural networks for sentiment analysis of short texts",
        "A uniﬁed generative framework for aspect-based sentiment analysis",
        "Towards generative aspect-based sentiment analysis",
        "Instruction tuning for few-shot aspect-based sentiment analysis",
        "Character-level convolutional networks for text classiﬁcation",
        "Embarrassingly simple unsupervised aspect extraction",
        "An unsupervised neural attention model for aspect extraction",
        "Fine-grained opinion mining with recurrent neural networks and word embeddings",
        "Target-dependent twitter sentiment classiﬁcation with rich automatic features",
        "Bert post-training for review reading comprehension and aspect-based sentiment analysis",
        "Dual graph convolutional networks for aspect-based sentiment analysis",
        "Eliminating sentiment bias for aspect-level sentiment classiﬁcation with unsupervised opinion extraction",
        "Neural networks for open domain targeted sentiment",
        "Self question-answering: Aspect-based sentiment analysis by role ﬂipped machine reading comprehension",
        "Position-aware tagging for aspect sentiment triplet extraction",
        "Knowing what, how and why: A near complete solution for aspect-based sentiment analysis",
        "Bidirectional machine reading comprehension for aspect sentiment triplet extraction",
        "A robustly optimized bmrc for aspect sentiment triplet extraction",
        "Controlled cnn-based sequence labeling for aspect extraction",
        "Universal language model ﬁne-tuning for text classiﬁcation",
        "BERT: pre-training of deep bidirectional transformers for language understanding",
        "Exploring the limits of transfer learning with a uniﬁed text-to-text transformer",
        "Mining opinion words and opinion targets in a two-stage framework",
        "Fully automatic lexicon expansion for domain-oriented sentiment analysis",
        "The pascal recognising textual entailment challenge",
        "Mpnet: Masked and permuted pre-training for language understanding",
        "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
        "Multitask prompted training enables zero-shot task generalization",
        "Overcoming catastrophic forgetting in neural networks",
        "Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative ﬁltering",
        "Transformers: State-of-the-art natural language processing",
        "Datasets: A community library for natural language processing"
      ],
      "meta_data": {
        "arxiv_id": "2305.11979v1",
        "authors": [
          "Robert Vacareanu",
          "Siddharth Varia",
          "Kishaloy Halder",
          "Shuai Wang",
          "Giovanni Paolini",
          "Neha Anna John",
          "Miguel Ballesteros",
          "Smaranda Muresan"
        ],
        "published_date": "2023-05-19T19:53:54Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of few-shot Aspect-Based Sentiment Analysis (ABSA) by proposing a weak supervision approach. The main contribution is a pipeline that constructs a noisy ABSA dataset from abundant unlabeled data, used to pre-train a sequence-to-sequence model. This method significantly improves few-shot performance (up to 15.84% absolute F1) on harder tasks and outperforms previous state-of-the-art in zero-shot learning for the Aspect Extraction Sentiment Classification (AESC) task, while also being capable of the harder Aspect Sentiment Triplet Extraction (ASTE) task. The approach preserves full fine-tuning performance.",
        "methodology": "The proposed methodology introduces a 'pretrain → Noisy ABSA Pre-Training (NAPT) → finetune' paradigm. The NAPT phase involves constructing a weakly annotated ABSA dataset without direct supervision. This dataset construction pipeline includes: (1) Aspect-term Extraction using spaCy tokenizer and frequent noun identification, extended to multi-word terms; (2) Opinion-term Extraction using a lexicon-based approach, including negations; (3) Linking Opinion-terms with Aspect-terms by reformulating it as a Natural Language Inference (NLI) problem, using an MPNet model with artificially constructed sentences; and (4) Sentiment Extraction using a sentence-level classifier on constructed sentences, applying a confidence threshold. The NAPT phase then pre-trains a base sequence-to-sequence model (T5) in a multi-task learning framework using instruction tuning across five factorized ABSA tasks. Overfitting is addressed through disjoint aspect-term splits in the noisy dataset and regularization techniques including standard weight decay, Tuple Dropout (50%), and biased weight decay.",
        "experimental_setup": "The experiments utilize T5 (t5-base and t5-large) as the pre-trained sequence-to-sequence model. Evaluation is performed using F1 score (exact match for tuples in k-shot/full-training, token-level F1 for zero-shot). The noisy-annotated dataset is created from 100K Amazon Electronics and 100K Yelp reviews, resulting in approximately 190K training and 12.5K validation examples with enforced disjoint aspect-terms. The model is tested on three widely used ABSA datasets: REST 15, REST 16, and LAP14, covering restaurant and laptop domains. Experiments are conducted in three scenarios: k-shot learning (with k=5, 10, 20, 50 examples), zero-shot evaluation, and full-training. Baselines include text-only, instruction tuning (IT), instruction tuning with multi-task learning (IT-MTL), and IT-MTL with in-domain pre-training (IT-MTL-ID). Each experiment is repeated with 5 random seeds, and the NAPT step with 3 seeds, averaging results over 15 runs for NAPT models.",
        "limitations": "The noisy pre-training step has only been evaluated on ABSA-related tasks, and its applicability to other structured prediction tasks remains unverified. The noisy corpus construction process is heavily dependent on English-based resources and pre-trained models, which makes extending the approach to other languages non-trivial. The paper also notes a lack of intrinsic evaluation (e.g., human supervision) regarding the quality of the constructed noisy corpus, relying instead on extrinsic evaluation.",
        "future_research_directions": "Future research directions include evaluating the proposed noisy pre-training step on other structured prediction tasks beyond ABSA to assess its generalizability. Another area for future work is to explore methods for extending the noisy corpus construction process to other languages, addressing its current heavy reliance on English-based resources and pre-trained models. Additionally, intrinsic evaluation of the noisy corpus, possibly through human supervision, is suggested to further assess its quality.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Instance-adaptive Zero-shot Chain-of-Thought Prompting",
      "full_text": "1 Life-long Learning and Testing for Automated Vehicles via Adaptive Scenario Sampling as A Continuous Optimization Process Jingwei Ge, Pengbo Wang, Cheng Chang, Yi Zhang, Senior Member, IEEE, Danya Yao, Li Li, Fellow, IEEE Abstract—Sampling critical testing scenarios is an essential step in intelligence testing for Automated Vehicles (A Vs). How- ever, due to the lack of prior knowledge on the distribution of crit- ical scenarios in sampling space, we can hardly efficiently find the critical scenarios or accurately evaluate the intelligence of A Vs. To solve this problem, we formulate the testing as a continuous optimization process which iteratively generates potential critical scenarios and meanwhile evaluates these scenarios. A bi-level loop is proposed for such life-long learning and testing. In the outer loop, we iteratively learn space knowledge by evaluating A V in the already sampled scenarios and then sample new scenarios based on the retained knowledge. Outer loop stops when all generated samples cover the whole space. While to maximize the coverage of the space in each outer loop, we set an inner loop which receives newly generated samples in outer loop and outputs the updated positions of these samples. We assume that points in a small sphere-like subspace can be covered (or represented) by the point in the center of this sphere. Therefore, we can apply a multi-rounds heuristic strategy to move and pack these spheres in space to find the best covering solution. The simulation results show that faster and more accurate evaluation of A Vs can be achieved with more critical scenarios. Index Terms—Intelligence testing, Senario generation, Life- long learning and testing, Optimization, Automated vehicles. I. I NTRODUCTION I NTELLIGENCE testing is a mandatory step in determining that Automated Vehicles (A Vs) are trained sufficiently well to be mass-produced [1] [2] [3] [4] [5]. The implementation of testing is mainly carried out in testing scenarios, where A V is required to handle various testing tasks and then be evaluated [6] [7] [8] [9] [10] [11]. Many studies [12] [13]have noted that A Vs rarely make mistakes in the scenarios where they have been trained or experienced. For certain scenarios that it has not experienced, A V is not always able to pass [14]. In testing, it is expected to identify these critical scenarios as soon as possible through sampling and generation. So that we can, in turn, upgrade or This work was supported in part by the National Key Research and De- velopment Program of China under Grant 2021YFB2501200. ( Corresponding author: Li Li, Yi Zhang ) Jingwei Ge, Pengbo Wang, Cheng Chang, and Danya Yao are with the Department of Automation, Tsinghua University, Beijing 100084, China . Yi Zhang is with Department of Automation, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua Univer- sity, Beijing 100084, China; Tsinghua-Berkeley Shenzhen Institute (TBSI), Shenzhen 518055, China; Jiangsu Province Collaborative Innovation Center of Modern Urban Traffic Technologies, Nanjing 210096, China. Li Li is with the Department of Automation, BNRist, Tsinghua University, Beijing 100084, China (e-mail: li-li@tsinghua.edu.cn). modify the A V based on the testing results, realizing a closed loop of learning and testing [15]. However, most of the current intelligent algorithms for training A Vs are unexplainable and black-box, which makes it difficult to know what specific scenarios are critical for an A V [16] [17]. Obviously, testing A V in all possible scenarios is unrealistic. Instead, we should speculate the potentially critical scenarios based on the A V evaluation results (or scores) in known scenarios. To realize this, the following two questions revealed in [18] need to be answered: Q1. ”How to define testing scenarios?” A testing scenario typically involves a static environment in a segment of space and interactions between dynamic traffic participants, i.e., Sur- rounding Vehicles (SVs) in this paper, over a period of time. Some literature discretizes the testing scenario and defines it as a spatio-temporal segment consisting of snapshots arranged by sampling time [19] [20] [21] [22]. Each snapshot describes the positional state of each SVs at a specific sampling moment [23] [24]. However, the testing scenarios are complex, and this approach is prone to what is known as the dimensionality catastrophe [17]. Therefore, in this paper, we opt for another type of scenario definition, namely parameterized scenarios, as shown in Fig.1. Important scenario parameters are extracted as variables and the scenario parameters space (we refer to this as space for simplicity) is constructed accordingly. The coordinates of each point in space represent the parameters that can be used to construct various detailed scenarios. A small number of these parameters can capture the reasonable and rich spatial-temporal relationships that arise from vehicle interactions in the scenarios [25] [26]. Q2. ”How to find critical scenarios as fast as possible?” Some studies assumed that A V may not work well in the rare scenarios and thus focused on naturalistic driving data. [27] [28]. This involves assuming the frequency of occurrence of different scenarios in the dataset as a form of distribu- tion. Monte Carlo search is then used in combination with importance sampling to search for a fitted distribution [29]. Through this, scenarios with low frequency of occurrence can be sampled as critical scenarios to accelerate testing efficiency. However, [15] points out that considering both the frequency and criticality of scenarios at the same time often makes it difficult to eliminate estimation and approximation errors. Ad- ditionally, optimization-based testing methods are also worth exploring further. For instance, in [30], the authors introduce the few-shot testing framework. This framework optimizes the arXiv:2405.00696v1  [cs.RO]  28 Mar 20242 Behavior model of i-th SV: 1 ( , , | , , )i i i i i i i i t t t tv f v x y   + =   Para 2 Sampled lane-changing behavior A sampleSV1 SV2AV SV1 SV2AV SV2 SV1 AV SV2 SV1 AV Required Testing Scenario A detailed testing scenarioTesting Scenario Space Para 1 Fig. 1. Scenario generation using parameterized space. The static environment of scenario is constructed and the parameters of SVs models, like αi and βi are extracted; Then the parameters are used to construct scenario sampling space and the point in space represents a scenario; The sampled point finally contributes to a detailed scenario for A V . Monte Carlo error to identify challenging scenarios with a minimal number of samples. By doing this, datasets with large- scale scenarios is covered equivalently. However, their method still relies on natural distribution of naturalistic driving data. In contrast, [25] proposes the dynamic testing for A V . This approach highlights an unknown distribution of critical sce- narios, rather than a given distribution. It describes sampling as an optimization problem for space coverage considering a limited number of samples. The discrepancy of the samples in space is used as an optimization objective to cover the space uniformly, thus achieving a fast understanding of the sampling space. However, it is important to consider that with the scenario scale increases, it is difficult to obtain the real distribution of scenarios in high-dimensional space [31] [32] [33]. Without a clear understanding of the scenario distribution, it is usually very difficult to cover the critical subspaces [18] [34]. Besides, up to now, no method has been claimed to be well enough to cover the entire high-dimensional space. Thus, method using only few samples or trying to enumerate all samples to cover the space is hard to be realized. In addition, the intelligence (or capacity) of A V may change significantly as learning progresses. When the measured capability changes, the distribution of challenging scenarios will also change and then the testing scenarios base will be updated to deal with the enhancement of A V [18]. Therefore, we believe a completely different scheme than the above approaches and emphasize that life-long testing is necessary, as pointed out in [15]. To realize this, we further propose a new scheme to conduct multiple and continu- ous tests on A V . The goal is to continuously evaluate the intelligence of A V by finding more critical scenarios. The criticality of scenario is determined by whether A V smoothly and successfully passes it. When A V encounters a challenging event, its score decreases, also indicating that the scenario is critical to it. Thus the testing can be regarded as a continuous optimiza- tion process to minimize the score of A V . When the samples cover all the space, the testing stops and the A V is evaluated with the final score. Based on this, a loop, namely outer loop, is set where we continuously generate new samples, evaluate A V in them, and then use the evaluation results as prior knowledge to assist the next round until the termination condition is reached. Different from the previous method, we believe although it needs to be synchronized to quickly find critical scenarios and cover the sampling space, they are difficult to have both. But in life-long learning and testing, scenarios that A V has not been experienced will always be sampled and generated from the unknown sampling space. Further, it leads to the third question: Q3. ”How to cover the unknown space as much as possible in each round based on prior knowledge?” . Q2 and Q3 are two related but different queations. To answer Q3, we assume that the testing results of a point in space, i.e., a sample, can represent the testing results of other points in a sufficiently small subspace around it. Such a subspace can be called the representable subspace of the point. The representable subspaces of all selected points should maximize the coverage of unknown space to avoid waste of testing resources. However, in the case of multiple rounds of sampling, the known space may be complex and irregular (such as the space full with randomly generated points), resulting in the subspace represented by the new points not only easy to overlap with the known space, but also overlap with each other. Thus, we set another loop, namely inner loop, embed the outer loop mentioned above. We formulate it into a sub- optimization problem which minimizes the overlapping range of these representable subspaces by reasonably reallocating the positions of the points generated in each outer loop. The solution to sub-optimization problem is also helpful to quickly find the optimal answer to the optimization in outer loop. To solve the sub-optimization problem, we propose an adaptive sampling algorithm with a multi-rounds heuristic strategy. It assumes such a representative subspace as a multi- dimensional sphere, the center of the which is the point repre- senting this subspace. Thus, the sphere radius corresponds to the degree of criticality of the sample. The subspace occupied by this sphere is the coverage of the sample point. Further, in every inner loop, we put new sample spheres in space (generated in outer loop) and constantly move them to reduce the overlap of spheres. In order to calculate the moving direction of the sphere, we regard the sample as a sphere with repulsive force, and the sample sphere will be repelled by the surrounding sphere. We design the sphere to move away from other spheres along the direction of the resultant force. The movement continues until the end of the iteration.3 C. AV Evaluation and Testing Scenario Base Generation …… Para 1 Para 2 Score Testing times1 2 …… 3 Score Testing times1 2 …… 3 Final score Score Testing times1 2 …… 3 Final score Para 1 Para 2 Para 1 Para 2 …… …… …… ………… …… Loop 1 Loop 2 Loop n …… …… …… ………… …… Loop 1 Loop 2 Loop n Inner loop Evaluation  Sampling …… Optimization Multiple rounds Scoring AV Generating  scenarios Para 1 Para 2 Para 1 Para 2 Multiple steps B. Learning and Testing as A Continuous Optimization in Outer Loop A. A Heuristic Strategy for Replacing Samples in Inner Loop Known noncritical subspace Known critical subspace  New samples in each loop Noncritical samplesCritical samples Unknown subspace Fig. 2. Important processes in life-long learning and testing. In addition, another advantage of sphere assumption is that we can make a unified geometric interpretation of the previous papers more vividly. For example, standard Monte Carlo is equivalent to putting spheres of equal size into space randomly, which faces the clumping problem because the sample spheres overlap. For better explanation, the following architecture is as follows: Section II provides preliminaries for this paper. In Section III, the overview on life-long learning and testing will be given, along with the construction of the two proposed optimization problems. Section IV presents the details on the adaptive sampling while simulation results are given in Section V proving the superiority of the proposed scheme. II. P RELIMINARIES The basic testing process has three main steps: testing prepa- ration, scenario generation and A V evaluation. Among them, testing preparation includes the necessary testing requirements, testing metrics or functionalities selection, testing tasks design, etc. Next, the concepts and roles of scenarios generation and A V evaluation will be described respectively. A. Description of Scenario Generation Based on Parameter- ized Space Testing scenario is the main carrier of testing imple- mentation, in which the A V processes various tasks and accepts evaluation [35] [36] [37]. Specifically, a scenario refers to a segment in a temporal and spatial context. It reflects the interaction between A V and the surrounding en- vironment (mainly SVs) [6] [38] [24] [39]. Mathematically, testing scenario is defined by a series of vehicle behavior- related variables and the set of scenarios is defined as X = {xi = (xi,1, ..., xi,D)|x ∈ R, i= 1, 2...}, where xi is the co- ordinate of any point in space, D is the space dimension, and xi is i-th sample. In this paper, we assume that the important parameters of the scenario are all about the behavior of SVs. That is, we only focus on the interactions between A V and SVs rather than the influence from static environment. Based on this assumption, we construct SVs ′ driving behavior models, the parameters of which are used as the dimensions of the space, like [25]. It can be seen that the points in space are mapped to the de- tailed scenarios, and the spatial position coordinate of a point represent the variable values of the scenario corresponding to one sample. Besides, when two points in space are close enough, we assume that the difference between them can be ignored. That is, the two scenarios formed by them share the same criticality. We define the set of similar points around the point as the representative subspace of the point. Since the discussion of scenario similarity is not the focus of this article, this part will not be further discussed. Related research can be seen in [21] for your interests. This representative4 subspace can also be viewed as the covering area of a point. Supposing there must exist {εi,d|d = 1, ..., D} for all points in χ(xi) = {|x′ d − xi,d| ≤εi,d|d = 1, 2, ..., D} that share similar testing results, we define this area is the coverage range of point xi and the size of this area is V olume(χ(xi)), where i is the label for one sample. The subspace represented by the already sampled points X is defined as a known subspace. The repeated coverage of the known space will undoubtedly increase the test cost. While the remaining unexplored subspace is defined as an unknown subspace, in which the distribution of critical scenarios is still unknown. Therefore, it is inevitable to retain spatial knowledge by sampling points and covering unkonwn subspaces. B. Definition of Critical Scenario and AV Evaluation As mentioned above, though it is difficult to cover the entire space, we still need to find enough critical scenarios as soon as possible to evaluate A V . In this paper, the criticality of the scenario is related to the results whether A V could pass it. So a critical scenario is defined as the scenario that A V passes when it fails to complete the testing task. The testing metric, relating to capability of A V , is the standard to assess the completion of the task, which can be 0/1 binary or numerical value. For example, A V is expected to have driving intelligence that can improve traffic safety and efficiency. There are many metrics that can be used. We choose TTC to be a safety related metric in this paper since it’s popular and often-used in many studies. While deceleration value can be used as an efficiency related metric. If the metric is less than the preset threshold, it means that A V fails to complete the task [7]. We use a indicator function to express whether scenario x is challenging: I (x) = \u001a 1, if T T C≤ T T Cθ or a ≤ −|aθ| 0, otherwise (1) where T T Cθ and aθ are the thresholds. III. L IFE -LONG LEARNING AND TESTING A. The Pipeline of the Life-long Learning and Testing Fig.2 is the flow chart of life-long learning and testing. Testing requirements are used as input, including concerned testing metrics, designed infinite (or finite) testing tasks, etc. The output includes two parts: the final score of A V and the generated testing scenario library. The main skeleton of the process consists of two loops, outer and inner. The inner loop maximizes the coverage of the currently unknown subspace by sampling a fixed number of points. In the outer loop, the evaluation results and these sce- narios will be stored in the scenario base as space knowledge, and these past knowledges will be used as prior knowledge to assist the next inner loop, as shown in Fig.3. The importance of life long learning and testing is reflected in three aspects. 1. We can quickly learn the knowledge of the unknown space, so as to speed up finding more critical scenarios. 2. The idea of life long can be well compatible with A V whose ability can change. This type of A V includes Initialization:  New samples generation Requirements: metrics, tasks,... Calculation:  Spheres distance calculation Update:  Spheres rearrangement Evaluation:  Scenario Generation & AV testing Output Scenario base Outer loop Inner loop Update:  Known subspaces deployment Sub-optimization: Current unknown subspaces coverage Optimization: AV score Fig. 3. Pipeline of the proposed scheme. not only active learning ones, but also passive upgraded or modified ones. 3. In the spiral of continuous learning and testing, we can correctly assess the current intelligence level of A V . B. Testing as Optimization in Outer Loop Though it is difficult to find all critical samples and we do not know what the adequate sample size is, we still have to evaluate the A V as accurately as possible. To realize this, we define the score of an A V to be dependent on the scenarios it experiences, especially the scenarios that can make the A V perform poorly. The more critical scenarios the A V encounters, and the more poorly it performs, the lower the score of the A V is. From this, we define the score Score(X) = S − X I(x), x ∈ X (2) where S is a constant which represents the max of score. The constant can be set to the total number of generated scenarios. So the testing of A V is to minimize the score of A V by generating critical scenarios, which is equal to find the solution for the optimization problem min X Score(X) (3) However, since we cannot enumerate all the possible sce- narios, we define a reference indicator to help us decide when to stop testing. In this paper, we suggest using the proportion of space covered by the obtained samples as such an indicator. CRate(X) = V olume(χ(X)) Vspace (4) where V olume(χ(X)) is the volume of the subspaceSχ(xi). CRate(X) is dependent on the size of the subspace repre- sented by the samples and the number of the obtained samples. If the volume of the subspace represented by a sample is small, more samples are required to fill the space. We will elaborate on this in the simulations.5 From the above, we can infer that the entire mechanism aims to achieve two objectives: reducing the A V score and increasing the CRate(X). It is evident that accomplishing both goals is challenging, as highlighted in Q2 and Q3, which are related but distinct issues. Therefore, outer loop is proposed with the Score(X) as the primary optimization objective and CRate(X) as a constraint. The core of the outer loop is the optimization search through batch sampling. The evaluation of A V is expected to be a gradual process, where scenarios with poor A V performance are continuously identified and the Score(X) is reduced until the coverage stopping condition is met. The outer loop generates a specific number of samples at each iteration and evaluates the A V based on the samples of the inner loop outputs. The final A V score is the total score obtained after testing the scenarios generated in all loop rounds. Thus, the score function can be rewritten as, Score(X) = S − X r N(r)X i=1 I(xi) (5) where xi ∈ S r X(r). Then the optimization for outer loop is min X S − X r N(r)X i=1 I(xi) s.t.CRate(X) < CRateθ s.t.X = ( xi|i = 1, 2, ..., X r N(r) ) s.t.N(r), r <∞ (6) where CRateθ is the thershold for covering area and N(r) is the total number of samples in r-th round. Besides, we also need to constantly estimate and update the current distribution function in every round. For example, if we have evaluation results y = {yi|i = 1, ...,PN(j)}, the distribution can be assumed as a non-parameterized function as, fDr (x) = 1 hd P r N(r) X r N(r)X i=1 yiϕ(x − xi h ) (7) where Dr is the distribution after r-th testing, h > 0 is a pre-chosen bandwidth, and ϕ(·) is a special kernel function. Then we can use new distribution Dr instead of previous Dr−1to assist the r+1th testing. C. Sampling as Sub-optimization in Inner Loop The inner loop is embedded in the outer loop, which takes the knowledge learned by the outer loop as the input and outputs the newly sampled sample points in the round. Similarly, we describe unknown subspace covering problem into a constrained optimization problem. At the r-th outer loop, the inner loop receives newly generated samples X(r) as input. Since after the previous r-1 outer loops, the previous samples formulate the known subspace, our job is to maxi- mize the coverage of the reamining unknown subspace using X(r). We define this coverage as χukn(X(r)). Thus, the sub- optimization problem is as follows: max V olume(χukn(X(r))) s.t.εi d > 0, ∀i, d s.t.χ(xi) = {| x′ d − xi,d |≤ εi,d | d = 1, 2, ...D,xi ∈ X(r)}, ∀i s.t.X(r) ⊆ ∁Space   r−1[ j=1 χ(X(j))   (8) where \u0010Sr−1 j=1 χ(X(j)) \u0011 is the known subspace formed by samples generated in previous outer loops and their covering subspaces while ∁Space \u0010Sr−1 j=1 χ(X(j)) \u0011 is the complement of these subspaces, which is unknown subspace in current outer loop. The objective function is the size of the remaining unknown subspace area. The first condition constrains that the coverage range of a sample must be strictly greater than 0. The second condition limits the coverage of one sample. The third condition reveals that the newly generated samples should be in unknown subspace. The solution to Eq.8 is helpful to solve Known critical  subspace Known   non-critical  subspace Unknown  subspace Non-critical  sample point Critical  sample point (1) (2) (3) (1) (2) (3) (1) (2) (b.1) Overlap with known subspace (b.2) Violate the boundary limitation (b.3) Overlap each other  New point (a) Covering space in one inner loop (b) Different bad situations on covering space  Fig. 4. Illustration for different situations on covering space under the two- dimensional projection. Eq.6 because it can accelerate the convergence of the outer loop by covering more unknown subspace. Otherwise, more samples would be generated to cover the space. The reason is that it is difficult to fill a certain number of samples perfectly into the unknown subspace at one time considering the com- plex and irregular known subspace. Even if the arrangement is regular, the samples will inevitably overlap [40]. Obviously, we cannot directly solve this optimization prob- lem. We need to iteratively update the positions of the points to find the best sampling scheme, aiming at better covering the unknown subspace. To realize this, we also make the following assumptions and rewrite the problem. 1) Assumptions on the coverage of sample: First, we as- sume that εd for one sample point are unified. Otherwise, the coverage of the sample may be a non convex multidimensional polygon and it will bring difficulties to the rapid solution. Furthermore, the coverage of a sample point can be regarded as a d-dimensional sphere χ(xi) = {x ∥ x − xi |2≤ (Ri)2, Ri > 0}, ∀i (9)6 where Ri is the sphere radius of the i-th point. Thus, the known subspace can be seen as a combination of several spheres of different sizes, which may be overlapped, tangent, separated. We assume that the sphere radius depends on the criticality of the sample. When the sample is critical, the sphere radius is relatively small, indicating that its neighbor- hood needs to be paid more attention for further exploration. On the contrary, when the sample is not challenging, its sphere radius is relatively large. From this assumption, we expect to explore subspace with higher possibility to have critical points due to a limited testing resource. The process is like packing a set of spheres in box-like space. We will expand it in the next section. 2) Modification on the objective function: Refer to [25], we transform the maximization of the coverage volume of the unknown subspace into the maximization of the weighted dis- tance between samples for a quick solution. Define Dist(xi, ·) as the weighted distance between any two spheres, which can be written as Dist(xi, ·) = min {λmDist(xi, um), αjDist(xi, xj)}∀i, j, m (10) where λm and αj are the two weighted coefficients. Then the sub-optimization problem in inner loop is rewritten as, max x 1 I X i Dist(xi, ·) s.t.i ≤ I s.t.χ(xi) = {x ∥ x − xi |2≤ (Ri)2} s.t.χ(um) = {u || u − um |2≤ (Rm)2} s.t.xi ∈ X(r), um ∈   r−1[ j=1 χ(X(j))   (11) The core idea of this modification is to let new spheres move away from known subspace by maximizing the average dis- tance between them. When these new spheres do not overlap with known spheres or with each other, they will certainly cover more unknown subspace. Besides, the calculating of distance between two spheres is much more faster and easier than directly calculating the coverage to unknown subspaces. IV. T HE ADAPTIVE PROCESS OF PACKING SPHERES IN INNER LOOP We suggest a heuristic strategy that utilizes the concept of ′anthropomorphism′ to visualize the sampling process. Instead of describing sampling as the taking of sample point from a space full of them, we can view it as packing a certain number of spheres in an box-like space. That is, the sampling process can be reversed to place one sample sphere into the space. In this way, the new spheres put into the space may have several undesirable states, as shown in Fig.4: the new spheres overlap with each other, the new spheres cover part of the known space, and the new spheres reach over the boundary limit, etc. These states will affect the coverage results. To alleviate this, we first assume that the known sphere is immovable for all following iterations. Next, we assume the existence of a repulsive force on the sphere. Then a new sphere, subject to the repulsive force of the neighbors, will move in the opposite direction at each iteration and also actively repel the other newly placed spheres (see in Fig.5). A KD-tree is constrcuted to model the relationship between the sample and its neighbors [41] and is helpful to search the samples in neighborhood quickly. Repulsive force from critical point Repulsive force from noncritical point The direction of total force Current position of the new sphere Past position of the new sphere Neighborhood of the new sphere Fig. 5. Illustration on the moving process step by step under the two- dimensional projection. To better understand this repulsion, we define the factors influencing it are the properties of the spheres and the distance between their centers. The idea is improved by us being inspired by the elegant solution to the packing problem in [42] [43]. The property of sphere, also called repulsion coeffi- cients, is related to the criticality of the sample. For example, repulsion coefficients of known critical samples are relatively small, suggesting the possibility of many critical scenarios in the subspace vicinity. Conversely, non-critical samples have a relatively large repulsion coefficient. Also, the repulsion force between two spheres is proportional to the inverse of the square of the distance between their centers. The closer the spheres are, the greater the repulsion force. Spheres will not repel the new sphere if they are outside the preset range of the new one. Based on this, the repulsive force between a known sphere and a new sphere is defined as, − →F (xi, um) = µQ(um)q(xi) − − →dist(um, xi)2 (12) where Q(um) is the coefficient of the known sphere, q(xi) is the coefficient of new sphere, and − − →dist(um, xi) is the distance between the centres of two spheres. Similar, we define the force between new spheres is − →F (xi, xj) = µq(xj)q(xi) − − →dist(xj, xi)2 (13) . Besides, to prevent the sphere moving outside the boundary, the log barrier function is used to define the binding force subject to the d-th dimension, − → F′ d(xi) = −β log(Prjd(xxx(k) i ))− →1 d (14) where β is the weight used to balance the repulsive force and the binding force from the boundary.7 After k iterations in a inner loop, the position of the points xi is expressed as xxx(k) i . And at the next iteration, we can calculate the combined force as follows, − →F total(xxx(k+1) i ) = MX m=1 |F(xxx(k) i ,uuum)| − − →dist(xxx(k) i ,uuum) |− − →dist(xxx(k) j ,uuum)| + I−1X j=1 |F(xxx(k) i ,xxx(k) j )| − − →dist(xxx(k) i ,xxx(k) j ) |− − →dist(xxx(k) j ,xxx(k) j )| + DX d=1 −β log(Prjd(xxx(k) i ))− →1 d (15) Given the huge size of the number of samples, we only count the sphere with the neighboring spheres in the space. The reason is that it has less influence on the change of the direction of the sphere movement when the repulsive force is relatively small. Besides, traversing all the spheres will bring a very high computational cost. The principle of KD-tree is to divide the space into hyper-rectangular regions based on the positions of all the spheres and represent the sphere center coordinates as tree nodes. Then the neighboring samples of each sample can be quickly found by tree search. Note that there is a special case where the updated sphere may still overlap with the known subspace or other spheres when moving in the direction guided by the combined force. To address this, a probability function is used to determine whether to accept the new position at current iteration. If the new solution is worse than the previous iteration’s solution, the probability of accepting it decreases exponentially with the number of iterations. Conversely, if the new solution is better than the previous iteration’s solution, the probability of updating is 1. δk+1 = \u001a 1, if1 I P i Dist(x(k+1) i , ·) > 1 I P i Dist(x(k) i , ·), ∀i eψ(k), otherwise (16) where ψ(k) = −τ(k) · 1 I P i Dist(x(k) i , ·) − 1 I P i Dist(x(k+1) i , ·) [1 I P i Dist(x(k) i , ·) − 1 I P i Dist(x(k+1) i , ·)] (17) , and τ(k) is an increasing function. So the position of i-th point at k+1-th iteration in one inner loop is x(k+1) i =    x(k) i − tk+1 − →F total(x(k) i ) |− →F total(x(k) i )|, if pk+1 < δk+1 x(k) i , otherwise (18) where pk+1 is the value generated randomly in (0, 1). Thus, we provide the algorithm for the life-long learning and testing. V. S IMULATION RESULTS A. Simulation Settings As shown in the Fig.6, the static environment of the generated atom scenario is unidirectional two-lane test road, while the traffic participants are six vehicles in each of the two Algorithm 1: Life-long learning and testing Input: Behavior models; r = 0, N(r); D; Ri;Scoremax;CRateθ Testing metrics Output: Score(X); X; Space knowledeg 1 Outer loop: 2 Construct sampling space 3 while CRate < CRateθ do 4 Generate set X(r) randomly with N(r) new points 5 Run Inner Loop 6 Evaluate critical scenarios and Update Score(X) 7 Update space knowledge 8 r+ = 1 9 end Algorithm 2: Adaptive sampling strategy for inner loop Input: X(r);Space knowledge; D; Ri;K for total iteration steps; δ, n Output: Updated points X(r) 1 while k < Kdo 2 Calculate Dist(X(r), ·) 3 T empDist= 1 I Dist(X(r), ·) 4 Select n points randomly 5 for x(k) in selected n points do 6 Search neighbors 7 Calculate − →F total(xxx(k)) and direction 8 Calculate the next position for point 9 end 10 Calculate Dist(X(r), ·) 11 if 1 I Dist(X(r), ·) > T empDistthen 12 Update the new positions 13 T empDist= 1 I Dist(X(r), ·) 14 end 15 else 16 Update the new positions with δk 17 end 18 k+ = 1 19 end lanes, including five SVs and one tested A V . In this simulation, only the surrounding vehicles can perform lane changing. Referring to [25], we adopt the IDM, a model with a describable composition of parameters, to simulate car- following behavior of the SVs. IDM model has five main parameters: v0 is the free stream velocity of the vehicle, α is the maximum acceleration, s0 is the minimum distance in congested traffic, T is a constant safe time gap, and b is the ’comfortable deceleration’. In addition, we use MOBIL as the lane-changing behavior model, which has two main parameters: p and a. p stands for ’politeness’, indicating the degree of politeness of the surrounding vehicles to change lanes while ∆ath represents the threshold for the change of acceleration. The following are the representations of these8 2SV 3SV 4SV 5SV AV 1SV Fig. 6. Settings on the static environment. two models, respectively: IDM : ai = α  1 − \u0012vi v0 \u00134 −   s∗ \u0000 vi, ∆vi\u0001 si !2 . (19) MOBIL : ˜ac − ac| {z } SV +p \u0000 ˜an − an| {z } new follower + ˜ao − ao| {z } old follower \u0001 > ∆ath (20) where s∗ \u0000 vi, ∆vi\u0001 = s0 + vT + vi∆vi 2 √ ab (21) It is also necessary to point out that our approach is more concerned with the critical scenarios where SVs play reasonable and rational, i.e., these drivers do not aim to create accidents. In this paper, the A V is required to safely and efficiently finish car-following task while SV cuts in. Since we only test the performance of A V , it’s not necessary to model the whole characteristics of it. Instead, a collision avoidance model can be used as an alternative model to mimic the A V to keep a reasonable distance from the leading vehicle, if its parameter G is modified properly [44] [45]. Thus, the following speed of A V at the next moment is calculated as follows vAV (t + ∆t) = ( max \b 0, vAV (t) − amax · ∆t \t , L(t) < G min \b vAV max, vAV (t) + amax · ∆t \t , L(t) ≥ G (22) where desired G is set to [6, 12]m in this paper. B. The Construction of Sampling Space We consider that a scenario begins when the parameters of models update and ends when SV2 completes a lane change behavior. It is because SV2 is most likely to put A V into accidents. Besides, the parameters remain constant in one scenario otherwise the size of the sampling space increases rapidly as the simulation time grows. Based on the literature [46], the upper and lower bounds of the behavioral model parameters of one SV are set as shown in the TABLE.II. All the simulations are conducted in the CA Vsim platform. CA Vsim supports a wide range of vehicle behavior modeling and can output the testing results such as TTC in real time. C. Results on Adaptive Sampling in Inner Loop This section demonstrates the process of iterating and mov- ing the sample spheres in the 3-dimensional parameter space. It also examines the impact of two important parameters, the number of samples and the radius of the spheres. Finally, the algorithm with our strategy is compared to other classical algorithms to highlight its superiority. Fig.7 displays the movement of 100 new spheres among 200 known spheres throughout the inner loop. Three subspaces are selected and we deepen the color of the spheres in them. The dark red sphere continues to move during the iteration process and stabilizes after 160 iterations, indicating that the optimal position has been found. In Fig.8, we added the known subspace and compared the inner loop’s initial and final states. Based on Fig.7 and Fig.8, it can be analyzed that in order to cover more unknown space, the new spheres keep moving away from the known subspace due to the repulsive force. To achieve a balance between high coverage and speed in identifying critical samples, the strategy let spheres explore the unknown subspaces near the critical samples first. We can see that new spheres are moved away from non-critical known subspaces as much as possible and moved away from critical subspaces with a relatively small repulsive force. To analyze the impact of the number of samples and the radius of the spheres. One thousand samples are randomly generated to form a known subspace, with 60 of them desig- nated as known critical samples. Fig.9 illustrates the coverage comparison of different numbers of new samples in one inner loop. The results indicate that the higher the number of samples we generate, the smaller the average Euclidean distance. Under our strategy, increasing the number of samples can better cover the unknown space. However, if the number of samples is too high, the spheres will overlap heavily, resulting in duplicated coverage of the space. Fig.10 illustrates the change in the average Euclidean distance under different sphere radius settings. When the size of the sphere is larger, meaning that our tolerance to the dissimilarity of neighboring scenes in space is higher, the coverage of the same number of sampling points improves. To verify the coverage effect of the algorithms based on our strategy in the inner loop, we used the Monte Carlo-based ran- domized generation algorithm and the greedy-strategy-based algorithm as benchmarks. Monte Carlo algorithm is the most commonly used method for scenario generation. It generates sample points directly in the sampling space without relying on any prior information about the subspace. The greedy- strategy-based algorithm is a commonly used for optimization problems. The algorithm repeats the random generation of a certain samples step by step, calculates and compares the coverage metric, and selects the optimal solution of the step as the result. The algorithm stops when the required number of sample points is achieved. In this experiment, the sphere radius for the critical sphere is set to 0.04, while the sphere radius for the non-critical sphere is set to 0.06. A total of 2000 samples were randomly generated to form the known subspace, with 150 of them designated as known critical samples. TABLE.I presents the comparison9 Step 0 Step 20 Step 40 Step 80 Step 100 Step 120 Step 160 Step 200 Coverage curve in inner loop 0.00974 0.02562 0.03709 0.04707 0.05157 0.055850.057590.058060.05987 0 50 100 150 200 0.00 0.02 0.04 0.06Average of Euclidean Distance Interations Fig. 7. Moving spheres at different iterations in inner loop. TABLE I AVERAGE OF EUCLIDEAN DISTANCE New Samples 200 600 1000 1400 2000 Greedy −7.81 ∗ 10−4 −5.66 ∗ 10−4 −6.93 ∗ 10−4 −7.02 ∗ 10−4 −7.06 ∗ 10−4 Monte Carlo −1.49 ∗ 10−2 −1.59 ∗ 10−2 −1.61 ∗ 10−2 −1.64 ∗ 10−2 −1.70 ∗ 10−2 Ours 0.0134 0 .0115 0 .0826 0 .0059 0 .0034 TABLE II BOUNDS OF PARAMETERS . Symbol Lower Bound Upper Bound v0 25m/s 30m/s α 1m/s−2 5m/s−2 T 0.05s 2s b 0.1m/s−2 4m/s−2 s0 0.1m 3 p 0 1 ∆ath 0m/s−2 0.3m/s−2 results of different algorithms for generating different number of new samples. Our algorithm provides better coverage of the remaining unknown space, while random sampling yields the worst coverage results. Though the greedy strategy can ensure obtaining a locally superior solution in each iteration, it may not lead to a globally superior solution. D. Results in Outer Loop During the simulation of the outer loop, each loop generates 200 new samples, evaluates the criticality of each scenario, and assigns A V testing score. In this experiment, we set the max Score S is 4000 which corresponds to the total number of generated scenarios. To demonstrate the validity of this scheme, we compare it with standard Monte Carlo (SMC) [47] [48] [49]and Random Quasi Monte Carlo (QMC) [50]] [51]. But unlike our scheme, none of these methods are itera- tive. Therefore when presenting the results, we will utilize these methods to generate different numbers of samples in- dependently and ensure that the number of these samples is10 (a) Step 0 (b) Step 200 Fig. 8. Illustrations on the start and end of the spheres packing in inner loop. After 200 iterations in inner loop, it can be vividly seen that the new samples (red spheres) are better explored in unknown subspace, from aggregated to dispersed. 0 60 120 180 0.02 0.03 0.04 0.05Average of Euclidean Distance Iterations  Samples=50  Samples=100  Samples=200  Samples=300  Samples=500  Samples=800  Samples=1000 Fig. 9. Comparison results on simulating different new samples in one inner loop. 0 60 120 180 −0.12 −0.06 0.00 0.06 Average of Euclidean Distance Iterations  r1,r2=0.005,0.01  r1,r2=0.01,0.02  r1,r2=0.02,0.04  r1,r2=0.04,0.08  r1,r2=0.05,0.1 Fig. 10. Comparison results on simulating spheres with different sizes in one inner loop. consistent with the total number of samples generated by our scheme over progressive rounds. Additionally, we conduct another comparison experiment where the space is covered with spheres of the same volume, i.e., we do not distinguish between critical and non-critical samples in terms of sphere radius. The results on A V score are presented in Fig.11. It is evident that until the coverage ratio stabilizes, the SMC and QMC methods sample only a few critical samples. This outcome is 0 5 10 15 20 3600 3700 3800 3900 4000 CoveringRate:0.969 Score for AV Iterations   Ours using different balls  Ours using same balls  Monte Carlo  Quasi Monte Carlo CoveringRate:0.992 CoveringRate:0.996 CoveringRate:0.989 Fig. 11. Results on scoring A V . We generate the same number of samples for the benchmarks as the total number of samples in our scheme after each iteration, e.g., we have generated a total of 1600 samples in 8 iterations and we also generate 1600 samples using SMC and QMC respectively. 0 5 10 15 20 0.00 0.25 0.50 0.75 1.00Covering Rate Iterations  Ours using equal-size spheres  Ours using unequal-size spheres  Monte Carlo  Quasi Monte Carlo Loop termination Fig. 12. Results on covering rate in outer loop. consistent with previous research [25]. In contrast, our method encounters a growing number of critical samples, leading to a significant decrease in A V score. The Fig.11 displays vertical line segments that indicate that the coverage metric approaches 1, meaning the space is almost completely covered.11 TABLE III THE NOMENCLATURE LIST SYMBOLS IN THE SCHEME MEANINGS X The set of scenarios xi The i-th scenario xi,d The d-th variable of i-th scenario εi,d A constant that constraints the coverage range of i-th sample on the d-th dimension χ(xi) The covering subspace of xi T T Cθ Threshold for T T C aθ Threshold for a I (x) Indicator function to show the criticality of scenario x Score(X) The evaluation result of A V passing scenarios X CRate(X) The proportion of space covered by the obtained samples CRateθ Thershold for covering area r Rouond number for outer loop N(r) Total number of samples in r-th round yi Evaluation of A V in xi Dr Distribution after r-th testing h Pre-chosen bandwidth ϕ(·) Special kernel function Ri Sphere radius of the i-th point Dist(xi, ·) Weighted distance between any two spheres λm 1st Weighted coefficient for Dist αj 2nd Weighted coefficient for Dist um m-th known scenario− →F The force between two spheres µ Coefficient for the force Q(um) Repulsion coefficient of the known sphere q(xi) Repulsion coefficient of the new sphere− − →dist Distance between the centres of two spheres β Weight used to balance the repulsive force and the binding force from the boundary k Step number in inner loop− →F total The combined force M Number of neighbors from known samples I Number of all new samples in one inner loop δk Probability of updating samples in inner loop α(k) An increasing function for controlling the update probability pk Randomly generated value n Number of samples randomly chosen to move in one outer loop in ALGORITHM.2 SYMBOLS IN SIMULATIONS MEANINGS v0 Free stream velocity of the vehicle α The maximum acceleration T A constant safe time gap b Comfortable deceleration s0 The minimum distance in congested traffic p Parameter that represents’politeness’ ∆ath Threshold for the change of acceleration G Desired following distance for A V The method that uses different volumes of sample spheres converges slower but find more critical samples compared to the method that equal-sized spheres. Furthermore, Fig.12 illustrates that SMC has low coverage and also struggles to quickly sample critical scenarios. On the other hand, QMC has exceptional coverage but performs poorly in quickly finding critical scenarios. Combining Fig.11 and Fig.12, our approach can obtain more critical scenarios and thus test the A Vs more accurately, although it sacrifices some coverage. Besides, it is important to note that we only use one kind of coverage metric as a termination condition for the outer loop. We expect further exploration can be conducted to use a better coverage metric. VI. C ONCLUSION This paper proposes a novel paradigm to test and improve the intelligence of A V . Most previous studies aim to minimize the expectation of the failing frequency of an A V in various scenarios. As a result, an A V may be taken as ”intelligent enough” even if it cannot successfully pass a certain kind of scenarios which are rare to occur in practice. Differently, we aim to fathomed all the possible scenarios that could be sampled to ensure an A V works well in all such scenarios. Because the sampled scenario space is continuous, we cannot exhaustically enumerate all the scenarios. Because the capability of an A V may vary significantly in different scenarios, we cannot evaluate the intelligence of an A V by just sampling a limited number of the scenarios as representatives and test the A V just in these few scenarios. Therefore, we proposed the life-long learning and test scheme to continously test and (possibly) upgrate the intelligence of A Vs. This scheme is indeed an implementation of our idea of building general artificial intel- ligent systems which is called parallel learning [10] [52] [11].12 The challenging scenario sampling strategy proposed in this paper prescibes the intelligent system to further improve itself; while this improvement also leads to new sampling procedures and finally forms a closed-loop spiral ascent of intelligent systems. Constrained by the length limit, there are several issues not discussed in details in this paper. In future, we will further investigate other metric of sampling space coverage. REFERENCES [1] F.-Y . Wang and C. Lv, “Foundation vehicles: From foundation intelli- gence to foundation transportation for future mobility,” IEEE Transac- tions on Intelligent Vehicles , vol. 8, no. 10, pp. 4287–4291, 2023. [2] S. Teng, X. Hu, P. Deng, B. Li, Y . Li, Y . Ai, D. Yang, L. Li, Z. Xuanyuan, F. Zhu, and L. Chen, “Motion planning for autonomous driving: The state of the art and future perspectives,” IEEE Transactions on Intelligent Vehicles, vol. 8, no. 6, pp. 3692–3711, 2023. [3] H. Yu, X. Liu, Y . Tian, Y . Wang, C. Gou, and F.-Y . Wang, “Sora-based parallel vision for smart sensing of intelligent vehicles: From foundation models to foundation intelligence,” IEEE Transactions on Intelligent Vehicles, pp. 1–4, 2024. [4] C. Chang, J. Zhang, K. Zhang, W. Zhong, X. Peng, S. Li, and L. Li, “Bev-v2x: cooperative birds-eye-view fusion and grid occupancy prediction via v2x-based data sharing,” IEEE Transactions on Intelligent Vehicles, 2023. [5] K. Zhang, C. Chang, W. Zhong, S. Li, Z. Li, and L. Li, “A systematic solution of human driving behavior modeling and simulation for auto- mated vehicle studies,” IEEE transactions on intelligent transportation systems, vol. 23, no. 11, pp. 21 944–21 958, 2022. [6] M. Winkelmann, C. Vasconi, and S. M ¨uller, “Vectorized scenario description and motion prediction for scenario-based testing,” arXiv preprint arXiv:2302.01161, 2023. [7] L. Li, W.-L. Huang, Y . Liu, N.-N. Zheng, and F.-Y . Wang, “Intelligence testing for autonomous vehicles: A new approach,” IEEE Transactions on Intelligent Vehicles, vol. 1, no. 2, pp. 158–166, 2016. [8] J. Ge, J. Zhang, Y . Zhang, D. Yao, Z. Zhang, and R. Zhou, “Autonomous vehicles testing considering utility-based operable tasks,” Tsinghua Science and Technology, vol. 28, no. 5, pp. 965–975, 2023. [9] Z. Yi, Y . Dan-ya, L. Li, P. Hua-xin, Y . Song, and G. Jing-wei, “Technologies and applications for intelligent vehicle-infrastructure co- operation systems,” Journal of Transportation Systems Engineering and Information Technology, vol. 21, no. 5, p. 40, 2021. [10] L. Li, Y . Lin, N. Zheng, and F.-Y . Wang, “Parallel learning: a perspective and a framework,”IEEE/CAA Journal of Automatica Sinica, vol. 4, no. 3, pp. 389–395, 2017. [11] L. Li, X. Wang, K. Wang, Y . Lin, J. Xin, L. Chen, L. Xu, B. Tian, Y . Ai, J. Wang et al. , “Parallel testing of vehicle intelligence via virtual-real interaction,” Science robotics, vol. 4, no. 28, p. eaaw4106, 2019. [12] G. Wang, J. Hu, Z. Li, and L. Li, “Harmonious lane changing via deep reinforcement learning,” IEEE Transactions on Intelligent Transporta- tion Systems, vol. 23, no. 5, pp. 4642–4650, 2021. [13] W. Ding, L. Zhang, J. Chen, and S. Shen, “Epsilon: An efficient planning system for automated vehicles in highly interactive environments,” IEEE Transactions on Robotics , vol. 38, no. 2, pp. 1118–1138, 2022. [14] S. Teng, L. Li, Y . Li, X. Hu, L. Li, Y . Ai, and L. Chen, “Fusionplanner: A multi-task motion planner for mining trucks via multi-sensor fusion,” Mechanical Systems and Signal Processing , vol. 208, p. 111051, 2024. [Online]. Available: https://www.sciencedirect. com/science/article/pii/S0888327023009597 [15] L. Li, N. Zheng, and F.-Y . Wang, “A theoretical foundation of in- telligence testing and its application for intelligent vehicles,” IEEE Transactions on Intelligent Transportation Systems , vol. 22, no. 10, pp. 6297–6306, 2020. [16] S. Feng, Y . Feng, C. Yu, Y . Zhang, and H. X. Liu, “Testing sce- nario library generation for connected and automated vehicles, part i: Methodology,”IEEE Transactions on Intelligent Transportation Systems, vol. 22, no. 3, pp. 1573–1582, 2020. [17] S. Feng, X. Yan, H. Sun, Y . Feng, and H. X. Liu, “Intelligent driving in- telligence test for autonomous vehicles with naturalistic and adversarial environment,” Nature communications, vol. 12, no. 1, p. 748, 2021. [18] L. Li, Y .-L. Lin, N.-N. Zheng, F.-Y . Wang, Y . Liu, D. Cao, K. Wang, and W.-L. Huang, “Artificial intelligence test: A case study of intelligent vehicles,” Artificial Intelligence Review , vol. 50, pp. 441–465, 2018. [19] J. Ge, J. Zhang, C. Chang, Y . Zhang, D. Yao, and L. Li, “Task- driven controllable scenario generation framework based on aog,” IEEE Transactions on Intelligent Transportation Systems , pp. 1–14, 2024. [20] R. Song, X. Li, X. Zhao, M. Liu, J. Zhou, and F.-Y . Wang, “Identifying critical test scenarios for lane keeping assistance system using analytic hierarchy process and hierarchical clustering,” IEEE Transactions on Intelligent Vehicles, vol. 8, no. 10, pp. 4370–4380, 2023. [21] C. Chang, D. Cao, L. Chen, K. Su, K. Su, Y . Su, F.-Y . Wang, J. Wang, P. Wang, J. Wei et al., “Metascenario: A framework for driving scenario data description, storage and indexing,” IEEE Transactions on Intelligent Vehicles, vol. 8, no. 2, pp. 1156–1175, 2022. [22] J. Hu, S. Sun, J. Lai, S. Wang, Z. Chen, and T. Liu, “Cacc simulation platform designed for urban scenes,” IEEE Transactions on Intelligent Vehicles, vol. 8, no. 4, pp. 2857–2874, 2023. [23] A. Corso, P. Du, K. Driggs-Campbell, and M. J. Kochenderfer, “Adap- tive stress testing with reward augmentation for autonomous vehicle validatio,” in 2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE, 2019, pp. 163–168. [24] M. Koren, A. Nassar, and M. J. Kochenderfer, “Finding failures in high-fidelity simulation using adaptive stress testing and the backward algorithm,” in 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) . IEEE, 2021, pp. 5944–5949. [25] J. Ge, J. Zhang, C. Chang, Y . Zhang, D. Yao, Y . Tian, and L. Li, “Dynamic testing for autonomous vehicles using random quasi monte carlo,” IEEE Transactions on Intelligent Vehicles , pp. 1–13, 2024. [26] W. Ding, C. Xu, M. Arief, H. Lin, B. Li, and D. Zhao, “A survey on safety-critical driving scenario generation—a methodological perspec- tive,” IEEE Transactions on Intelligent Transportation Systems , 2023. [27] D. Zhao, H. Lam, H. Peng, S. Bao, D. J. LeBlanc, K. Nobukawa, and C. S. Pan, “Accelerated evaluation of automated vehicles safety in lane-change scenarios based on importance sampling techniques,” IEEE transactions on intelligent transportation systems , vol. 18, no. 3, pp. 595–607, 2016. [28] W. Ding, B. Chen, B. Li, K. J. Eun, and D. Zhao, “Multimodal safety- critical scenarios generation for decision-making algorithms evaluation,” IEEE Robotics and Automation Letters , vol. 6, no. 2, pp. 1551–1558, 2021. [29] R. Chen, M. Arief, W. Zhang, and D. Zhao, “How to evaluate proving grounds for self-driving? a quantitative approach,” IEEE Transactions on Intelligent Transportation Systems , vol. 22, no. 9, pp. 5737–5748, 2020. [30] S. Li, J. Yang, H. He, Y . Zhang, J. Hu, and S. Feng, “Few-shot scenario testing for autonomous vehicles based on neighborhood coverage and similarity,” arXiv preprint arXiv:2402.01795 , 2024. [31] Y . Wang, Y . Ren, S. Elliott, and W. Zhang, “Enabling courteous vehicle interactions through game-based and dynamics-aware intent inference,” IEEE Transactions on Intelligent Vehicles , vol. 5, no. 2, pp. 217–228, 2019. [32] Y . Xia, M. Geng, Y . Chen, S. Sun, C. Liao, Z. Zhu, Z. Li, W. Y . Ochieng, P. Angeloudis, M. Elhajj et al., “Understanding common human driving semantics for autonomous vehicles,” Patterns, 2023. [33] Z. Zhu and H. Zhao, “Learning autonomous control policy for inter- section navigation with pedestrian interaction,” IEEE Transactions on Intelligent Vehicles, 2023. [34] E. Lamb, “Two-hundred-terabyte maths proof is largest ever,” Nature, vol. 534, no. 7605, 2016. [35] M. Althoff and S. Lutz, “Automatic generation of safety-critical test scenarios for collision avoidance of road vehicles,” in 2018 IEEE Intelligent Vehicles Symposium (IV) . IEEE, 2018, pp. 1326–1333. [36] A. Tenbrock, A. K ¨onig, T. Keutgens, and H. Weber, “The conscend dataset: Concrete scenarios from the highd dataset according to alks regulation unece r157 in openx,” in 2021 IEEE Intelligent Vehicles Symposium Workshops (IV Workshops). IEEE, 2021, pp. 174–181. [37] Y . Lin and M. Althoff, “Rule-compliant trajectory repairing using satis- fiability modulo theories,” in 2022 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2022, pp. 449–456. [38] A. Li, S. Chen, L. Sun, N. Zheng, M. Tomizuka, and W. Zhan, “Scegene: Bio-inspired traffic scenario generation for autonomous driving testing,” IEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 9, pp. 14 859–14 874, 2021. [39] J. Ge, J. Zhang, Y . Zhang, D. Yao, Z. Zhang, and R. Zhou, “Autonomous vehicles testing considering utility-based operable tasks,” Tsinghua Science and Technology, vol. 28, no. 5, pp. 965–975, 2023. [40] A. Buchholz and N. Chopin, “Improving approximate bayesian compu- tation via quasi-monte carlo,” Journal of Computational and Graphical Statistics, vol. 28, no. 1, pp. 205–219, 2019.13 [41] Z. Guo, H. Liu, H. Shi, F. Li, X. Guo, and B. Cheng, “Kd-tree-based euclidean clustering for tomographic sar point cloud extraction and segmentation,” IEEE Geoscience and Remote Sensing Letters , vol. 20, pp. 1–5, 2023. [42] L. Yu and W. Huang, “Two strategies for solving the equal sphere packing problem,” Journal of Software , vol. 23, no. 9, pp. 2285–2296, 2012. [43] W. Chen and Z. Chen, “Introduction: Special issue dedicated to the mem- ory of professor wenqi huang,” Journal of Combinatorial Optimization , vol. 32, pp. 331–340, 2016. [44] Y . Meng, L. Li, F.-Y . Wang, K. Li, and Z. Li, “Analysis of cooperative driving strategies for nonsignalized intersections,” IEEE Transactions on Vehicular Technology, vol. 67, no. 4, pp. 2900–2911, 2017. [45] E. Kometani and T. Sasaki, “On the stability of traffic flow (report-i),” Journal of the Operations Research Society of Japan , vol. 2, no. 1, pp. 11–26, 1958. [46] A. Alhariqi, Z. Gu, and M. Saberi, “Calibration of the intelligent driver model (idm) with adaptive parameters for mixed autonomy traffic using experimental trajectory data,” Transportmetrica B: transport dynamics , vol. 10, no. 1, pp. 421–440, 2022. [47] A. Broadhurst, S. Baker, and T. Kanade, “Monte carlo road safety reasoning,” in IEEE Proceedings. Intelligent Vehicles Symposium, 2005. IEEE, 2005, pp. 319–324. [48] A. Eidehall and L. Petersson, “Threat assessment for general road scenes using monte carlo sampling,” in 2006 IEEE Intelligent Transportation Systems Conference. IEEE, 2006, pp. 1173–1178. [49] ——, “Statistical threat assessment for general road scenes using monte carlo sampling,” IEEE Transactions on intelligent transportation sys- tems, vol. 9, no. 1, pp. 137–147, 2008. [50] B. Feil, S. Kucherenko, and N. Shah, “Comparison of monte carlo and quasi monte carlo sampling methods in high dimensional model representation,” in 2009 first international conference on advances in system simulation. IEEE, 2009, pp. 12–17. [51] Y .-D. Zhou, K.-T. Fang, and J.-H. Ning, “Mixture discrepancy for quasi- random point sets,” Journal of Complexity, vol. 29, no. 3-4, pp. 283–301, 2013. [52] L. Li, Y . Lin, Y . Wang, and F.-Y . Wang, “Simulation driven ai: From artificial to actual and vice versa,” IEEE Intelligent Systems , vol. 38, no. 1, pp. 3–8, 2023. Jingwei Ge is currently pursuing the Ph.D. degree with the Department of Automation, Tsinghua Uni- versity, China. His current research interests focus on intelligent transportation systems, intelligence testing, and autonomous vehicles testing. Pengbo Wang is currently pursuing the Ph.D. de- gree with the Department of Automation, Tsinghua University, China. His current research interests fo- cus on intelligent transportation systems, intelligent vehicles and deep reinforcement learning. Cheng Chang received the B.S. degree from Ts- inghua University, Beijing, China, in 2021, where he is currently pursuing the Ph.D. degree with the Department of Automation. His current research interests include intelligent transportation systems, intelligent vehicles and machine learning. Yi Zhang received the B.S. and M.S. degrees from Tsinghua University, China, in 1986 and 1988, re- spectively, and the Ph.D. degree from the Univer- sity of Strathclyde, U.K., in 1995. He is currently a Professor in control science and engineering at Tsinghua University, with his current research inter- ests focusing on intelligent transportation systems. His active research areas include intelligent vehicle- infrastructure cooperative systems, analysis of urban transportation systems, urban road network man- agement, traffic data fusion and dissemination, and urban traffic control and management. His research fields also cover the ad- vanced control theory and applications, advanced detection and measurement, and systems engineering. Danya Yao received the B.S., M.S., and Ph.D. degrees from Tsinghua University, Beijing, China, in 1988, 1990, and 1994, respectively. He is currently a Full Professor with the Department of Automation, Tsinghua University. His research interests include intelligent detection technology, system engineering, mixed traffic flow theory, and intelligent transporta- tion systems. Li Li (Fellow, IEEE) is currently a Professor with the Department of Automation, Tsinghua University, Beijing, China, working in the fields of artificial in- telligence, intelligent control and sensing, intelligent transportation systems, and intelligent vehicles. He has published over 170 SCI-indexed international journal articles and over 70 international confer- ence papers as a first/corresponding author. He is a member of the Editorial Advisory Board for the Transportation Research Part C: Emerging Technolo- gies, and a member of Acta Automatica Sinica. He also serves as an Associate Editor for IEEE Transactions on Intelligent Transportation Systems and IEEE Transactions on Intelligent Vehicles.",
      "references": [
        "Foundation vehicles: From foundation intelligence to foundation transportation for future mobility",
        "Motion planning for autonomous driving: The state of the art and future perspectives",
        "Sora-based parallel vision for smart sensing of intelligent vehicles: From foundation models to foundation intelligence",
        "Bev-v2x: cooperative birds-eye-view fusion and grid occupancy prediction via v2x-based data sharing",
        "A systematic solution of human driving behavior modeling and simulation for automated vehicle studies",
        "Vectorized scenario description and motion prediction for scenario-based testing",
        "Intelligence testing for autonomous vehicles: A new approach",
        "Autonomous vehicles testing considering utility-based operable tasks",
        "Technologies and applications for intelligent vehicle-infrastructure co-operation systems",
        "Parallel learning: a perspective and a framework",
        "Parallel testing of vehicle intelligence via virtual-real interaction",
        "Harmonious lane changing via deep reinforcement learning",
        "Epsilon: An efficient planning system for automated vehicles in highly interactive environments",
        "Fusionplanner: A multi-task motion planner for mining trucks via multi-sensor fusion",
        "A theoretical foundation of intelligence testing and its application for intelligent vehicles",
        "Testing scenario library generation for connected and automated vehicles, part i: Methodology",
        "Intelligent driving intelligence test for autonomous vehicles with naturalistic and adversarial environment",
        "Artificial intelligence test: A case study of intelligent vehicles",
        "Task-driven controllable scenario generation framework based on aog",
        "Identifying critical test scenarios for lane keeping assistance system using analytic hierarchy process and hierarchical clustering",
        "Metascenario: A framework for driving scenario data description, storage and indexing",
        "Cacc simulation platform designed for urban scenes",
        "Adaptive stress testing with reward augmentation for autonomous vehicle validatio",
        "Finding failures in high-fidelity simulation using adaptive stress testing and the backward algorithm",
        "Dynamic testing for autonomous vehicles using random quasi monte carlo",
        "A survey on safety-critical driving scenario generation—a methodological perspective",
        "Accelerated evaluation of automated vehicles safety in lane-change scenarios based on importance sampling techniques",
        "Multimodal safety-critical scenarios generation for decision-making algorithms evaluation",
        "How to evaluate proving grounds for self-driving? a quantitative approach",
        "Few-shot scenario testing for autonomous vehicles based on neighborhood coverage and similarity",
        "Enabling courteous vehicle interactions through game-based and dynamics-aware intent inference",
        "Understanding common human driving semantics for autonomous vehicles",
        "Learning autonomous control policy for intersection navigation with pedestrian interaction",
        "Two-hundred-terabyte maths proof is largest ever",
        "Automatic generation of safety-critical test scenarios for collision avoidance of road vehicles",
        "The conscend dataset: Concrete scenarios from the highd dataset according to alks regulation unece r157 in openx",
        "Rule-compliant trajectory repairing using satisfiability modulo theories",
        "Scegene: Bio-inspired traffic scenario generation for autonomous driving testing",
        "Improving approximate bayesian computation via quasi-monte carlo",
        "Kd-tree-based euclidean clustering for tomographic sar point cloud extraction and segmentation",
        "Two strategies for solving the equal sphere packing problem",
        "Introduction: Special issue dedicated to the memory of professor wenqi huang",
        "Analysis of cooperative driving strategies for nonsignalized intersections",
        "On the stability of traffic flow (report-i)",
        "Calibration of the intelligent driver model (idm) with adaptive parameters for mixed autonomy traffic using experimental trajectory data",
        "Monte carlo road safety reasoning",
        "Threat assessment for general road scenes using monte carlo sampling",
        "Statistical threat assessment for general road scenes using monte carlo sampling",
        "Comparison of monte carlo and quasi monte carlo sampling methods in high dimensional model representation",
        "Mixture discrepancy for quasi-random point sets",
        "Simulation driven ai: From artificial to actual and vice versa"
      ],
      "meta_data": {
        "arxiv_id": "2405.00696v1",
        "authors": [
          "Jingwei Ge",
          "Pengbo Wang",
          "Cheng Chang",
          "Yi Zhang",
          "Danya Yao",
          "Li Li"
        ],
        "published_date": "2024-03-28T13:22:48Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper addresses the challenge of efficiently sampling critical testing scenarios for Automated Vehicles (AVs) due to the lack of prior knowledge of critical scenario distribution. It formulates AV intelligence testing as a continuous optimization process, proposing a novel bi-level loop scheme for life-long learning and testing. The main contribution is an adaptive scenario sampling strategy that iteratively generates and evaluates potential critical scenarios, leading to faster and more accurate evaluation of AVs by identifying a higher number of critical scenarios.",
        "methodology": "The proposed methodology involves a bi-level loop structure. The outer loop continuously learns space knowledge from evaluated scenarios and samples new ones, while minimizing the AV's score. The inner loop, embedded within the outer loop, optimizes the coverage of unknown subspace by reallocating the positions of newly generated samples. This is achieved by assuming sample coverage as multi-dimensional spheres whose radii depend on scenario criticality. A multi-rounds heuristic strategy is used, treating spheres with repulsive forces that move away from known and other new spheres to maximize weighted distances. A KD-tree is employed for efficient neighbor searching, and a probability function determines the acceptance of new sphere positions during iteration. The AV evaluation uses TTC and deceleration as criticality metrics.",
        "experimental_setup": "The simulation environment is a unidirectional two-lane test road in the CAVsim platform, involving six vehicles (one AV and five SVs). Surrounding Vehicles (SVs) behavior is modeled using the Intelligent Driver Model (IDM) for car-following (parameters: v0, α, s0, T, b) and MOBIL for lane-changing (parameters: p, ∆ath). The Automated Vehicle (AV) behavior is mimicked by a collision avoidance model with a desired following distance parameter G. Criticality is defined by Time-To-Collision (TTC) and deceleration thresholds. The proposed adaptive sampling strategy in the inner loop is compared against Monte Carlo-based randomized generation and a greedy-strategy-based algorithm. The overall life-long learning and testing scheme in the outer loop is benchmarked against Standard Monte Carlo (SMC) and Random Quasi Monte Carlo (QMC) methods, evaluating AV score and space coverage rate.",
        "limitations": "The paper makes several assumptions, including: unified εd for sample points to simplify coverage as d-dimensional spheres, the sphere radius's dependence on scenario criticality (small for critical, large for non-critical), and the immovability of known spheres during inner loop iterations. Practical challenges include the difficulty in obtaining real scenario distributions in high-dimensional spaces and the inevitable overlap of samples when packing spheres. Acknowledged trade-offs include sacrificing some space coverage to find more critical scenarios. The study also notes that only one type of coverage metric is used as a termination condition for the outer loop.",
        "future_research_directions": "Future research directions include further investigation into other metrics for sampling space coverage to potentially improve the termination conditions of the outer loop. The authors also suggest that this scheme serves as an implementation of their broader idea of building general artificial intelligent systems through 'parallel learning', implying potential extensions within that framework.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Reasoning Implicit Sentiment with Chain-of-Thought Prompting",
      "full_text": "arXiv:2304.14815v1  [hep-lat]  28 Apr 2023 Preprint number: KYUSHU-HET-260, OU-HET-1185, YITP-23-5 8 Magnetic operators in 2D compact scalar ﬁeld theories on the lattice Motokazu Abe 1, Okuto Morikawa 2, Soma Onoda 1, Hiroshi Suzuki 1, and Yuya Tanizaki3 1Department of Physics, Kyushu University, 744 Motooka, Nis hi-ku, Fukuoka 819-0395, Japan 2Department of Physics, Osaka University, Toyonaka, Osaka 5 60-0043, Japan 3Yukawa Institute for Theoretical Physics, Kyoto Universit y, Kyoto 606-8502, Japan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . In lattice compact gauge theories, we must impose the admissibility co ndition to have well- deﬁned topological sectors. The admissibility condition, however, u sually forbids the presence of magnetic operators, and it is not so trivial if one can study the mo nopole physics depending on the topological term, such as the Witten eﬀect, on the lattice. I n this paper, we address this question in the case of 2D compact scalars as it would be one of th e simplest examples having analogues of the monopole and the topological term. To deﬁn e the magnetic operator, we propose the “excision method,” which consists of excising lattice lin ks (or bonds) in an appropriate region containing the monopole and deﬁning the dual lat tice in a particular way. The size of the excised region is O(1) in lattice units so that the monopole becomes point-like in the continuum limit. We give the lattice derivation of the ’t Hooft anom alies between the electric and magnetic symmetries and also derive the higher-group- like structure related to the Witten eﬀect. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Subject Index B01, B02, B06, B31 1 typeset using PTPTEX.cls1 Introduction When studying quantum ﬁeld theories (QFTs), we extract their dyn amics from the corre- lation functions of local operators. In the path integral formulat ion, we can construct various operators from the fundamental variables of path integral, and w e refer to them as electric operators. Interestingly, this is not the whole story, and we can in troduce other kinds of local operators as the defect of the path integral especially when the t arget space enjoys nontrivial topology [ 1]. By their nature, such operators are called defect operators or magnetic opera- tors. In the perturbative regime of QFTs, magnetic operators ar e typically quite heavy and do not aﬀect the dynamics at all, but they can play a signiﬁcant role in t he nonperturbative dynamics. Furthermore, the spectrum of electric and magnetic op erators are constrained by the generalization of Dirac quantization condition [ 2, 3], and it is now thought of as a part of the deﬁning data of QFTs [ 4]. Lattice regularization provides the rigorous foundation for study ing nonperturbative aspects of QFTs, and, moreover, lattice discretization of the spa cetime gives a natural setup to introduce the magnetic operators. On the other hand, the naiv e lattice regularization spoils the notion of the continuity of ﬁelds, which is essential ingredie nts to deﬁne the mag- netic operators in the continuum formulation. To reinstate the top ological structure, we can impose the “admissibility condition” on the lattice ﬁeld conﬁgurations [ 5]. In this paper, we shall construct magnetic operators on the lattice ﬁeld theories with admissibility con- straints and discuss their properties. To concretely present our ideas, we mainly focus on the two-dimensional (2D) compact bosons. 2D compact boson φ(x) has the periodicity, φ(x) ∼ φ(x) + 2 π, and this is analogous to the gauge redundancy. Any physical operators should respect this id entiﬁcation, and thus eiφ(x) and ∂µφ(x) are examples of physical operators while φ(x) itself is unphysical. This allows us to introduce the branch-cut singularity of φ(x) such as φ(x) = [1 /(2i)] ln[(z − z∗)/(¯z − ¯z∗)] + (smooth) with z = x1 + ix2, and this means that we introduce the magnetic operator M(x∗). The magnetic operator M(x∗) can also be deﬁned by imposing the boundary condition so that φ(x) has the winding number 1 around x∗, and this is an analogue of the ’t Hooft loop in the 4D gauge theories. In Sect. 2, we deﬁne the lattice counterpart of M(x∗) when the lattice conﬁgurations satisfy the admissibility condition by excising a ﬁ nite region from the lattice (“excision method”). This theory has the U(1)(e) × U(1)(m) symmetry that acts on these electric and magnetic operators, and we derive the mixed ’t Ho oft anomaly between them in the lattice formalism. In Sect. 3, we consider the 2D theory with two compact bosons φ1 and φ2. This theory has the continuous θ parameter, θ (2π)2 dφ1 ∧ dφ2, with the periodicity θ ∼ θ + 2π. Like the 24D Maxwell theory, this theory does not have the instanton solutio n, so one may think that the θ angle does not aﬀect the local dynamics at all. Although this observa tion is somewhat true for electric operators, the magnetic operators a re signiﬁcantly aﬀected by the θ angle: The analogue of the Witten eﬀect [ 6] occurs, that is, the magnetic operators acquire the fractional electric charge. We ﬁrst show that these p henomena can be understood as the higher-group-like structure [ 7–12] in the language of background gauge ﬁelds using the continuum description. After that, we discuss its lattice count erpart by extending the discussions in Sect. 2. Let us comment on other related studies, and this would clarify our m otivation more. In this paper, we use the Wilson-type lattice regularization that uses t he compact variables as the fundamental variables, and the topology of ﬁeld space appear s by imposing the admis- sibility constraint. For Abelian theories, there is another approach called the Villain-type formulation, which uses the R-valued n-form ﬁeld coupled with Z-valued (n + 1)-form gauge ﬁeld as fundamental variables to represent the U(1)-valued n-form gauge ﬁeld. A modiﬁed version of the Villain formulation is highly developed in Refs. [ 13–19] recently, and it can give the lattice derivation of our observations more transparently . Still, we will stick to the Wilson-type formulation in this paper because we would like to uncover the properties of magnetic operators in an extendable way for non-Abelian theories. 2 Single compact scalar ﬁeld In this section, we discuss the lattice formulation of the 2D compact boson. We ﬁrst give a review on its continuum description emphasizing the role of symm etries and mixed ’t Hooft anomaly. After the brief review, we consider the topology o f lattice bosons by imposing the admissibility condition, and we reproduce the continuum o bservations in the lattice description. 2.1 Review on 2D compact boson in the continuum description Let us start from the case of a single compact real scalar ﬁeld on 2D closed Riemannian manifold M2. Its continuum action is given by S[φ] = R2 4π ∫ M2 |dφ|2 = R2 4π ∫ M2 dφ ∧ ⋆ dφ , (2.1) where R > 0 denotes the compact-boson radius, and φ(x) enjoys the identiﬁcation φ(x) ∼ φ(x) + 2 π. (2.2) 3This system possesses U(1) zero-form “electric” and “magnetic” global symmetries, and t heir Noether currents are given by j(e)(x) ≡ ⋆R2 2π dφ(x) , j (m)(x) ≡ 1 2π dφ(x) , (2.3) respectively. In the electric picture ( 2.1), the conservation of the former, d j(e) = 0, is nothing but the equation of motion, and the one for the latter, d j(m) = 0, follows from the Bianchi identity. The charged object for U(1)(e) is the vertex operator eiφ(x), and the one for U(1)(m) is realized as the defect operator; one should remove a small neighb orhood of the given point p and impose the boundary condition so that ∫ S1p dφ 2π = 1 for small S1 surrounding p. Notable feature of the U(1)(e) × U(1)(m) symmetry is the existence of the mixed ’t Hooft anomaly. To see this, we introduce background gauge ﬁelds A(e) and A(m) that couple to the global symmetries. 1 The gauged continuum action is given by S[φ, A(e), A(m)] = R2 4π ∫ M2 ⏐ ⏐ ⏐dφ + A(e) ⏐ ⏐ ⏐ 2 + i 2π ∫ M2 A(m) ∧ [ dφ + A(e) ] . (2.4) This gauged action is manifestly invariant under the electric gauge tr ansformations, φ ↦→ φ − Λ (e) and A(e) ↦→A(e) + dΛ (e), where the gauge transformation parameter Λ (e) is also a 2 π-periodic scalar. Under the magnetic gauge transformation, A(m) ↦→A(m) + dΛ (m), the gauged action transforms as S → S + i 2π ∫ M2 dΛ (m) ∧ [ dφ + A(e) ] = S + i 2π ∫ M2 dΛ (m) ∧ A(e) + 2πiZ. (2.5) Deﬁning the partition function, Z[A(e), A(m)] ≡ ∫ [dφ] e−S, (2.6) we then ﬁnd the mixed ’t Hooft anomaly Z[A(e) + dΛ (e) , A(m) + dΛ (m)] = exp [ − i 2π ∫ M2 dΛ (m) ∧ A(e) ] Z[A(e), A(m)]. (2.7) We note that there are no 2D local counter terms that cancel this anomaly, so this is a genuine anomaly. This anomaly can be cancelled by the anomaly inﬂow from the 3 D topological 1 In this paper, we use the lower cases, such as φ, for the dynamical ﬁelds and the upper cases, such as A, for the background ﬁelds. We follow this convention also for the lat tice models, so, for instance, ( φ, ℓµ ) denotes the dynamical variables while ( Aµ , Nµν ) denotes the background ﬁelds. 4action, S3D[A(e), A(m)] = i 2π ∫ M3 A(m) ∧ dA(e) , (2.8) with ∂M3 = M2, so that Z[A(e), A(m)] exp(−S3D) is manifestly gauge invariant. 2.2 Lattice formulation with the admissibility condition Let us take M2 = T 2 and approximate it as the 2D square lattice Γ = ( Z/LZ)2 of size L with the periodic boundary condition. On the lattice, the basic dynam ical variable is deﬁned as eiφ(n) ∈ U(1), (2.9) where n denotes the lattice sites, and one can take the lattice counterpar t of the Euclidean action ( 2.1) as S = β ∑ n,µ {1 − cos [φ(n + ˆµ) − φ(n)]} , (2.10) where β = R2/(2π) and ˆµ denotes the unit vector in the µth direction. Let us deﬁne the variable φ(n) itself on the lattice by taking the logarithm of eiφ(n) with the principal branch, − π < φ (n) ≤ π. (2.11) We also deﬁne a directional diﬀerence of φ(n) in the same way as ∂φ(n, µ) ≡ 1 i ln [ e−iφ(n)eiφ(n+ˆµ) ] , −π < ∂φ (n, µ) ≤ π. (2.12) ∂φ(n, µ) is deﬁned on the lattice link (or bond) connecting a pair of sites n and n + ˆµ, which we will simply denote ( n, µ). We ﬁnd that ∂φ(n, µ) is related to the naive lattice derivative of φ(n) as ∂φ(n, µ) = φ(n + ˆµ) − φ(n)   ≡ ∆ µ φ(n) +2πℓµ(n), (2.13) with some ℓµ(n) ∈ Z. ℓµ(n) is a local functional of φ and it can be regarded as a Z-valued 1-form ﬁeld. 2 As a consequence, the directional line sum of ∂φ(n, µ) along any closed path C 2 In the (modiﬁed) Villain formulation [ 13–19], we start from the R-valued ﬁeld φ(n) and the Z-valued link ﬁeld ℓµ ∈ Z, and we impose the Z-valued gauge invariance, φ → φ + 2πλ and ℓµ → ℓµ − ∆ µ λ, with λ(n) ∈ Z. This gauge invariance can be ﬁxed by imposing φ(n) ∈ (−π, π], and this is called the Villain gauge. In the Villain formalism, ℓµ (n) can run over whole Z even after choosing the Villain gauge. However, in our Wilson-type formulation, ℓµ (n) can take only 0, ±1 by construction as ℓµ is an auxiliary ﬁeld to satisfy ∆ µ φ + 2πℓµ ∈ (−π, π]. This diﬀerence becomes more evident when we try to introduce the magnetic defects. To put it the other way around, the Wilson and Villain formulations shar e the similar properties up to the above diﬀerence. Therefore, most of our discussion, except the one for magnetic defects, can be applied in the same way for both formulations. 5is always an integral multiple of 2 π; Qmag(C) ≡ 1 2π ∑ (n,µ)∈C ∂φ(n, µ) = ∑ (n,µ)∈C ℓµ(n) ∈ Z, (2.14) where the sum is taken over links belonging to the loop C. Throughout this paper, we understand that the summand of the directional line sum is ∂φ(n, µ) when the path C goes through the link ( n, µ) in the direction from n to n + ˆµ, while −∂φ(n, µ) when C goes though ( n, µ) in the opposite direction, i.e., this is a lattice analogue of the line integr al. Equation ( 2.14) holds because ∆ µφ(n) does not contribute to the directional line sum along a closed loop; note that the ﬁeld φ(n) ∈ (−π, π] is single-valued on the lattice. We would like to identify 1 2π ∂φ(n, µ) as the lattice counterpart of the magnetic symmetry generator j(m) = 1 2π dφ. However, Qmag(C) on the lattice suﬀers from the discontinuous change under the deformation of C in general, and we need a remedy to correctly deﬁne the topological sectors. To this end, we impose the “admissibility con dition” on allowed conﬁgurations of φ(n): Let us ﬁx 0 < ǫ < π/ 2, then the conﬁguration is called admissible if sup n,µ |∂φ(n, µ)| < ǫ, (2.15) or, equivalently, for all the links ( n, µ) |1 − cos [∂φ(n, µ)]| < 2 sin2 ǫ 2. (2.16) Let Aǫ denote the set of admissible lattice ﬁelds, and the path integral is pe rformed only on Aǫ. Due to this restriction, Qmag(C) turns out to be topological, i.e., it does not change under any continuous deformations of C. Let us show that Qmag(C) is topological. We ﬁrst note that the directional line sum of ℓµ(n) along the boundary of a single plaquette p is bounded as ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ∑ (n,µ)∈p ℓµ(n) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ = 1 2π ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ∑ (n,µ)∈p ∂φ(n, µ) ⏐ ⏐ ⏐ ⏐ ⏐ ⏐ ≤ 1 2π ∑ (n,µ)∈p |∂φ(n, µ)| < 2 π ǫ < 1, (2.17) where we note that there are 4 links belonging to a 2D minimal loop. Sinc e the most left- hand-side of this equation is a sum of integers, we obtain 3 ∑ (n,µ)∈p ∂φ(n, µ) = 0 . (2.18) Since any deformation of the loop C can be realized by repeatedly adding or removing a single plaquette from the loop, we see that Qmag(C) deﬁned by ( 2.14) is invariant under a 3 In the modiﬁed Villain formulation, this condition is imposed by introducin g the Lagrange multiplier ˜φ so that its equation of motion gives ∆ µ ℓν − ∆ ν ℓµ = 0 in Z, and the magnetic defect is simply given by ei ˜φ . In the Wilson-type formulation, this trick does not work, so we need to develop other methods. 6change of C, once the admissibility is imposed. We note that Eq. ( 2.18) can be written as ∑ µ,ν εµν∆ µ 1 2π ∂φ(n, ν) = 0 , (2.19) which corresponds to d j(m) = 0 in the continuum theory. The admissibility condition decomposes the ﬁeld space Aǫ into distinct topological sectors. For example, the following conﬁguration eiφ(n) = exp [ 2πi L (w1n1 + w2n2) ] , (2.20) with 0 ≤ n1,2 < L , is admissible if the number of lattice points is large enough as L > (2π/ǫ) max(|w1|, |w2|), and it has the winding numbers w1, w2 ∈ Z along the 1, 2 directions, respectively. This corresponds to H1(T 2; Z) ≃ Z⊕2 ∋ (w1, w2) in the continuum formulation, and the conﬁgurations with diﬀerent ( w1, w2) cannot be continuously connected without violating the admissibility condition. 2.3 Magnetic defect operators on the lattice with admissibi lity The admissibility condition allows us to deﬁne the conserved current j(m) on the lattice, and the ﬁeld conﬁgurations are also topologically classiﬁed as in the ca se of the contin- uum theory. To establish the U(1)(m) symmetry, we introduce the charged object for the current j(m) in this subsection. The admissibility ( 2.15) tells Qmag(C) = 0 for any contractible loops C on the lattice Γ, and thus we cannot naively introduce the magnetically charged oper ator. This would remind us of the fact that the magnetic operators are introduced as the defects in the continuum description. Therefore, let us pick a certain 2D region D and remove all lattice points and links contained in D (see Fig. 1). We then deﬁne the magnetic operator of charge m ∈ Z by imposing the boundary condition m ≡ Qmag(∂D). (2.21) We note that the size of the excised region D need to be suﬃciently large for the magnetic defect to be well-deﬁned: As the admissibility is applied to links belonging to ∂D, for a given Qmag(∂D) = m, the 1D size of the region must be at least ∼ | m|π/(2ǫ) ( ≳ m) in lattice units. As the size of D can be determined independently from the coupling constants including the lattice constant, the magnetically charged object bec omes point-like in the continuum limit. This localized magnetic object can be identiﬁed with the vertex opera tor eim ˜φ(x) in the continuum theory, where ˜φ(x) is the dual scalar ﬁeld identiﬁed as ∑ ν εµν∂νφ(x) ∼ 7D Fig. 1: Excised region D on Γ. As the lattice points and links inside D are completely eliminated, the topological charge Qmag(∂D) around D can take nonzero value in Z. ∂µ ˜φ(x)/R2. In the continuum theory, the correlation functions containing bo th einφ(x) and eim ˜φ(y) are well-deﬁned as a single-valued function when n, m ∈ Z. One can readily conﬁrm this result in the present lattice formulation as follows: We ﬁr st note that eiφ(x) = eiφ(x′) exp  i ∑ (n,µ)∈Cx′x ∂φ(n, µ)   , (2.22) where Cx′x is a path connecting sites x′ and x. Moving eiφ(x) around a magnetically charged object with the magnetic charge m ∈ Z once thus results eiφ(x) → eiφ(x)e2πim = eiφ(x). (2.23) This shows that the correlation functions of eiφ(x) are single-valued even in the presence of magnetically charged objects deﬁned by our excision method; eiφ(x) and the magnetically charged object with m ∈ Z are mutually local. 2.4 Background gauging and ’t Hooft anomaly on the lattice 2.4.1 Introduction of background gauge ﬁelds We have shown that the lattice theory has both the electric and mag netic U(1) symme- tries, and thus we can now ask if it also has the correct ’t Hooft anom aly. To see this, let us consider the coupling to external gauge ﬁelds as Eq. ( 2.4). For this, we introduce two U(1) link variables, U(e)(n, µ), U (m)(˜n, µ). (2.24) Note that we put magnetic variables on the dual lattice whose site is d eﬁned from n ∈ Γ by ˜n ≡ n + 1 2 ˆ 1 + 1 2 ˆ 2. (2.25) 8The link (˜n, µ) is connecting two sites on the dual lattice, ˜ n and ˜n + ˆµ. The lattice electric gauge transformation is then given by φ(n) ↦→φ(n) − Λ (e)(n), U (e)(n, µ) ↦→e−iΛ (e)(n)U(e)(n, µ)eiΛ (e)(n+ˆµ), (2.26) and the magnetic gauge transformation is U(m)(˜n, µ) ↦→e−iΛ (m)(˜n)U(m)(˜n, µ)eiΛ (m)(˜n+ˆµ). (2.27) We deﬁne the covariant diﬀerence with respect to the electric gaug e symmetry by Dφ(n, µ) ≡ 1 i ln [ e−iφ(n)U(e)(n, µ)eiφ(n+ˆµ) ] , −π < Dφ (n, µ) ≤ π, (2.28) which is invariant under the electric gauge transformation. Instea d of Eq. ( 2.15), we now impose the admissibility of the form sup n,µ |Dφ(n, µ)| < ǫ, 0 < ǫ < π 2 . (2.29) Although the above link variables are the fundamental degrees of f reedom for lattice gauge ﬁelds, we can equivalently describe lattice Abelian gauge theor ies in terms of gauge potentials, such as the corresponding continuum theory. Let us s et A(e) µ (n) ≡ 1 i ln U(e)(n, µ), −π < A (e) µ (n) ≤ π, A(m) µ (˜n) ≡ 1 i ln U(m)(˜n, µ), −π < A (m) µ (˜n) ≤ π. (2.30) The corresponding ﬁeld strengths are deﬁned by F (e) µν (n) ≡ 1 i ln [ U(e)(n, µ)U(e)(n + ˆµ, ν)U(e)(n + ˆν, µ)−1U(e)(n, ν)−1 ] , (2.31) F (m) µν (˜n) ≡ 1 i ln [ U(m)(˜n, µ)U(m)(˜n + ˆµ, ν)U(m)(˜n + ˆν, µ)−1U(m)(˜n, ν)−1 ] (2.32) with −π < F (e,m) µν ≤ π. Under the electric or magnetic gauge transformation in Eqs. ( 2.26) and ( 2.27), we have A(e,m) µ ↦→A(e,m) µ + ∆ µΛ (e,m) + 2πL(e,m) µ , (2.33) where the integer ﬁelds L(e) µ (n), L(m) µ (˜n) ∈ Z are necessary because of our deﬁnition of the gauge potentials; L(e) µ (n) and L(m) µ (˜n) are local functionals of Λ (e)(n) and Λ (m)(˜n), respec- tively. We rewrite the following gauge-invariant variables in terms of A(e,m) µ ; the ﬁeld strength 9as F (e,m) µν = ∆ µA(e,m) ν − ∆ νA(e,m) µ + 2πN (e,m) µν , (2.34) and the covariant diﬀerence as Dφ(n, µ) = ∆ µφ(n) + A(e) µ (n) + 2 πℓ(e) µ (n), (2.35) where N(e) µν (n), N(m) µν (˜n), ℓ(e) µ (n) ∈ Z. Under the electric or magnetic gauge transformation, we ﬁnd that N(e,m) µν ↦→N(e,m) µν − ∆ µL(e,m) ν + ∆ νL(e,m) µ , ℓ (e) µ (n) ↦→ℓ(e) µ (n) − L(e) µ (n). (2.36) To show the topological nature even on the lattice, we have imposed the admissibility condition for the lattice boson φ(n) in Eq. ( 2.29). We further need a lattice counterpart of the Bianchi identity d j(m) = 0 in the continuum theory, that is, a constraint corresponding to ∑ µ,ν εµν∆ µℓν(n) = 0 in Eq. ( 2.17) with the background gauge ﬁelds. To see this, we assume that the external gauge ﬁelds are admissible as 4 sup n,µ,ν ⏐ ⏐ ⏐F (e) µν (n) ⏐ ⏐ ⏐< δ, sup ˜n,µ,ν ⏐ ⏐ ⏐F (m) µν (˜n) ⏐ ⏐ ⏐< δ, 0 < δ < min(π, 2π − 4ǫ). (2.37) Then, noticing that ⏐ ⏐ ⏐ ⏐ ⏐ ∑ µ,ν εµν [ ∆ µℓ(e) ν (n) − 1 2N(e) µν (n) ] ⏐ ⏐ ⏐ ⏐ ⏐= 1 2π ⏐ ⏐ ⏐ ⏐ ⏐ ∑ µ,ν εµν [ ∆ µDφ(n, ν) − 1 2F (e) µν (n) ] ⏐ ⏐ ⏐ ⏐ ⏐ < 2 π ǫ + 1 2π δ < 1, (2.38) we have ∑ µ,ν εµν [ ∆ µℓ(e) ν (n) − 1 2N(e) µν (n) ] = 0 , (2.39) because ℓ(e) µ (n) and N(e) µν (n) are integers. Therefore, ℓ(e) µ (n) satisﬁes the gauge-invariant constraint ( 2.39) similar to the Bianchi identity modiﬁed by N(e) µν (n); see the gauge transformation given in Eq. ( 2.36). Under Eq. ( 2.39), one ﬁnds that ∆ µDφ(n, ν) − ∆ νDφ(n, µ) = F (e) µν (n). (2.40) 4 We note that these background ﬁelds are introduced to detect th e ’t Hooft anomalies, and thus their ﬁeld strength can be arbitrarily weak. Therefore, we can assume a ny admissibility without loss of generality. 10n + ˆµ n + ˆµ + ˆν ˜n ˜n + ˆµ Fig. 2: Structure appearing in Eqs. ( 2.41) and ( 3.14). 2.4.2 Computation of the ’t Hooft anomaly on the lattice Now, with the above preparations, we take the following lattice actio n: S ≡ R2 4π ∑ n∈Γ ∑ µ Dφ(n, µ)Dφ(n, µ) + i 2π ∑ n∈Γ ∑ µ,ν εµνA(m) µ (˜n)Dφ(n + ˆµ, ν) + i 2 ∑ n∈Γ ∑ µ,ν εµνN(m) µν (˜n)φ(n + ˆµ + ˆν). (2.41) The ﬁrst line of the action corresponds to Eq. ( 2.4), and is manifestly invariant under the electric gauge transformation ( 2.26). We note that the second and third terms of the action, the magnetic couplings, have the structure depicted in Fig. 2. The second line, which is a local counter term as we will discuss later, is not invariant under the elect ric gauge transformation so that we see the ’t Hooft anomaly as e−S → e−S exp [ i 2 ∑ n∈Γ ∑ µ,ν εµνN(m) µν (˜n)Λ (e)(n + ˆµ + ˆν) ] . (2.42) Under the magnetic gauge transformation ( 2.33), the action changes as S → S + i 2π ∑ n∈Γ ∑ µ,ν εµν [ ∆ µΛ (m)(˜n)Dφ(n + ˆµ, ν) + 2 πL(m) µ (˜n)A(e) ν (n + ˆµ) ] + 2πiZ. (2.43) In the ﬁrst term in the square brackets on the right-hand side, pa ying attention to the terms containing Λ (m)(˜n) with a particular ˜ n, we see the structure 1 2 ∑ µ,ν εµνΛ (m)(˜n) [−∆ µDφ(n, ν) + ∆ νDφ(n, µ)] = −1 2 ∑ µ,ν εµνΛ (m)(˜n)F (e) µν (n), (2.44) under the admissibility conditions for Dφ(n) and F (e) µν (n). The lattice sites relevant to this combination are depicted in Fig. 3. Noting that any term depending on the dynamical ﬁelds, 11n n + ˆµ n + ˆν n + ˆµ + ˆν ˜n (a) Terms containing a ﬁeld at ˜ n. ˜n − ˆµ − ˆν ˜n − ˆν ˜n − ˆµ ˜n n (b) Terms containing a ﬁeld at n Fig. 3: Structure appearing in Eqs. ( 2.44) and ( 3.15). e.g., ( φ(n), ℓ(e) µ (n)), disappears in the expression S → S − i 2π ∑ n∈Γ ∑ µ,ν εµν [ 1 2Λ (m)(˜n)F (e) µν (n) − 2πL(m) µ (˜n)A(e) ν (n + ˆµ) ] + 2πiZ (2.45) thanks to the third term in the action ( 2.41), this can be regarded as a lattice counterpart of the mixed anomaly ( 2.7) in terms of the background ﬁelds. 5 It is interesting to ask what happens if we put a magnetic object in th e system ( 2.41) by our excision method; recall Fig. 1. For such a lattice with some region excised, it turns out that the dual lattice, on which the magnetic gauge ﬁeld A(m) µ (˜n) is residing, should be deﬁned as depicted in Fig. 4, where the dual site ˜ n∗ is deﬁned inside the excised region. The rule is that the product of A(m) µ and Dφ is deﬁned in the way depicted in Fig. 2; links on Γ and links on the dual lattice cross in that way. With these understan dings, we thus consider S ≡ R2 4π ∑ n∈Γ −D ∑ µ Dφ(n, µ)Dφ(n, µ) + i 2π ∑ n∈Γ −D ∑ µ,ν εµνA(m) µ (˜n)Dφ(n + ˆµ, ν) + i 2 ∑ n∈Γ −D ∑ µ,ν εµνN(m) µν (˜n)φ(n + ˆµ + ˆν), (2.46) where D is the region excised to represent the magnetic object. Also in this s etting we assume the condition in Eq. ( 2.39) on Γ − D , and then, Eq. ( 2.40) follows. As Eq. ( 2.43), under the 5 Let us discuss some consequences of the ’t Hooft anomaly in this latt ice formulation. First, the violation of the magnetic gauge invariance suggests that the partition functio n should vanish if ∫ dA(e) ̸= 0. In the lattice realization, there is no conﬁguration ( φ, ℓµ ) that satisﬁes the admissibility condition when ∑ n F (e) 12 (n) ̸= 0, and the path integral vanishes as expected. Inserting a magnetic operator that compensates for the monopole ﬂux of F (e), the unnormalized path integral can have a nontrivial value. As the dual of this observation, the path integral should also vanis h when ∫ dAm ̸= 0. In this case, the admissible lattice conﬁguration exists, but the integration over φ’s zero mode gives the complete cancellation as we added the term N(m) 12 φ in the local counter term. As a result, the path integral vanishes, which is consistent with the anomaly requirement. 12˜n∗ Fig. 4: Dual lattice in the presence of the excised region. magnetic gauge transformation, S → S + i 2π ∑ n∈Γ −D ∑ µ,ν εµν [ ∆ µΛ (m)(˜n)Dφ(n + ˆµ, ν) + 2 πL(m) µ (˜n)A(e) ν (n + ˆµ) ] + 2πiZ. (2.47) Even with the presence of the excised region as Fig. 4, we ﬁnd that the argument is almost the same as above. The ﬁrst term in the square brackets, howeve r, produces the line sum of ℓ(e) µ along ∂D and, noting the structure of the dual lattice within D, we ﬁnd e−S → e−S exp { i 2π ∑ n∈Γ −D ∑ µ,ν εµν [ 1 2Λ (m)(˜n)F (e) µν (n) − 2πL(m) µ (˜n)A(e) ν (n + ˆµ) ] } × exp [ imΛ (m)(˜n∗) ] (2.48) where m ≡ ∑ (n,µ)∈∂D ℓ(e) µ (n) can be regarded as the magnetic charge in view of Eqs. ( 2.21) and ( 2.14). The last factor in Eq. ( 2.48) gives rise to a breaking of the magnetic gauge symmetry owing to the presence of the magnetic object. This breaking howev er may be cured by connecting an “open ’t Hooft line” in the dual lattice to the magnetic o bject. In fact, by supplementing the phase factor exp  −im ˜n∗∑ (˜n,µ)∈P A(m) µ (˜n)   , (2.49) 13where P denotes a path on the dual lattice ending at ¯ n∗, in the functional integral. With this understanding, under the magnetic gauge transformation, e−S → e−S exp { i 2π ∑ n∈Γ −D ∑ µ,ν εµν [ 1 2Λ (m)(˜n)F (e) µν (n) − 2πL(m) µ (˜n)A(e) ν (n + ˆµ) ] } . (2.50) This completes our discussion on the single scalar case. We observed that the excision method to deﬁne a magnetic object on the lattice works quite well to reprod uce phenomenon expected in the continuum theory. 3 The case of two compact scalar ﬁelds In this section, we extend the previous discussion to the case with t wo compact scalars. As H2(S1 × S1; Z) ≃ Z, one can introduce the continuous θ angle, and the analogue of the Witten eﬀect occurs. This is quite natural since we can obtain th is model by putting the Maxwell theory on M4 = M2 × T 2. We ﬁrst discuss these properties in the continuum description and then give the lattice reformulation. 3.1 Continuum description of the θ angle, ’t Hooft anomaly, and Witten eﬀect Having two periodic scalar ﬁelds, φa(x) ( a = 1 and 2), we can deﬁne the topological charge by Q = 1 4π2 ∫ M2 dφ1 ∧ dφ2 ∈ Z, (3.1) which corresponds to H2(S1 × S1; Z). The action is then given by Sθ[φa] = ∫ M2 ∑ a,b Gab dφa ∧ ⋆ dφb − iθ 4π2 ∫ M2 dφ1 ∧ dφ2 , (3.2) where Gab is a positive symmetric matrix. As a simplest choice, one may take it as Gab = R2 4π δab. We note that θ can be regarded as the 2 π periodic variable, θ ∼ θ + 2π. This theory has the U(1)(e,a) × U(1)(m,a) symmetry for each φa, and we write the back- ground gauge ﬁelds as A(e,a) and A(m,a) with a = 1, 2. The gauged action is then given by Sθ[φa, A(e,a), A(m,a)] = ∫ M2 ∑ a,b Gab [ dφa + A(e,a) ] ∧ ⋆ [ dφb + A(e,b) ] − iθ 4π2 ∫ M2 [ dφ1 + A(e,1) ] ∧ [ dφ2 + A(e,2) ] + i 2π ∫ M2 ∑ a A(m,a) ∧ [ dφa + A(e,a) ] . (3.3) 14As we have discussed in Eq. ( 2.4), this is manifestly invariant under the electric gauge transformations φa ↦→φa − Λ (e,a) and A(e,a) ↦→A(e,a) + dΛ (e,a), but the magnetic gauge transformation, A(m,a) ↦→A(m,a) + dΛ (m,a), has an anomaly. This ’t Hooft anomaly can be cancelled by regarding this theory as the boundary theory of the 3 D topological action, S3D[A(e,a), A(m,a)] = i 2π ∫ M3 ∑ a A(m,a) ∧ dA(e,a) , (3.4) with ∂M3 = M2. As in the case of Eq. ( 2.8), the gauged partition function Zθ[A(e,a), A(m,a)] cannot respect the background gauge invariance, but Zθ[A(e,a), A(m,a)] exp(−S3D) does. The presence of the continuous θ angle provides a richer structure to the global symmetry. To see this, we ﬁrst note that the 2 π periodicity of θ is explicitly broken by the introduction of the background gauge ﬁelds in Eq. ( 3.3): Sθ+2π − Sθ = − i 2π ∫ M2 [ dφ1 + A(e,1) ] ∧ [ dφ2 + A(e,2) ] = − i 2π ∫ M2 [ dφ1 ∧ dφ2 + A(e,1) ∧ dφ2 − A(e,2) ∧ dφ1 + A(e,1) ∧ A(e,2) ] . (3.5) The ﬁrst term on the right-hand side is quantized as 2 πiZ, and thus it does not aﬀect the path-integral weight. The last term does not cause the serious pr oblem as it depends only on the background gauge ﬁelds. The serious violation of the θ periodicity comes from the mixed term, A(e,a) ∧ dφb, and we shall ﬁnd its remedy by considering the higher-group-type extension of the symmetry [ 7–12]. The key observation is that the mixed terms in Eq. ( 3.5) can be cancelled by the shift of the magnetic gauge ﬁelds: θ → θ + 2π, A (m,1) → A(m,1) − A(e,2), A (m,2) → A(m,2) + A(e,1). (3.6) As a result, we ﬁnd that Zθ+2π[A(e,a), A(m,1) − A(e,2), A(m,2) + A(e,1)] = exp [ − i 2π ∫ M2 A(e,1) ∧ A(e,2) ] Zθ[A(e,a), A(m,1), A(m,2)]. (3.7) The phase factor on the right-hand side is called the global inconsist ency or also called the generalized anomaly of higher-group-like structure involving the ( −1)-form symmetry [ 20– 27]. We note that this anomaly ( 3.7) is also cancelled by the 3D topological action ( 3.4): S3D[A(e,a), A(m,a) − εabA(e,b)] − S3D[A(e,a), A(m,a)] = i 2π ∫ M3 [ −A(e,2) ∧ dA(e,1) + A(e,1) ∧ dA(e,2) ] = − i 2π ∫ M2 A(e,1) ∧ A(e,2). (3.8) 15In the following subsection, we are particularly interested in how the structure ( 3.7) is implemented on the lattice with the presence of the magnetically char ged object. Before moving on to the discussion on the lattice regularization, let u s discuss the physical meaning of Eq. ( 3.6): The higher-group structure ( 3.6) detects the Witten eﬀect [ 6] for the magnetic defects. As we have discussed, θ has the periodicity 2 π as the consequence of the quantization Q ∈ Z. This suggests that the QFTs at θ and θ + 2π are unitary equivalent, but the unitary transformation may have nontrivial actions on the ene rgy levels and/or operator spectrum. For example, when we compute the correlation function that contains M1(x) (the magnetic defect for φ1), we have the relation ⟨M1(x) · · · ⟩θ+2π = ⣨ M1(x)eiφ2(x) · · · ⟩ θ , (3.9) up to the renormalization procedure. Similarly, M2(x) should be replaced by M2(x)e−iφ1(x) when relating θ + 2π and θ. This is nothing but the analogue of the Witten eﬀect in the present system, and the higher group ( 3.6) captures this phenomenon. 3.2 Lattice formulation of the θ angle and the Witten eﬀect 3.2.1 Deﬁnition of the topological charge on the lattice Now, for the case of two scalar ﬁelds, we will ﬁnd that the topologica l charge on the lattice possesses a better behavior if the ﬁeld φ2(x) is put on the dual lattice instead of the original lattice Γ. Thus, corresponding to Eq. ( 2.12), we deﬁne ∂φ1(n) ≡ 1 i ln [ e−iφ1(n)eiφ1(n+ˆµ) ] , ∂φ 2(˜n) ≡ 1 i ln [ e−iφ2(˜n)eiφ2(˜n+ˆµ) ] . (3.10) As Eq. ( 2.13), we then have ∂φ1(n, µ) = ∆ µφ1(n) + 2 πℓ1,µ(n), ∂φ 2(˜n, µ) = ∆ µφ2(˜n) + 2 πℓ2,µ(˜n), (3.11) where ℓ1,µ(n) and ℓ2,µ(˜n) are integers. The admissibility is set as sup n,µ |∂φ1(n, µ)| < ǫ, sup ˜n,µ |∂φ2(˜n, µ)| < ǫ. (3.12) Then, we have the Bianchi identities, ∑ µ,ν εµν∆ µℓ1,ν(n) = 0 , ∑ µ,ν εµν∆ µℓ2,ν(˜n) = 0 . (3.13) Now, as a lattice counterpart of the topological charge ( 3.1), we adopt Q ≡ − 1 4π2 ∑ n∈Γ ∑ µ,ν εµν∂φ2(˜n, µ)∂φ1(n + ˆµ, ν). (3.14) 16We note that this topological charge is given by the sum of terms rep resented by Fig. 2, where the solid line represents the original link and the broken line the dual link because ∂φ2(˜n, µ) and ∂φ1(n + ˆµ, ν) are put on those links, respectively. Let us ﬁrst conﬁrm that Q (3.14) takes integral values. By substituting Eq. ( 3.11) into Eq. ( 3.14), we have Q = − 1 4π2 ∑ n∈Γ ∑ µ,ν εµν [ ∆ µφ2(˜n)∆ νφ1(n + ˆµ) + 2π∆ µφ2(˜n)ℓ1,ν(n + ˆµ) + 2 πℓ2,µ(˜n)∆ νφ1(n + ˆµ) + 4π2ℓ2,µ(˜n)ℓ1,ν (n + ˆµ) ] . (3.15) For the ﬁrst and second terms on the right-hand side, we can repe at the argument in Eq. ( 2.44); recall Fig. 3a. By replacing Λ (m)(˜n) → φ2(˜n) and Dφ(n, ν) → ∆ νφ1(n) or ℓ1,ν(n), we observe that these identically vanish because of the ﬁrst of t he Bianchi identi- ties ( 3.13). The situation is similar for the third term on the right-hand side of E q. ( 3.15) by exchanging the role of the original lattice and the dual lattice; see F ig. 3b. Because of the second of the Bianchi identities ( 3.13), this also identically vanishes. In this way, we ﬁnd Q = − ∑ n∈Γ ∑ µ,ν εµνℓ2,µ(˜n)ℓ1,ν(n + ˆµ). (3.16) Since ℓ2,µ(˜n) and ℓ1,µ(n) are integers, the lattice topological charge ( 3.14) is manifestly an integer. We emphasize that for this, the admissibility condition which e nsures the Bianchi identities are crucial. Actually, since all ﬁeld conﬁgurations of φ1(n) and φ2(˜n) on the lattice are connected, it is impossible to assign an integer topological charg e to conﬁgurations in a well-deﬁned way without excluding some (non-smooth) ﬁeld conﬁgur ations; the admissibility does this. 3.2.2 Witten eﬀect on the lattice Since we have an integer topological charge on the lattice, we may co nsider a lattice action containing the θ term corresponding to Eq. ( 3.2): S ≡ R2 4π ∑ n∈Γ ∑ µ ∑ a ∂φa(n, µ)∂φa(n, µ) + iθ 4π2 ∑ n∈Γ ∑ µ,ν εµν∂φ2(˜n, µ)∂φ1(n + ˆµ, ν). (3.17) Moreover, since we can introduce a magnetically charged object by the excising method, we may study an analogue of the Witten eﬀect in the form of Eq. ( 3.9) in the present lattice formulation. 17Let us thus consider a magnetic object which possesses the magne tic charge m1 with respect to the magnetic symmetry associated with φ1(n). This implies that we excise a region, D, in the original lattice Γ as Fig. 1. The magnetic charge is given by m = Qmag(∂D) ( 2.21) with φ(n) → φ1(n). The dual lattice, on which the ﬁeld φ2(˜n) is residing, is deﬁned in the previous section; see Fig. 4. The rule is similar to that in the previous section and the product of φ1(n) and φ2(˜n) is deﬁned in the way depicted in Fig. 2; links on Γ and links on the dual lattice cross in that way. With these understandings, we consider S ≡ R2 4π ∑ n∈Γ −D ∑ µ ∑ a ∂φa(n, µ)∂φa(n, µ) + iθ 4π2 ∑ n∈Γ −D ∑ µ,ν εµν∂φ2(˜n, µ)∂φ1(n + ˆµ, ν). (3.18) Under θ → θ + 2π, this action changes by (see Eq. ( 3.15)), S → S + i 2π ∑ n∈Γ −D ∑ µ,ν εµν [ ∆ µφ2(˜n)∆ νφ1(n + ˆµ) + 2π∆ µφ2(˜n)ℓ1,ν(n + ˆµ) + 2 πℓ2,µ(˜n)∆ νφ1(n + ˆµ) ] + 2πiZ. (3.19) As we have analyzed, even with the presence of the excised region D as in Fig. 4, the same argument goes well for most of the parts to identically vanish; comp are Figs. 3b and 4. This is however not applied to the second term because of the existe nce of ˜ n∗ inside D; compare Figs. 3a and 4. Instead, we have the line sum of ℓ1,µ(n) along the boundary of D, i.e., S → S − iφ2(˜n∗) ∑ (n,µ)∈∂D ℓ1,µ(n) + 2 πiZ = S − im1φ2(˜n∗) + 2 πiZ, (3.20) where we have used Eq. ( 2.21) (with m → m1 and φ → φ1). Regarding ˜n∗ as the position of the magnetic object, this precisely reproduces the Witten eﬀect in Eq. ( 3.9). We emphasize that our particular discretization made the electric charge of the m agnetic object induced by θ → θ + 2π precisely quantized even with ﬁnite lattice spacings. 18Finally, we consider a lattice counterpart of Eq. ( 3.7). With obvious generalizations of our notations, we set ( a runs over 1 and 2) S ≡ R2 4π ∑ n∈Γ ∑ µ ∑ a Dφa(n, µ)Dφa(n, µ) + i 2π ∑ n∈Γ ∑ µ,ν εµν [ A(m,1) µ (˜n)Dφ1(n + ˆµ, ν) − Dφ2(˜n, µ)A(m,2) ν (n + ˆµ) ] + i 2 ∑ n∈Γ ∑ µ,ν εµν [ N(m,1) µν (˜n)φ1(n + ˆµ + ˆν) + φ2(˜n)N(m,2) µν (n) ] + iθ 4π2 ∑ n∈Γ ∑ µ,ν εµνDφ2(˜n, µ)Dφ1(n + ˆµ, ν). (3.21) The third line is a local counter term as we will discuss later. Here, we a ssumed that the ﬁelds φ1(n), A(m,2) µ (n) and N(m,2) µν (n) are residing on the original lattice while φ2(˜n), A(m,1) µ (˜n) and N(m,1) µν (˜n) are residing on the dual lattice. Also, on possible conﬁgurations of dynamical and external lattice ﬁelds, we assume appropriate restrictions sim ilar to those in the previous section. Then, a lattice analogue of the shifts in Eq. ( 3.6) is θ → θ + 2π, A (m,1) µ (˜n) → A(m,1) µ (˜n) − A(e,2) µ (˜n), A (m,2) µ (n) → A(m,2) µ (n) + A(e,1) µ (n). (3.22) Then, we also shift N(m,a) µν , which appears in the ﬁeld strength F (m,a) µν , as N(m,1) µν (˜n) → N(m,1) µν (˜n) − N(e,2) µν (˜n), N (m,2) µν (n) → N(m,2) µν (n) + N(e,1) µν (n). (3.23) These shifts of A(m,a) µ and N(m,a) µν respect those of the gauge-invariant external ﬁelds, F (m,1) µν (˜n) → F (m,1) µν (˜n) − F (e,2) µν (˜n) and F (m,2) µν (n) → F (m,2) µν (n) + F (e,1) µν (n). Under Eqs. ( 3.22) and ( 3.23), the lattice action ( 3.21) now changes as S → S − i 2π ∑ n∈Γ ∑ µ,ν εµνA(e,2) µ (˜n)A(e,1) ν (n + ˆµ) + 2 πiZ. (3.24) Here, we have used relations similar to Eq. ( 2.43), that is, ∑ µ,ν εµν [ ∆ µℓ(e,1) ν (n) − 1 2N(e,1) µν (n) ] = 0 , ∑ µ,ν εµν [ ∆ µℓ(e,2) ν (˜n) − 1 2N(e,2) µν (˜n) ] = 0 . (3.25) Note that this change of the action depends only on the backgroun d ﬁelds, not on the dynamical ﬁelds, e.g., ( φ(n), ℓ(e) µ (n)). This is because of the third term in the action ( 3.21), which can be regarded as a lattice counterpart of the mixed anomaly (3.7) in terms of the background ﬁelds. Then, we can conclude that this change of the a ction provides a lattice analogue of the mixed ’t Hooft anomaly ( 3.7). 194 Summary and discussions In this paper, we have studied the properties of magnetic operato rs on the lattice ﬁeld theories with the admissibility condition. Such defect operators oft en play important roles to characterize the phases of QFTs, and thus it should be useful to u nderstand their properties at the ﬁnite lattice spacing. As the simplest model, we focus on the 2D compact bosons and give the lattice derivation of the mixed ’t Hooft anomaly between the e lectric and magnetic U(1) symmetries. When there are several compact bosons, the mo del admits the continuous θ angle as in the case of 4D Maxwell theory. By introducing the magnet ic operators, we can observe the analogue of the Witten eﬀect, and the lattice theo ry can also derive such a phenomenon. To observe these phenomena, the most important task is to introd uce the magnetic operators. When we impose the admissibility condition to reinstate th e topological feature, the lattice conﬁgurations no longer accept the magnetic operator s in a naive way, and thus we propose the excision method that removes a small region of the la ttice and imposes the boundary condition. This method works so nicely for 2D compact bosons that we can reproduce the Witten eﬀect in an ultra-local way at ﬁnite lattice spa cings. At the formal level, we can apply this excision method for any lattice t heories, including higher-dimensional non-Abelian theories, to introduce the magnet ic operators. It would be an interesting future study to uncover if this method can derive no ntrivial properties of magnetic operators for such theories, such as the Witten eﬀect o f 4D SU (N)/ZN gauge theories with the lattice θ angle [ 28, 29]. Acknowledgments This work was partially supported by Japan Society for the Promotio n of Science (JSPS) Grant-in-Aid for Scientiﬁc Research Grant Numbers JP21J30003 ( O.M.), JP20H01903, JP23K03418 (H.S.), and JP22H01218 (Y.T.). The work of M.A. was sup ported by a Kyushu University Innovator Fellowship in Quantum Science. The work of Y.T. was supported by Center for Gravitational Physics and Quantum Information (CGPQ I) at Yukawa Institute for Theoretical Physics. References [1] G. ’t Hooft, “On the phase transition towards permanent q uark conﬁnement,” Nucl.Phys.B 138 (1978) 1–25 . [2] P. Goddard, J. Nuyts, and D. I. Olive, “Gauge Theories and Magnetic Charge,” Nucl. Phys. B125 (1977) 1–28. [3] A. Kapustin, “Wilson-’t Hooft operators in four-dimens ional gauge theories and S-duality,” Phys. Rev. D74 (2006) 025005 , arXiv:hep-th/0501015. [4] O. Aharony, N. Seiberg, and Y. Tachikawa, “Reading betwe en the lines of four-dimensional gauge theories,” JHEP 08 (2013) 115 , arXiv:1305.0318 [hep-th]. 20[5] M. L¨ uscher, “Topology of Lattice Gauge Fields,” Commun. Math. Phys.85 (1982) 39 . [6] E. Witten, “Dyons of Charge eθ/2π,” Phys. Lett. B86 (1979) 283–287 . [7] E. Sharpe, “Notes on generalized global symmetries in QF T,” Fortsch. Phys. 63 (2015) 659–682 , arXiv:1508.04770 [hep-th]. [8] F. Benini, C. C´ ordova, and P.-S. Hsin, “On 2-Group Globa l Symmetries and their Anomalies,” JHEP 03 (2019) 118 , arXiv:1803.09336 [hep-th]. [9] C. C´ ordova, T. T. Dumitrescu, and K. Intriligator, “Exp loring 2-Group Global Symmetries,” JHEP 02 (2019) 184, arXiv:1802.04790 [hep-th]. [10] Y. Tanizaki and M. ¨Unsal, “Modiﬁed instanton sum in QCD and higher-groups,” JHEP 03 (2020) 123 , arXiv:1912.01033 [hep-th]. [11] Y. Hidaka, M. Nitta, and R. Yokokura, “Global 3-group sy mmetry and ’t Hooft anomalies in axion electrodynamics,” JHEP 01 (2021) 173 , arXiv:2009.14368 [hep-th]. [12] Y. Hidaka, M. Nitta, and R. Yokokura, “Topological axio n electrodynamics and 4-group symmetry,” Phys. Lett. B 823 (2021) 136762 , arXiv:2107.08753 [hep-th]. [13] C. Gattringer, D. G¨ oschl, and T. Sulejmanpasic, “Dual simulation of the 2d U(1) gauge Higgs model at topological angle θ = π : Critical endpoint behavior,” Nucl. Phys. B935 (2018) 344–364 , arXiv:1807.07793 [hep-lat]. [14] T. Sulejmanpasic and C. Gattringer, “Abelian gauge the ories on the lattice: θ-terms and compact gauge theory with(out) monopoles,” Nucl. Phys. B943 (2019) 114616 , arXiv:1901.02637 [hep-lat]. [15] T. Sulejmanpasic, D. G¨ oschl, and C. Gattringer, “Firs t-Principles Simulations of 1+1D Quantum Field Theories at θ = π and Spin Chains,” Phys. Rev. Lett.125 no. 20, (2020) 201602 , arXiv:2007.06323 [cond-mat.str-el]. [16] M. Anosova, C. Gattringer, and T. Sulejmanpasic, “Self -dual U(1) lattice ﬁeld theory with a θ-term,” JHEP 04 (2022) 120 , arXiv:2201.09468 [hep-lat]. [17] T. Jacobson and T. Sulejmanpasic, “Modiﬁed Villain for mulation of abelian Chern-Simons theory,” arXiv:2303.06160 [hep-th]. [18] P. Gorantla, H. T. Lam, N. Seiberg, and S.-H. Shao, “A mod iﬁed Villain formulation of fractons and other exotic theories,” J. Math. Phys.62 no. 10, (2021) 102301 , arXiv:2103.01257 [cond-mat.str-el]. [19] Y. Choi, C. C´ ordova, P.-S. Hsin, H. T. Lam, and S.-H. Sha o, “Noninvertible duality defects in 3+1 dimensions,” Phys. Rev. D105 no. 12, (2022) 125016 , arXiv:2111.01139 [hep-th]. [20] D. Gaiotto, A. Kapustin, Z. Komargodski, and N. Seiberg , “Theta, Time Reversal, and Temperature,” JHEP 05 (2017) 091 , arXiv:1703.00501 [hep-th]. [21] Y. Tanizaki and Y. Kikuchi, “Vacuum structure of bifund amental gauge theories at ﬁnite topological angles,” JHEP 06 (2017) 102 , arXiv:1705.01949 [hep-th]. [22] Z. Komargodski, A. Sharon, R. Thorngren, and X. Zhou, “C omments on Abelian Higgs Models and Persistent Order,” SciPost Phys. 6 no. 1, (2019) 003 , arXiv:1705.04786 [hep-th]. [23] Y. Kikuchi and Y. Tanizaki, “Global inconsistency, ’t H ooft anomaly, and level crossing in quantum mechanics,” Prog. Theor. Exp. Phys.2017 (2017) 113B05 , arXiv:1708.01962 [hep-th]. [24] Y. Tanizaki and T. Sulejmanpasic, “Anomaly and global i nconsistency matching: θ-angles, SU (3)/U(1)2 nonlinear sigma model, SU (3) chains and its generalizations,” Phys. Rev. B98 no. 11, (2018) 115126 , arXiv:1805.11423 [cond-mat.str-el]. [25] A. Karasik and Z. Komargodski, “The bi-fundamental gau ge theory in 3+1 dimensions: The vacuum structure and a cascade,” JHEP 05 (2019) 144 , arXiv:1904.09551 [hep-th]. [26] C. C´ ordova, D. S. Freed, H. T. Lam, and N. Seiberg, “Anom alies in the Space of Coupling Constants and Their Dynamical Applications I,” SciPost Phys. 8 no. 1, (2020) 001 , arXiv:1905.09315 [hep-th]. [27] C. C´ ordova, D. S. Freed, H. T. Lam, and N. Seiberg, “Anom alies in the Space of Coupling Constants and Their Dynamical Applications II,” SciPost Phys. 8 no. 1, (2020) 002 , arXiv:1905.13361 [hep-th]. [28] M. Abe, O. Morikawa, S. Onoda, H. Suzuki, and Y. Tanizaki , “Topology of SU (N) lattice gauge theories coupled with ZN 2-form gauge ﬁelds,” arXiv:2303.10977 [hep-lat]. [29] M. Abe, O. Morikawa, and S. Onoda, “A note on lattice desc ription of generalized symmetries in SU (N)/ZN gauge theories,” arXiv:2304.11813 [hep-th]. 21",
      "references": [
        "On the phase transition towards permanent quark confinement",
        "Gauge Theories and Magnetic Charge",
        "Wilson-'t Hooft operators in four-dimensional gauge theories and S-duality",
        "Reading between the lines of four-dimensional gauge theories",
        "Topology of Lattice Gauge Fields",
        "Dyons of Charge eθ/2π",
        "Notes on generalized global symmetries in QFT",
        "On 2-Group Global Symmetries and their Anomalies",
        "Exploring 2-Group Global Symmetries",
        "Modified instanton sum in QCD and higher-groups",
        "Global 3-group symmetry and 't Hooft anomalies in axion electrodynamics",
        "Topological axion electrodynamics and 4-group symmetry",
        "Dual simulation of the 2d U(1) gauge Higgs model at topological angle θ = π : Critical endpoint behavior",
        "Abelian gauge theories on the lattice: θ-terms and compact gauge theory with(out) monopoles",
        "First-Principles Simulations of 1+1D Quantum Field Theories at θ = π and Spin Chains",
        "Self-dual U(1) lattice field theory with a θ-term",
        "Modified Villain formulation of abelian Chern-Simons theory",
        "A modified Villain formulation of fractons and other exotic theories",
        "Noninvertible duality defects in 3+1 dimensions",
        "Theta, Time Reversal, and Temperature",
        "Vacuum structure of bifundamental gauge theories at finite topological angles",
        "Comments on Abelian Higgs Models and Persistent Order",
        "Global inconsistency, 't Hooft anomaly, and level crossing in quantum mechanics",
        "Anomaly and global inconsistency matching: θ-angles, SU (3)/U(1)2 nonlinear sigma model, SU (3) chains and its generalizations",
        "The bi-fundamental gauge theory in 3+1 dimensions: The vacuum structure and a cascade",
        "Anomalies in the Space of Coupling Constants and Their Dynamical Applications I",
        "Anomalies in the Space of Coupling Constants and Their Dynamical Applications II",
        "Topology of SU (N) lattice gauge theories coupled with ZN 2-form gauge fields",
        "A note on lattice description of generalized symmetries in SU (N)/ZN gauge theories"
      ],
      "meta_data": {
        "arxiv_id": "2304.14815v1",
        "doi": "10.1093/ptep/ptad078",
        "authors": [
          "Motokazu Abe",
          "Okuto Morikawa",
          "Soma Onoda",
          "Hiroshi Suzuki",
          "Yuya Tanizaki"
        ],
        "published_date": "2023-04-28T12:51:16Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The main research problem addresses how to define and study monopole physics, particularly the Witten effect, on the lattice when admissibility conditions typically prevent the naive introduction of magnetic operators. The key contributions include proposing the \"excision method\" for defining magnetic operators in 2D compact scalar field theories on the lattice, providing a lattice derivation of the mixed 't Hooft anomalies between electric and magnetic U(1) symmetries, and extending this framework to two compact scalar fields to derive the higher-group-like structure related to the Witten effect. The paper demonstrates that the Witten effect, where magnetic operators acquire fractional electric charge, is precisely reproduced and quantized at finite lattice spacings using their proposed method.",
        "methodology": "The study primarily focuses on 2D compact scalar fields on a lattice using Wilson-type lattice regularization. The core technique is the \"excision method,\" which involves removing a small, O(1) region of lattice links (or bonds) where a monopole is located, and defining the dual lattice specifically around this excised region. This allows for the imposition of boundary conditions to define magnetic operators with integer charges. The \"admissibility condition\" is applied to lattice field configurations to ensure topological features. Background U(1) gauge fields are introduced on both the original and dual lattices to analyze 't Hooft anomalies. Lattice actions are constructed to incorporate electric, magnetic, and θ-terms, and their transformations under gauge and θ-periodicity shifts are analyzed to derive anomalies and the Witten effect.",
        "experimental_setup": "This paper is theoretical in nature, focusing on the development of a lattice formalism for magnetic operators and related phenomena. It does not describe any experimental setup, datasets, or benchmarks in the traditional sense. The 'validation' of the proposed methods comes from their ability to reproduce known continuum physics phenomena, such as the 't Hooft anomaly and the Witten effect, within the lattice framework.",
        "limitations": "The paper primarily focuses on 2D compact scalar field theories, which are presented as the simplest examples. While the Wilson-type lattice regularization used is extendable, it is noted to be less transparent for certain derivations compared to the Villain-type formulation. The admissibility condition for defining magnetic defects requires the excised region to be sufficiently large (at least ~|m|π/(2ǫ) in lattice units), which is a constraint on the method's applicability for very small defects. The discussion is limited to Abelian theories in 2D.",
        "future_research_directions": "The authors suggest extending the proposed excision method to higher-dimensional non-Abelian theories. A specific future research direction mentioned is to investigate whether this method can derive non-trivial properties of magnetic operators, such as the Witten effect, in 4D SU(N)/ZN gauge theories with a lattice θ angle.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Automatic Chain of Thought Prompting in Large Language Models",
      "full_text": "Click-Feedback Retrieval Zeyu Wang, Yu Wu Princeton University zeyuwang,yuwu@cs.princeton.edu Abstract Retrieving target information based on input query is of fundamental importance in many real-world applications. In practice, it is not uncommon for the initial search to fail, where additional feedback information is needed to guide the searching process. In this work, we study a set- ting where the feedback is provided through users clicking liked and disliked searching results. We believe this form of feedback is of great practical interests for its convenience and efﬁciency. To facilitate future work in this direction, we construct a new benchmark termed “click-feedback re- trieval” based on a large-scale dataset in fashion domain. We demonstrate that incorporating click-feedback can dras- tically improve the retrieval performance, which validates the value of the proposed setting. We also introduce sev- eral methods to utilize click-feedback during training, and show that click-feedback-guided training can signiﬁcantly enhance the retrieval quality. We hope further exploration in this direction can bring new insights on building more efﬁcient and user-friendly search engines. 1. Introduction One of the most frequent activities users perform on the Internet is searching. From learning knowledge to shop- ping clothes, retrieving target information by inputting a search query is always the ﬁrst step. In this work, we study the issue of how to help users obtain target information effectively. Speciﬁcally, we focus on the image retrieval task for fashion product search [13, 14], as it is a setting of much practical interest, and attracts lots of attention re- cently [20, 15, 1, 4]. However, we note that the underly- ing ideas are generalizable and can be potentially applied to other searching tasks as well. A typical situation in practical fashion product search is that the user fails to get the target product after just a single search [19]. It could be due to the user’s query is ambiguous, only containing partial information of the in- tended product, or simply because the search engine is not strong enough and makes noisy retrieval. In such scenar- CUP Initial  Search Click-Feedback Final Results Figure 1. The diagram showing the proposed task of click- feedback retrieval. The task contains three steps. First, a text de- scription is input as query to search target product. Then based on the initial retrieval result, a feedback agent (human in practical settings) provides feedback through clicking the liked and disliked images. Finally, the retrieval result is updated based on the initial retrieval and given click-feedback. ios, additional information is needed to guide the search en- gine to retrieve the target product. Many previous works have attempted on the issue, and a popular line of works investigate the solution of utilizing extra text input as feed- back [32, 4, 12]. Speciﬁcally, they assume that after the ini- tial search, the user would then provide a description of the desired changes upon the retrieved product [22, 2]. Some works have also explored other forms of feedback, for ex- ample, letting users to draw a sketch of target product [31] or asking them questions to answer [3]. In this work, we instead focus on a different type of feed- back, where the users only need to do a few clicks to pro- vide their preferences. We build a new retrieval benchmark around this form of feedback and call it click-feedback re- trieval. As Figure 1 shows, the task is composed of three steps. Initially, a text description is used as query for a ﬁrst round search (the typical image retrieval setting). After the retrieval result is obtained, several top candidates are input to a feedback agent, and the agent provides feedback and arXiv:2305.00052v1  [cs.CV]  28 Apr 2023returns a set of liked images (similar to target product and contain desired features) and disliked images (irrelevant or contain unwanted features). Finally, a second round of re- trieval is performed based on the ﬁrst-round result and re- ceived feedback. In practice, step two and three can repeat multiple times until target product is retrieved. In comparison to other forms, click-feedback provides several unique beneﬁts. First and foremost is its conve- nience. Compared with typing extra descriptions or drawing sketches [11, 31], clicking a few buttons is undeniably much more simple and efﬁcient. In practice, this means more rounds of search can be performed within a ﬁxed time bud- get. This is very beneﬁcial in the situation where the user does not have the exact target features in mind before the search (e.g. looking for a cup but does not have other details otherwise) and is forming the preference through browsing along the searching process. More rounds of search exposes the user with more candidates and thus better helps the user form the preference. Besides, click-feedback is also helpful in the case where the desirable feature is hard to describe in language, e.g. a speciﬁc shape or texture that is uncommon. To facilitate future work on studying how to better in- corporate click-feedback into retrieval, we construct a new benchmark based on Fashion200K dataset [14]. One chal- lenge of building the benchmark is that the feedback needs to be generated dynamically online based on the current re- trieval result, but it is not easy to have human-in-the-loop training in reality. We tackle the issue by approximating the human preference with a strong image encoder [15] and ﬁnd it work reasonably well in practice. We experiment with several methods that can utilize click-feedback to update re- trieval, and the result shows that the retrieval performance can be improved dramatically after incorporating the click- feedback. This validates the effectiveness of the proposed setting. As a summary, we make the following contributions in this work: • We study a previously less-explored form of feedback in the fashion image retrieval setting, where the feed- back is provided through users clicking groups of liked and disliked images after the initial search. • We introduce a new task named click-feedback re- trieval and construct a benchmark to facilitate future work in this direction. • We experiment with a training-free method to incor- porate click-feedback in retrieval and demonstrate sig- niﬁcant improvement of retrieval performance (R@10 being improved from 41.7% to 51.1%, and median rank being halved from 18 to 9), which shows the ef- fectiveness of the proposed setting. • We further propose methods to train the model with click-feedback, and show additional enhancement of performance over inference-only baseline (R@10 be- ing improved from 51.1% to 58.5%, and median rank being reduced from 9 to 5). 2. Related Work Text-image retrieval. Text-image retrieval has been ex- tensively studied by many researchers due to its high real- world application value. The scenario is to retrieve images of one modality with a given query text of another modal- ity. Existing methods calculate the similarity of each text- image pair by mapping the input of the two modalities to the same feature space. To extract text and image features, early works [9, 6, 16, 8, 41] mainly focus on visual seman- tic embedding with regard to data and the loss function re- spectively which provides high-efﬁciency baselines. Fur- ther, [21, 5, 7, 27] leverage cross-attention and self-adaptive approaches to explore the interaction between the text and image data deeply. After the feature extraction stage, some works propose aligning cross-modal features for better rep- resentations. [8, 33, 41] pay attention to global alignment while [21, 34] follow interest with local alignment. Be- yond the above, some works raise the retrieval efﬁciency by hash encoding [36, 39] and model compression [10, 17]. Recently, many researchers [24, 18, 23] have begun to de- sign different model architectures, which promote retrieval performance by a large margin. Some [24, 18, 23] design pre-training pretext tasks to obtain more discriminate fea- tures in an end-to-end manner. Others [37, 23] concentrate on increasing the scale of pre-training data which naturally boosts the downstream retrieval task. Retrieval with feedback. Since the correspondence be- tween text and image is full of diversity and uncertainty, it is often difﬁcult to obtain target image at one shot. Often times, addition feedback information is needed to adjust the retrieval results. To this end, many works have studied a variety of feedback methods, including using absolute at- tributes [40, 14, 1], relative attributes [25, 19, 38], attribute- like modiﬁcation text [32], and natural language [12, 13]. Other works also explored on using sketches or asking ques- tions [31, 3]. In this work, we study a different type of feed- back, which we call click-feedback, where the users provide feedback through clicking the liked and disliked images. It provides much convenience and efﬁciency compared to other feedback forms. Click-feedback retrieval resembles and draws inspirations from a classic line of works on rel- evance feedback [29, 42, 30]. However, we note that early works on it are done in the pre-deep learning era and there haven’t been much focus on it recently for image retrieval with deep neural networks [26].Text  Image database Top candidates Retrieval  model Target Positive   feedback Negative   feedback Feedback  Agent Final   output Contrastive  loss Ranking  loss Figure 2. Left: concrete implementation of the proposed click-feedback retrieval process. It consists of three steps. First, given input text description, the retrieval model searches the image database and generates initial retrieval. Second, top candidates from ﬁrst step is input to feedback agent, which, based on the similarity to the target, outputs a set of positive feedback images and a set of negative feedback images (simulating user clicking likes and dislikes). Finally, the retrieval model outputs the ﬁnal retrieval result based on the text and the click-feedback generated in the second step. Right: illustration of two type of losses for click-feedback-guided training. 3. Click-feedback Retrieval Retrieving target information is a fundamental operation people interact with Internet. In this section, we formalize this interaction and introduce our proposed setting of re- trieval with click-feedback. We will focus on the scenario of product search, where a user inputs a text description of the target product as query and the search engine returns a list of candidates in the form of images. But note that the underlying idea can be generalized to other scenarios with potentially different input-output format. Given a user text query q, the aim of search engine is to retrieve target image it from a large group of all can- didates Gall, where Gall .= {i1,i2,...,i n}, and it ∈ Gall. Essentially, the retrieval operation performs a rank- ing of elements in Gall based on q, i.e. it gives a rank for each image, rq(i), such that rq(i) ∈{1,2,...,n }and {rq(i1),rq(i2),...,r q(in)} = {1,2,...,n }(lower rank means better alignment with q). Under this notation, a suc- cessful retrieval would have rq(it) as small as possible. In practice, unsatisfactory searches are very common, where rq(it) is large (target product is not contained in the ﬁrst few returned pages). This could be due to vari- ous factors, and a frequent one is that the input query q is not speciﬁc enough such that too many candidates can be matched to it. Under such situation, additional information is needed to help retrieve the target product. Therefore pre- vious works have tackled on various ways to provide the necessary information through feedback, e.g. by adding ad- ditional language descriptions, sketches of target product, or asking questions [22, 31, 3]. In this work, we argue for a previously less explored setting where the feedback is in the form of clicking the likes and dislikes. The main beneﬁt of it is convenience, as a simple click is much easier than typing sentences or drawing pictures. Speciﬁcally speaking, after the initial retrieval, users can view the top-k retrieved products, and then select among them the ones Glike they like (containing desirable fea- tures and want to see more), and the ones Gdislike they dislike (containing undesirable features and want to see less). Formally, we deﬁne feedback f .= {Glike,Gdislike}. And the updated retrieval would generate a new ranking rq,f and the aim is to improve the search result such that rq,f (it) ≪rq(it) . The complete three-step process of the proposed click-feedback retrieval is summarized in the left part of Figure 2. Evaluation. We adopt the widely-used evaluation metrics in retrieval community [35, 15], i.e. R@K (recall at rank K, higher the better), median and mean rank (lower the better). Formally, R@K is deﬁned as the fraction of test instances where r(it) < K. Following previous works, we report R@1, R@5 and R@10. Median and mean rank are median and mean of r(it) among all test instances respectively.4. Methods In this section, we propose several methods that tackle the setting of click-feedback retrieval introduced in Sec- tion 3. They will serve as baselines for future works in this direction. Broadly, they can be divided into two categories, one without training and the other with training. 4.1. Training-free inference Given an input queryq, the language encoder El embeds qto a vector vq, and correspondingly the vision encoder Ev embeds image ito a vector vi in the same latent space, i.e. vq,vi ∈Rd. Then the retrieval rank of the image, r(i), is generated based on some measure of similarity S between vi and vq. Usually, the cosine similarity is used for its sim- plicity. Therefore, the ranking function for the setting with- out feedback is: RNF = S(vi,vq) (1) When click-feedback is available, the ranking function can be updated with, RF = S(vi,vq) +λpS(vi,Glike) −λnS(vi,Gdislike) (2) where the similarity between an image iwith a group of images G can be deﬁned as the average similarity between iand images in G: S(vi,G) = 1 |G| ∑ i′∈G S(vi,vi′ ) (3) Intuitively,RF up-weights a candidate image by its sim- ilarity to the images liked by the user, and down-weights with the similarity to those disliked. And the coefﬁcients λp and λn control the relative contribution of the positives and negatives. 4.2. Training methods The previous section introduces how to adapt an exist- ing model to incorporating click-feedback during inference. When the feedback is available during model development, additional training techniques can be utilized to further im- prove the performance. Speciﬁcally, we experiment with two different loss functions. Ranking loss. For ranking loss, we encourage the simi- larity between the target image and the positive feedback images (liked ones) to be larger than the similarity between the target image and the negative feedback images (disliked ones), Lr feedback = max(0,−S(vi,Glike) + S(vi,Gdislike) + m) (4) where mis a hyperparameter to control the margin of the separation. Contrastive loss. For contrastive loss, we encourage the distance between embeddings of the target image and matched feedback images to be small, and the distance between embeddings of the target image and mismatched feedback images to be large. Only positive feedback im- ages are used in the contrastive loss here, as empirically we ﬁnd contrasting away negative feedback images hurts the learned representation. Concretely, the loss is deﬁned as: Lit2if = −1 B B∑ j log exp (Ev(it j)T Ev(if j )/t) ∑B k=1 exp (Ev(it j)T Ev(if k)/t) Lif 2it = −1 B B∑ j log exp (Ev(if j )T Ev(it j)/t) ∑B k=1 exp (Ev(if j )T Ev(it k)/t) Lc feedback = 1 2(Lit2if + Lif 2it ) (5) where it is the target image, and if is the positive feed- back image. Text-image alignment. As the feedback losses men- tioned before only updates the image encoder, to avoid the learned image representation deviating too much from the text representation, a text-image alignment loss is also added during feedback training to keep image and text em- bedding aligned. Speciﬁcally, we use a constrastive loss similar to equation 5: Lt2i = −1 B B∑ j log exp (El(qj)T Ev(ij)/t)∑B k=1 exp (El(qj)T Ev(ik)/t) Li2t = −1 B B∑ j log exp (Ev(ij)T El(qj)/t)∑B k=1 exp (Ev(ij)T El(qk)/t) (6) Lti−align = 1 2(Lt2i + Li2t) The total loss for training with click-feedback is then: Lall = Lfeedback + Lti−align (7) After training, equation 2 is used as the ﬁnal ranking function during inference as before.5. Experiments In this section, we ﬁrst introduce the concrete setup of a benchmark for retrieval with click-feedback, including the dataset and how click-feedback is generated. Then we elab- orate on the implementation details on model architecture and training. Finally, experiment results are shown with de- tailed analysis on the effectiveness of the proposed setting. 5.1. Experimental setup Benchmark. We build our retrieval with click-feedback benchmark upon the Fashion200K dataset [14], which is a large-scale dataset containing more than 200,000 clothing images spanning across ﬁve major fashion categories (dress, top, pants, skirt and jacket) with various styles. The dataset comes with different types of annotations including detailed product information and bounding boxes. We only use the images and corresponding attribute-based text descriptions (e.g. “black roll-up sleeve blouse”) for our experiments. Click-feedback. Ideally the feedback of likes and dis- likes should be provided by human to simulate the real use case. However, in practice it is hard to train models with hu- man in-the-loop, especially considering the training process can easily contain hundreds thousands of iterations. There- fore, to make training feasible, we need other ways to sim- ulate the feedback process automatically without human in- volvement. This boils down to generating the similarities between the candidate images and the target image. One way to obtain this is to utilize a good image encoder net- work and compute the similarities in its latent space. An- other way is to approximate the image similarities with the similarities of the corresponding text annotations ( e.g. cal- culating the intersection-over-union of the ground-truth at- tributes). In this work, we utilize the former approach as we ﬁnd empirically that the generated similarities are more ﬁne-grained using the dense representation from an image encoder. Implementation details. To approximate human feed- back, we use the FashionViL model proposed by Han et al. [15]. FashionViL is a fashion-focused vision-language model with speciﬁc designs that fully exploit the specialties in fashion domain. We utilize the model released by the au- thors that was pretrained with over 1.35 million image-text pairs from several public fashion-related datasets, includ- ing Fashion200K, the dataset we build our benchmark on. The model is only used to simulate human preference and is not modiﬁed or used for retrieval in the experiments. Con- cretely, after the initial retrieval, FashionViL model com- putes the similarities between the top ten retrieved images and the target image, and outputs the most similar one to target image as Glike and the least similar one as Gdislike. For the retrieval model, we utilize CLIP [28], which has been used as initialization for many vision-language tasks recently due to its great transferability. Since CLIP is not designed for fashion product retrieval, we ﬁrst ﬁne- tune it on Fashion200K dataset to have a better starting point (avoiding the situation where no relevant images are among top ten after the initial retrieval for a reasonable feedback). Speciﬁcally, we use the publicly available ‘ViT- B/32’ model and ﬁnetune it on Fashion200K training set for 30 epochs with the loss of equation 6. AdamW optimizer is used with a cosine learning rate scheduler with max learn- ing rate of 3e-6 and a linear warm up of 5 epochs. After ﬁnetuning, the median rank (lower the better) on test set de- creases from 135 to 18. With this as initial point, we train the model with click- feedback for another 30 epochs using the feedback loss of equation 7. The margin m is set to 0.2 for ranking loss and the same optimizer and scheduler conﬁguration is used. During test, we use equation 2 to rank all candidate images based on the input text description as well as the feedback given after the initial retrieval. We set λp as 1.0 and λn as 0.5 to give a higher weight to positive feedback. 5.2. Results without training The inference-only entry in Table 1 shows the result when adding additional feedback of liked and disliked im- ages only during inference. Compared to baseline (the ini- tial retrieval result without feedback), there is a dramatic enhancement in performance. The median rank is halved from 18 to 9, and the mean rank is decreased from 173.0 to 155.2. The R@10 is improved by an absolute of 9.3% (from 41.7% to 51.0%). Note that there is even larger improve- ment of R@1 and R@5 (from 13.8% to 41.7% for R@1 and from 31.7% to 46.3% for R@5). However, that increase is mainly contributed by the instances where the target image is retrieved as top 10 during the initial retrieval, resulting the positive feedback image to be the target image itself. So we will mostly focus on the improvement of R@10, me- dian rank and mean rank, and include R@1 and R@5 here for completeness. Inﬂuence of positive and negative feedback. Table 2 shows how positive and negative feedback contributes to the overall improvement by varying the coefﬁcients λp and λn in equation 2. The second row (λp = 1.0, λn = 0.0) shows the result when only liked images are used as feedback to update retrieval. As can be seen, the positive feedback ac- counts for most of the improvement. R@10 is improved from 41.7% to 49.4%, and median rank is improved from 18 to 11, compared to R@10 of 51.1% and median rank of 9 when using full feedback. The third row ( λp = 0.0, λn = 0.1) shows the result with only disliked images usedMethod Feedback Training R@1 ↑ R@5 ↑ R@10 ↑ MedR ↓ MeanR ↓ Baseline \u0017 \u0017 13.8 31.7 41.7 18 173.0 Inference-only \u0013 \u0017 41.7 46.3 51.1 9 155.2 Contrastive loss \u0013 \u0013 41.1 47.7 53.8 7 104.5 Contrastive loss + SepEnc \u0013 \u0013 41.1 47.6 55.2 7 79.9 Ranking loss \u0013 \u0013 43.1 48.8 54.2 6 95.7 Ranking loss + SepEnc \u0013 \u0013 39.1 50.6 58.5 5 70.5 Table 1. Performance of different methods on Fashion200K [14] retrieval task. Baseline is a CLIP [28] model ﬁnetuned on the dataset without feedback. Inference-only is the baseline model with click-feedback added during test.Contrastive loss and Ranking loss use click- feedback as additional supervision during training. Contrastive loss+SepEnc and Ranking loss+SepEnc use two separate image encoders, one for computing cross-modal text-to-image similarity, and the other one for computing unimodal image-to-image similarity. λp λn R@10 ↑ MedR ↓ MeanR ↓ 0.0 0.0 41.7 18 173.0 1.0 0.0 49.4 11 148.6 0.0 0.1 43.6 16 170.5 1.0 0.5 51.1 9 155.2 Table 2. Inﬂuence of positive and negative feedback for retrieval performance. λp and λn are coefﬁcients in equation 2. Positive feedback introduces more improvement over negative feedback, but the two are complementary to each other and give the best performance when combined together. as feedback 1. It manages to introduce improvement over baseline, improving R@10 from 41.7% to 43.6% and mei- dan rank from 18 to 16. But the enhancement of perfor- mance is relative small compared to using only positive feedback. This shows the positive examples are relatively more effective in helping the retrieval. Intuitively, this is because positive feedback provides a more direct guidance. Despite this, the negative feedback is still useful as the im- provement it introduces is complementary to that of positive feedback, as can be seen from the last row of Table 2. 5.3. Results with training While using feedback only during inference has already introduced much improvement, additional increase in per- formance is achieved using feedback-based training. As shown in Table 1, feedback-guided training with either con- trastive loss or ranking loss can boost the performance. Concretely, Contrastive loss helps increasing R@10 from 51.1% to 53.8%, improving median rank from 9 to 7, and mean rank from 155.2 to 104.5. Ranking loss provides even larger improvement, which increases R@10 by 3.1% (from 1Note that here a smaller number forλn is used to further down-weight the contribution of negative feedback in equation 2, as otherwise the totally irrelevant images would be given a high score (as they are most dissimilar to the negative feedback images) and dominate the retrieval (R@10 is 2.7% and median rank is 1005 when using λp = 0.0, λn = 1.0). 51.1% to 54.2 %), and reduces median rank and mean rank from 9 to 6 and from 155.2 to 95.7 respectively. This vali- dates the effectiveness of the proposed training with click- feedback. We assume the reason why Ranking loss works better than Contrastive loss is that it utilizes both positive and negative feedback images, while Contrastive loss only utilizes the positive ones (we experimented on using nega- tive feedback with contrastive loss as well but that fails to introduce improvement, as contrasting away negative feed- back tends to hurt the learned representation). This again shows the unique value provided by the negative feedback, as it provides complementary information to the positive feedback. Empirically, we ﬁnd that further improvement can be obtained by using separate image encoders for calculat- ing cross-modal text-to-image similarity (the S(vi,vq) in equation 2) and unimodal image-to-image similarity (the S(vi,Glike) and S(vi,Gdislike) in equation 2). Concretely, Contrastive loss+SepEnc improves overContrastive loss on R@10 for 1.4 points (from 53.8% to 55.2%) and largely re- duces mean rank (from 104.5 to 79.9). Similarly, Ranking loss+SepEnc enhances R@10 by an additional 4.3 points (from 54.2% to 58.5%), and reduces median rank to 5 and mean rank to 70.5 (from 95.7). We assume the reason why using separate encoders helps might be that there is some inherent differences between multi-modal embedding space and uni-modal embedding space, which is hard to reconcile within one model (at least for the model used in our experiment). Therefore, using separate encoders avoids the interference when trying to capture both text-to-image similarity and image-to-image similarity. And the reason why Ranking loss enjoys more improvement than Contrastive loss after using separate en- coders could be that it provides separation of two different type of loss (ranking and contrastive), as the text-to-image alignment uses contrastive loss as well. However, note that using separate encoders comes at the cost of increased pa- rameters and computation cost. We leave to the future work for addressing the issue.nlike ndislike R@10 ↑ MedR ↓ MeanR ↓ 1 1 51.1 9 155.2 2 2 51.1 10 137.2 3 3 50.3 10 134.3 4 4 49.3 11 135.3 5 5 48.3 12 139.1 5 1 47.5 13 146.9 1 5 51.5 9 150.3 Table 3. Performance of Inference-only model with varying num- ber of feedback instances. 5.4. Additional experiments Number of feedback instances. For all the experiments shown previously, the feedback agent provides only one positive image and one negative image. Here we change that assumption and vary the number of images provided as feedback. The result is shown in Table 3, where nlike is the number of positive feedback images inGlike and ndislike is the number of negative feedback images in Gdislike. From ﬁrst ﬁve rows of the table (number of positive/negative feed- back images increasing from one to ﬁve), increasing the number of feedback instances doesn’t help in this case. While the performance on mean rank increases, the perfor- mance of R@10 and median rank drops. The last two rows show that the performance drop of R@10 and median rank mainly comes from the increasing of positive feedback in- stances. We ﬁnd that this is because the false positives, as simply choosing top ﬁve most similar images as positive feedback could include images that are in fact not similar to the target image. Therefore, we keep nlike and ndislike to be one in all other experiments. Adding diversity for feedback candidates. In previous experiments, the top ten images based on the similarity to input text description are given to the feedback agent as candidates for generating the feedback Glike and Gdislike. However, it is not required to only use this criterion, and choosing which set of images to ask for feedback is a de- sign choice that can be changed. Here we experiment with a heuristic that tries to increase the visual diversity of the candidate images. The intuition is that this could avoid the situation where the top retrieved images are too simi- lar to each other and the feedback based on them doesn’t provide enough information. Speciﬁcally, we utilize an it- erative method to select candidate images ({ic}): ic n = argmaxi (S(vi,vq) −λdiversityS(vi,{ic 1,ic 2,...,i c n−1})) (8) λdiversity R@10 ↑ MedR ↓ MeanR ↓ 0.0 51.1 9 155.2 0.2 51.2 9 151.5 0.4 50.9 9 151.6 0.6 50.0 10 153.9 0.8 48.8 12 151.3 1.0 47.1 13 155.8 Table 4. Performance of Inference-only model with a diversity heuristic to select visually-different images as candidates to re- ceive feedback. which down-weights the images which are too similar to those already selected, and λdiversity here controls the degree of the diversity. Table 4 shows the results with λdiversity ranging from 0.2 to 1. Here the added diversity doesn’t help on the performance. We ﬁnd that this is be- cause for Fashion200k and the CLIP model used, the can- didate set through naive selection of top ten images after initial retrieval is already very diverse. The additional di- versity heuristic described by equation 8 doesn’t add more on the diversity side but decreases the quality of the candi- dates. However, we note that this might help in practice as the web is full of very similar and often identical products. 6. Limitations and future work We believe the task of retrieval with click-feedback holds great promise on improving the efﬁciency of retrieval and the overall user satisfaction in real-world use cases. We have demonstrated such penitential through various experi- ments introduced in previous sections. Here we list several promising directions to explore in the future. In this work, we only focus on one round of feedback for simplicity. It is natural to extend the task to retrieval with multiple rounds of click-feedback. This better approximates the real-world searching scenarios and introduces the inter- esting challenge of how to handle the long history of inter- actions, which is not covered in the single-round feedback setting studied in this paper. For feedback-guided training, it would be interesting to explore how to utilize reinforcement learning to directly su- pervise the retrieval model using the ﬁnal groundtruth rank as reward. Concretely, the reword would encourage the ﬁnal groundtruth rank after the feedback to be as small as possi- ble. This is a more intuitive supervision as that directly op- timizes the real target of the retrieval. Besides, it brings an- other beneﬁt of supervising the three steps (initial retrieval, feedback generation, retrieval update) as a whole. In theory, this could lead to a better policy on choosing which set of candidates for receiving feedback (instead of always choos- ing the top retrieved instances after initial retrieval or usingMulticolor zephyr broderie  anglaisecotton silk-blend  tapered pants  668 5 Blue high neck evening  black dress high low maxi 411 2 Blue craft fair ﬂair top 243 4 Belted textured pencil skirt 136 4 Black seamed sheath dress 16 4 Gray slouchy longline blazer 111 350 Target image  and description Top-10 retrieved images and click-feedback GT rank Initial After Figure 3. Qualitative examples. First ﬁve rows show how click-feedback helps retrieving target image to top-ten by utilizing the rich visual information contained in it. Notice how it drastically improves the performance when the target is ranked way back in the initial retrieval (ﬁrst few rows). The last row shows a typical failure case where the performance degrades with feedback. It can be attributed to poor initial retrieval, where the feedback agent provides a dissimilar image as positive feedback. some manually-designed heuristics as we explored in sec- tion 5.4). However, we note that it is not straightforward on how to properly train with reinforcement learning to super- vise the whole three-step click-feedback retrieval process using the ﬁnal retrieval rank as reward. We leave to future work for advancing in this direction. Finally, to avoid human-in-the-loop training, we utilized a strong image encoder to approximate the human prefer- ence. While we found it work well in practice, it could be imperfect at times and provide incorrect feedback. Fur- thermore, it is not easy to quantitatively evaluate how much noise it introduces. Therefore, it would be of interests to explore other methods to generate click-feedback that bet- ter capture human judgement. 7. Conclusion As a summary, in this work we study how to help users retrieving target information efﬁciently during search. In this regard, we focus on a previously less-explored setting where the user provides feedback through clicking a set of liked and disliked images after seeing the initial retrieval results. We proposed a new task termed click-feedback retrieval and built a large-scale benchmark in the fashion product retrieval domain around it. We introduced several methods to incorporate click-feedback and demonstrated that the retrieval performance can be improved signiﬁcantly. We believe further efforts on the task would greatly beneﬁt the ﬁeld and help building more efﬁcient and user-friendly search engines for real-world applications.References [1] Kenan E Ak, Ashraf A Kassim, Joo Hwee Lim, and Jo Yew Tham. Learning attribute representations with localization for ﬂexible fashion search. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 7708–7717, 2018. 1, 2 [2] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Al- berto Del Bimbo. Effective conditioned and composed im- age retrieval combining clip-based features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 21466–21474, 2022. 1 [3] Guanyu Cai, Jun Zhang, Xinyang Jiang, Yifei Gong, Lianghua He, Fufu Yu, Pai Peng, Xiaowei Guo, Feiyue Huang, and Xing Sun. Ask&conﬁrm: active detail enriching for cross-modal retrieval with partial query. In Proceedings of the IEEE/CVF International Conference on Computer Vi- sion, pages 1835–1844, 2021. 1, 2, 3 [4] Yanbei Chen and Loris Bazzani. Learning joint visual se- mantic matching embeddings for language-guided retrieval. In Computer Vision–ECCV 2020: 16th European Confer- ence, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXII 16, pages 136–152. Springer, 2020. 1 [5] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXX , pages 104–120. Springer, 2020. 2 [6] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. Probabilistic embeddings for cross-modal retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8415–8424, 2021. 2 [7] Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji Zhang, Meng Wang, and Jun Yu. Rosita: Enhancing vision- and-language semantic alignments via cross-and intra-modal knowledge integration. In Proceedings of the 29th ACM International Conference on Multimedia , pages 797–806, 2021. 2 [8] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. Vse++: Improving visual-semantic embeddings with hard negatives. arXiv preprint arXiv:1707.05612, 2017. 2 [9] A Frome, GS Corrado, J Shlens, et al. A deep visual- semantic embedding model. Proceedings of the Advances in Neural Information Processing Systems , pages 2121–2129, 2013. 2 [10] Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang Wang, Jingjing Liu, Lijuan Wang, and Zicheng Liu. Playing lottery tickets with vision and lan- guage. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 36, pages 652–660, 2022. 2 [11] Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal, Rakesh Chada, Yue Wu, Varsha Hedau, and Pradeep Natarajan. Fashionvlp: Vision language transformer for fashion re- trieval with feedback. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 14105–14115, 2022. 2 [12] Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, and Rogerio Feris. Dialog-based interactive image retrieval. Advances in neural information processing sys- tems, 31, 2018. 1, 2 [13] Xiaoxiao Guo, Hui Wu, Yupeng Gao, Steven Rennie, and Rogerio Feris. The fashion iq dataset: Retrieving images by combining side information and relative natural language feedback. arXiv preprint arXiv:1905.12794, 1(2):7, 2019. 1, 2 [14] Xintong Han, Zuxuan Wu, Phoenix X Huang, Xiao Zhang, Menglong Zhu, Yuan Li, Yang Zhao, and Larry S Davis. Au- tomatic spatially-aware fashion concept discovery. In Pro- ceedings of the IEEE international conference on computer vision, pages 1463–1471, 2017. 1, 2, 5, 6 [15] Xiao Han, Licheng Yu, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and Tao Xiang. Fashionvil: Fashion-focused vision-and- language representation learning. In ECCV, 2022. 1, 2, 3, 5 [16] Peng Hu, Xi Peng, Hongyuan Zhu, Liangli Zhen, and Jie Lin. Learning cross-modal retrieval with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 5403–5413, 2021. 2 [17] Z Huang, Z Zeng, B Liu, D Fu, and J Pixel-BERT Fu. Align- ing image pixels with text by deep multi-modal transformers. arxiv 2020. arXiv preprint arXiv:2004.00849. 2 [18] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision- and-language transformer without convolution or region su- pervision. In International Conference on Machine Learn- ing, pages 5583–5594. PMLR, 2021. 2 [19] Adriana Kovashka, Devi Parikh, and Kristen Grauman. Whittlesearch: Image search with relative attribute feedback. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2973–2980. IEEE, 2012. 1, 2 [20] Zhanghui Kuang, Yiming Gao, Guanbin Li, Ping Luo, Yimin Chen, Liang Lin, and Wayne Zhang. Fashion retrieval via graph reasoning networks on a similarity pyramid. In Pro- ceedings of the IEEE/CVF international conference on com- puter vision, pages 3066–3075, 2019. 1 [21] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xi- aodong He. Stacked cross attention for image-text matching. In Proceedings of the European conference on computer vi- sion (ECCV), pages 201–216, 2018. 2 [22] Seungmin Lee, Dongwan Kim, and Bohyung Han. Cosmo: Content-style modulation for image retrieval with text feed- back. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 802–812, 2021. 1, 3 [23] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learn- ing with momentum distillation. Advances in neural infor- mation processing systems, 34:9694–9705, 2021. 2 [24] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural information processing systems, 32, 2019. 2[25] Devi Parikh and Kristen Grauman. Relative attributes. In 2011 International Conference on Computer Vision , pages 503–510. IEEE, 2011. 2 [26] Lorenzo Putzu, Luca Piras, and Giorgio Giacinto. Convo- lutional neural networks for relevance feedback in content based image retrieval: A content based image retrieval sys- tem that exploits convolutional neural networks both for fea- ture extraction and for relevance feedback.Multimedia Tools and Applications, 79:26995–27021, 2020. 2 [27] Leigang Qu, Meng Liu, Jianlong Wu, Zan Gao, and Liqiang Nie. Dynamic modality interaction modeling for image-text retrieval. In Proceedings of the 44th International ACM SI- GIR Conference on Research and Development in Informa- tion Retrieval, pages 1104–1113, 2021. 2 [28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn- ing transferable visual models from natural language super- vision. In ICML, 2021. 5, 6 [29] Yong Rui, Thomas S Huang, Michael Ortega, and Sharad Mehrotra. Relevance feedback: a power tool for interactive content-based image retrieval. IEEE Transactions on circuits and systems for video technology, 8(5):644–655, 1998. 2 [30] Ian Ruthven and Mounia Lalmas. A survey on the use of rel- evance feedback for information access systems.The Knowl- edge Engineering Review, 18(2):95–145, 2003. 2 [31] Patsorn Sangkloy, Wittawat Jitkrittum, Diyi Yang, and James Hays. A sketch is worth a thousand words: Image retrieval with text and sketch. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXXVIII, pages 251–267. Springer, 2022. 1, 2, 3 [32] Nam V o, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing text and image for image retrieval-an empirical odyssey. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6439–6448, 2019. 1, 2 [33] Liwei Wang, Yin Li, Jing Huang, and Svetlana Lazebnik. Learning two-branch neural networks for image-text match- ing tasks. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence, 41(2):394–407, 2018. 2 [34] Zihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, and Jing Shao. Camp: Cross-modal adaptive message passing for text-image retrieval. In Pro- ceedings of the IEEE/CVF international conference on com- puter vision, pages 5764–5773, 2019. 2 [35] Zeyu Wang, Yu Wu, Karthik Narasimhan, and Olga Rus- sakovsky. Multi-query video retrieval. In Computer Vision– ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XIV , pages 233– 249. Springer, 2022. 3 [36] Erkun Yang, Cheng Deng, Wei Liu, Xianglong Liu, Dacheng Tao, and Xinbo Gao. Pairwise relationship guided deep hash- ing for cross-modal retrieval. In proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 31, 2017. 2 [37] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: ﬁne-grained interactive language-image pre-training. arXiv preprint arXiv:2111.07783, 2021. 2 [38] Aron Yu and Kristen Grauman. Thinking outside the pool: Active training image creation for relative attributes. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 708–718, 2019. 2 [39] Xi Zhang, Hanjiang Lai, and Jiashi Feng. Attention-aware deep adversarial hashing for cross-modal retrieval. In Pro- ceedings of the European conference on computer vision (ECCV), pages 591–606, 2018. 2 [40] Bo Zhao, Jiashi Feng, Xiao Wu, and Shuicheng Yan. Memory-augmented attribute manipulation networks for in- teractive fashion search. In Proceedings of the IEEE con- ference on Computer Vision and Pattern Recognition, pages 1520–1528, 2017. 2 [41] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang, Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional image-text embeddings with instance loss. ACM Transac- tions on Multimedia Computing, Communications, and Ap- plications (TOMM), 16(2):1–23, 2020. 2 [42] Xiang Sean Zhou and Thomas S Huang. Relevance feedback in image retrieval: A comprehensive review.Multimedia sys- tems, 8:536–544, 2003. 2",
      "references": [
        "Learning attribute representations with localization for flexible fashion search",
        "Effective conditioned and composed image retrieval combining clip-based features",
        "Ask&confirm: active detail enriching for cross-modal retrieval with partial query",
        "Learning joint visual semantic matching embeddings for language-guided retrieval",
        "Uniter: Universal image-text representation learning",
        "Probabilistic embeddings for cross-modal retrieval",
        "Rosita: Enhancing vision-and-language semantic alignments via cross-and intra-modal knowledge integration",
        "Vse++: Improving visual-semantic embeddings with hard negatives",
        "A deep visual-semantic embedding model",
        "Playing lottery tickets with vision and language",
        "Fashionvlp: Vision language transformer for fashion retrieval with feedback",
        "Dialog-based interactive image retrieval",
        "The fashion iq dataset: Retrieving images by combining side information and relative natural language feedback",
        "Automatic spatially-aware fashion concept discovery",
        "Fashionvil: Fashion-focused vision-and-language representation learning",
        "Learning cross-modal retrieval with noisy labels",
        "Aligning image pixels with text by deep multi-modal transformers",
        "Vilt: Vision-and-language transformer without convolution or region supervision",
        "Whittlesearch: Image search with relative attribute feedback",
        "Fashion retrieval via graph reasoning networks on a similarity pyramid",
        "Stacked cross attention for image-text matching",
        "Cosmo: Content-style modulation for image retrieval with text feedback",
        "Align before fuse: Vision and language representation learning with momentum distillation",
        "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
        "Relative attributes",
        "Convolutional neural networks for relevance feedback in content based image retrieval: A content based image retrieval system that exploits convolutional neural networks both for feature extraction and for relevance feedback",
        "Dynamic modality interaction modeling for image-text retrieval",
        "Learning transferable visual models from natural language supervision",
        "Relevance feedback: a power tool for interactive content-based image retrieval",
        "A survey on the use of relevance feedback for information access systems",
        "A sketch is worth a thousand words: Image retrieval with text and sketch",
        "Composing text and image for image retrieval-an empirical odyssey",
        "Learning two-branch neural networks for image-text matching tasks",
        "Camp: Cross-modal adaptive message passing for text-image retrieval",
        "Multi-query video retrieval",
        "Pairwise relationship guided deep hashing for cross-modal retrieval",
        "Filip: fine-grained interactive language-image pre-training",
        "Thinking outside the pool: Active training image creation for relative attributes",
        "Attention-aware deep adversarial hashing for cross-modal retrieval",
        "Memory-augmented attribute manipulation networks for interactive fashion search",
        "Dual-path convolutional image-text embeddings with instance loss",
        "Relevance feedback in image retrieval: A comprehensive review"
      ],
      "meta_data": {
        "arxiv_id": "2305.00052v1",
        "authors": [
          "Zeyu Wang",
          "Yu Wu"
        ],
        "published_date": "2023-04-28T19:03:03Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper studies click-feedback in the fashion image retrieval setting, where user feedback is provided by clicking liked and disliked images after an initial search. It introduces a new task called 'click-feedback retrieval' and constructs a new benchmark for it based on the Fashion200K dataset. The work demonstrates that incorporating click-feedback significantly improves retrieval performance, both with training-free inference methods and through click-feedback-guided training, achieving substantial gains in metrics like R@10 (from 41.7% to 58.5%) and median rank (halved from 18 to 5).",
        "methodology": "The proposed methods fall into two categories: training-free inference and training-based approaches. For training-free inference, the ranking function is updated by incorporating similarities to liked images (up-weighting) and disliked images (down-weighting) using cosine similarity. For training, two loss functions are explored: a Ranking loss, which encourages higher similarity between target and liked images than between target and disliked images, and a Contrastive loss, which aims to minimize distance to positive feedback images and maximize distance to mismatched ones (only positive used empirically). A text-image alignment loss is also used during training to maintain embedding alignment. Further performance enhancement is achieved by using separate image encoders for cross-modal text-to-image similarity and unimodal image-to-image similarity.",
        "experimental_setup": "The benchmark is built upon the Fashion200K dataset, comprising over 200,000 clothing images with attribute-based text descriptions. Click-feedback is simulated automatically by using the FashionViL model to approximate human preference; it selects the most similar image to the target as 'liked' and the least similar as 'disliked' from the top ten initial retrieval results. The retrieval model used is CLIP (ViT-B/32), initially fine-tuned on Fashion200K for 30 epochs with a text-image alignment loss, and then further trained for 30 epochs with the proposed click-feedback losses. AdamW optimizer with a cosine learning rate scheduler (max LR 3e-6, 5-epoch linear warm-up) is employed. Evaluation metrics include R@K (R@1, R@5, R@10), median rank (MedR), and mean rank (MeanR).",
        "limitations": "The current work focuses on a single round of feedback, which simplifies real-world multi-round search scenarios. The click-feedback generation relies on approximating human preference using a strong image encoder (FashionViL), which may introduce imperfections and noise, and its quantitative impact is hard to evaluate. Additionally, the use of separate image encoders for cross-modal and unimodal similarities, while improving performance, leads to increased model parameters and computational costs. A diversity heuristic for selecting feedback candidates did not yield performance improvements in their specific experimental setup.",
        "future_research_directions": "Future work includes extending the task to incorporate multiple rounds of click-feedback to better simulate real-world interactions and address the challenge of handling long interaction histories. Another promising direction is to explore reinforcement learning to directly supervise the retrieval model with the final groundtruth rank as a reward, which would optimize the true retrieval objective and allow for joint optimization of the initial retrieval, feedback generation, and retrieval update steps. Finally, investigating alternative methods for generating click-feedback that more accurately capture human judgment, beyond relying on strong image encoders for approximation, is suggested.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models",
      "full_text": "MNRAS000, 1–12 (2024) Preprint 14 February 2024 Compiled using MNRAS L ATEX style file v3.0 Cosmological evolution of metallicity correlation functions from the Auriga simulations Zefeng Li ,1,2,3★ Robert, J. J. Grand,4 Emily Wisnioski ,1,2 J. Trevor Mendel ,1,2 Mark R. Krumholz ,1,2 Yuan-Sen Ting ,1,2,5 Ruediger Pakmor ,6 Facundo A. Gómez ,7,8 Federico Marinacci 9,10 and Ioana Ciucă 1,2,5 1Research School of Astronomy & Astrophysics, Australian National University, Weston Creek, ACT 2611, Australia 2ARC Centre of Excellence for All Sky Astrophysics in 3 Dimensions (ASTRO 3D), Canberra, ACT 2611, Australia 3Centre for Extragalactic Astronomy, Department of Physics, Durham University, South Road, Durham DH1 3LE, UK 4Astrophysics Research Institute, Liverpool John Moores University, 146 Brownlow Hill, Liverpool L3 5RF, UK 5Research School of Computing, Australian National University, Acton, ACT 2612, Australia 6Max-Planck-Institut für Astrophysik, Karl-Schwarzschild-Str 1, D-85748 Garching, Germany 7Departamento de Astronomía, Universidad de La Serena, Av. Juan Cisternas 1200 Norte, La Serena, Chile 8Instituto Multidisciplinario de Investigación y Postgrado, Universidad de La Serena, Av. Raúl Bitrán 1305, La Serena, Chile 9Department of Physics & Astronomy ‘Augusto Righi’, University of Bologna, Via Gobetti 93/2, I-40129 Bologna, Italy 10INAF, Osservatorio di Astrofisica e Scienza dello Spazio Bologna, Via Gobetti 93/3, I-40129 Bologna, Italy Accepted XXX. Received YYY; in original form ZZZ ABSTRACT We study the cosmological evolution of the two-point correlation functions of galactic gas-phase metal distributions using the 28 simulated galaxies from the Auriga Project. Using mock observations of the𝑧 = 0 snapshots to mimic our past work, we show that the correlation functions of the simulated mock observations are well matched to the correlation functions measured fromlocalgalaxysurveys.Thiscomparisonsuggeststhatthesimulationscapturetheprocessesimportantfordeterminingmetal correlationlengths,thekeyparameterinmetallicitycorrelationfunctions.Weinvestigatetheevolutionofmetallicitycorrelations over cosmic time using the true simulation data, showing that individual galaxies undergo no significant systematic evolution in their metal correlation functions from𝑧∼3 to today. In addition, the fluctuations in metal correlation length are correlated with but lag ahead fluctuations in star formation rate. This suggests that re-arrangement of metals within galaxies occurs at a higher cadence than star formation activity, and is more sensitive to the changes of environment, such as galaxy mergers, gas inflows / outflows, and fly-bys. Key words:galaxies: abundances – galaxies: evolution – galaxies: ISM. 1 INTRODUCTION Understanding the baryonic processes responsible for metal (ele- ments heavier than hydrogen and helium) transportation is crucial in galaxy evolution. Metals can be found in stars, their birth places wheretheyaresynthesizedthroughnucleosynthesis;theycanalsobe found in interstellar medium (ISM), into which they are injected by stars through various mechanisms including supernova explosions andstellarwinds.OnceintheISM,metalsmixwiththesurrounding gasanddiffuse,leadingtochangesintheoverallmetallicitydistribu- tion within galaxies, participating in next-generation star formation, and thus forming a crucial part of the baryon cycle (Tinsley 1980). Both observers and theorists have studied this process. On the observational side, gas-phase metallicity is most commonly mea- sured by the abundance of oxygen, the most abundant metal in the Universe. The invention of integrated field units (IFUs, e.g. Croom ★ E-mail: zefeng.li@anu.edu.au et al. 2012; Sánchez et al. 2012; Bundy et al. 2015; Erroz-Ferrer et al. 2019) has made it possible to measure the spatial distribution of oxygen abundance in nearby galaxies (e.g. Rosales-Ortega et al. 2011; Sánchez-Menguiano et al. 2016). IFU studies have revealed that metallicity gradients are ubiquitous and that their steepness is correlated with other galaxy properties such as stellar mass (e.g. Belfioreetal.2017;Hoetal.2018;Poetrodjojoetal.2018;Sánchez- Menguiano et al. 2018; Kreckel et al. 2019). On the theoretical side, numerical models have begun to explore the origin and evolution of metallicity gradients (e.g. Di Matteo et al. 2009; Ma et al. 2017; Sharda et al. 2021; Tissera et al. 2022), but have also gone beyond simply1Dstatisticssuchasgradientstoexaminechemicalmixingin a broader view. The models contain various mechanisms, including bar driven mixing (Di Matteo et al. 2013), spiral-arm driven mixing (Grand et al. 2016; Orr et al. 2023), thermal instabilities (Yang & Krumholz 2012), gravitational instabilities (Petit et al. 2015), cos- mological accretion (Ceverino et al. 2016), and supernova-driven turbulence (de Avillez & Mac Low 2002; Colbrook et al. 2017). © 2024 The Authors arXiv:2402.08632v1  [astro-ph.GA]  13 Feb 20242 Z. Li et al. Exploring beyond simple metallicity gradients requires new sta- tisticaltoolstocharacterise2Dmetallicitymaps.Oneofthesimplest and most straightforward to measure from observations is the two- pointcorrelationfunction.Two-pointcorrelationsareabletoprovide unique information of ISM turbulence and chemical mixing by de- coding metal fields of galaxies. Krumholz & Ting (2018, hereafter KT18) propose a model to predict this quantity that considers metal injectionanddiffusion.Thispredictionhasinspiredanumberofob- servationalstudies(Kreckeletal.2020;Methaetal.2021;Williams etal.2022)thatmeasurethetwo-pointcorrelations(orclosely-related statistics)ofPHANGS-MUSEgalaxies.Whilemuchofthisefforthas focused on very nearby galaxies observed at extremely high resolu- tion, more recently, Li et al. (2021, 2023) extended the method to larger but more distant galaxy samples in CALIFA (Sánchez et al. 2012)andAMUSING++(López-Cobáetal.2020),respectively,and demonstrateditsrobustnessagainstbeamsmearing,choiceofmetal- licity diagnostic, and binning used to remove the overall metallicity gradient.Thesamplesinthesestudiesarelargeenoughtopermitfor thefirsttimeanexaminationofstatisticaldistributionsofmetallicity correlationfunctionsacrossthelocalgalaxypopulation,andthecor- relations between metal distributions and other galactic properties. Most strikingly, these studies have uncovered a robust correlation betweenthegalaxies’metallicitycorrelationlengths–thecharacter- istic size scales of their metallicity correlations after removing the large-scale gradient – and their other bulk properties such as stellar mass, size, and star formation rate. This discovery raises immediate questions:howandwheninthecourseofgalaxyevolutionwerethese relationships established? Have they varied over cosmic time? While there has been extensive work on the cosmological evolu- tionofmetallicitygradientsusingcosmologicalzoom-insimulations (e.g.DiMatteoetal.2009;Torreyetal.2012;Maetal.2017;Bellar- dini et al. 2021; Metha & Trenti 2023), there has yet to be a similar study of higher-order metallicity statistics such as the metallicity correlation function, despite the growing body of observational lit- erature. In this paper we present a pioneering study aimed at filling this gap. The correlation function is of interest because it encodes the fundamental physics of chemical mixing in the ISM, the process responsiblefordistributingmetalsfromtheirbirthsites.Thisprocess in turn depends on the evolution of ISM turbulence across cosmic time, another process of great interest. Our specific aims are to (1) determineifthesimulationsareabletoreproducethedistributionof two-pointcorrelationfunctionsfoundinobservedlocalgalaxies,(2) usethesimulationstostudythecosmologicalevolutionoftwo-point correlationssothatwecanunderstandwhenandhowtheyareestab- lished, and (3) guide future observational work aimed at measuring metallicity correlations beyond the local Universe. This paper is organized as follows. In Section 2, we describe the Aurigasimulationsandtheproceduresbywhichweextractmetallic- ity maps from them. In Section 3, we discuss the pipeline of mock observations we use to test whether the simulations are consistent with local galaxy observations. In Section 4, we describe our find- ings regarding the cosmological evolution and origin of metallicity correlations. Finally, we discuss and summarise our work in Sec- tion 5. 2 SIMULATIONS 2.1 Description In this work, we examine 28 simulated galaxies from the Auriga Project (Grand et al. 2017, named as AuN, where N is the halo number). The Auriga simulations are high-resolution cosmological zoom-in simulations using the magnetohydrodynamic (MHD) code arepo (Springel 2010).arepo is a quasi-Lagrangian, moving-mesh code that tracks the evolution of MHD cells and collisionless par- ticles in aΛCDM cosmological setting, with Planck Collaboration et al. (2014) cosmological parametersΩm = 0.307, Ωb = 0.048, ΩΛ =0.693, and a Hubble constant of𝐻0 =100ℎkm s−1 Mpc−1, whereℎ=0.6777.Thesimulationsincludeprimordialandmetal-line cooling,auniformultraviolet(UV)backgroundfieldforreionisation, star formation, magnetic fields, active galactic nuclei, energetic and chemicalfeedbackfromTypeIIsupernovae,andmassloss/metalre- turn due to Type Ia supernovae and asymptotic giant branch (AGB) stars, accounting for 9 elements (H, He, C, N, O, Ne, Mg, Si, Fe; Grand et al. 2017). StarformationintheAurigasimulationsoccursincellsthatexceed a density threshold of𝜌0 = 0.13 atoms cm−3; cells exceeding this threshhold form stars with a star formation timescale of𝜏SF = 2.2 Gyr(followingSpringel&Hernquist2003).TheAurigasimulations assume that every star cell represents a simple stellar population (SSP) with a specified age and metallicity that is directly inherited from the surrounding gas. The distribution of stellar masses present in each SSP follows a Chabrier (2003) initial mass function. TheAurigasimulationsuiteincludesrunsfromfourdifferentpar- ticle resolutions, and in this work we discuss two of them. Six halos (Au6, Au16, Au21, Au23, Au24, and Au27) have been simulated with a baryonic mass resolution of∼6 ×103M⊙, corresponding to maximum gravitational softening length of 184 physical pc; these areresolutionlevel3intheAurigaterminology.Bycontrast28have a baryonic mass resolution of∼5 ×104M⊙, corresponding to a maximum softening length of 369 physical pc (level 4); note that the 6 halos simulated at level 3 were also simulated at level 4. We perform a comparison of the results obtained from the six halos that aresimulatedatbothresolutionsinAppendixA.Thereweshowthat theresultsderivedatthetworesolutionsarestatisticallysimilar,and thus for the bulk of our analysis in this paper we will use the 28 halosavailableatlowerresolutionduetothegreaterstatisticalpower they offer. The exception is in Section 3, where we construct mock observations, and the higher spatial resolution is useful for studying observational effects (e.g. beam smearing). 2.2 Data extraction and analysis Foreachsimulationsnapshotweextractaboxcentredonthegalactic centre for analysis; our boxes extend to±10 kpc on either size of the galactic plane1, but the choice of size in the directions parallel to the galactic plane requires some care. For the purposes of our mock observationsof 𝑧=0 galaxies(Section3)weusea 40×40 kpc2 box, which is well matched to the field of view (FoV) of MUSE for local galaxy observations, but for the purposes of studying the evolution of the correlation function over cosmological times (Section 4) we instead use a smaller20 ×20 kpc2 region to avoid contamination by mergersandfly-bys,whicharemuchmorecommonathigherredshift; we discuss the choice of box size in more detail in Appendix B. 1 In the Auriga simulations the𝑧 axis, which by convention is normal to the galactic plane, is defined to be parallel to the total angular momentum vector of the star particles within0.1𝑅200 of the galactic centre, where𝑅200 is the virial radius. We have verified that alternative choices of definition of the 𝑧 axis, for example defining it based on dense gas cells rather than star particles, produces negligible changes in the results at both local and high-redshift Universe. MNRAS000, 1–12 (2024)Metallicity correlations in Auriga simulations 3 20 10 0 10 20 y (kpc) H (i)  (ii)  (iii)  (iv)  (v) 20 10 0 10 20 y (kpc) [NII] 6584 0.00 0.02 0.04 0.06 0.08 0.10 Line flux (10 16 erg s 1 cm 2) -10 0 10 x (kpc) 20 10 0 10 20 y (kpc) 12 + log(O/H) -10 0 10 x (kpc) -10 0 10 x (kpc) -10 0 10 x (kpc) -10 0 10 x (kpc) 8.6 8.8 9.0 9.2 9.4 12 + log(O/H) Figure 1.Illustration of the mock observations pipeline applied to the galaxy Au6. Each column shows one step in the procedure described in Section 3.4, respectively; these steps are (i) producing true emission line maps, (ii) convolution of the maps with a synthetic beam and addition of noise, (iii) pruning of low signal-to-noisepixels,(iv)reconstructionofthelowsignal-to-noiseareasusingthe adabin algorithm;(v)maskingtheremaininglowsignal-to-noiseregionsof the adaptively-binned maps. From top to bottom the rows show H𝛼 maps, [Nii]𝜆6584 maps, and metallicity maps. The first two rows correspond to the colour bar on the upper right, showing line flux emitted per unit area, and the bottom row corresponds to the lower right colour bar, showing metallicity. Our first step is to resample the star-forming gas cells within the boxinto 125×125×125 pc3 cells.Wethenconvertthe3Dboxintoa 2Dmapbyintegratingtheelement(hydrogenandoxygen)massinall thecellsalongalineofsightnormaltothegalacticplane.Weobtain aface-onoxygenmetallicitymapinacommonlyusedformfromthe definitionofmetallicity, 12 +log(𝑁O/𝑁H),where 𝑁O isthecolumn density of oxygen nuclei. The end result is a projected metallicity map with a spatial resolution of 125 pc. To complement this we also produceastarformationratemapatthesameresolutionbyprojecting the total masses of star-forming gas parcels and dividing by the star formation timescale𝜏SF used in the simulations (see Section 2.1). To quantitatively compare multiple metallicity maps, we extract two-point correlations from metallicity maps, and estimate corre- sponding correlation lengths. We do this in several steps, following theprocedureoutlinedbyLietal.(2021,2023).First,wesubtractthe radially-averaged metallicity in each annulus to obtain a metallicity residual map. Next we compute the two-point correlation function of the residual map, which we fit with the functional form for the two-point correlation function predicted by the KT18 model. This model predicts the correlation function in terms of two free param- eters: injection width (𝑤inj) and correlation length (𝑙corr). Injection width describes the size of the initial bubble formed in explosion events(e.g.,supernovae),andisusuallytoosmall( ≲ 100 pc;Lietal. 2023) to measure in either observations or cosmological simulated galaxies. Correlation length describes a characteristic length of ISM chemical mixing and is a key parameter of the KT18 model. We fit the simulation correlation function to the KT18 functional form usingaleastsquaresapproachwith 𝑤inj and𝑙corr asfreeparameters. We refer to fits on the pure simulation data as the “true” values to distinguishthemfromthevaluesderivedfrommockobservationsas we describe next. 3 MOCK OBSERVATIONS IN THE LOCAL UNIVERSE Before using the simulations to study the evolution of metallicity correlations over cosmic time, we first confirm that the simulations can reasonably reproduce the metallicity distributions of galaxies at 𝑧 = 0 derived from observations Li et al. (2021, 2023). In this section we create mock observations of the Auriga galaxies for di- rect comparison with previous results, using realistic observational effects (e.g. noise, PSF, spatial sampling) drawn from the AMUS- ING++ survey (López-Cobá et al. 2020). This survey uses MUSE observations, and at the mean 129 Mpc distance of AMUSING++ targets, MUSE’s1′×1′field of view corresponds to40 ×40 kpc2, which motivates our choice of region to extract from the the Auriga snapshots in Section 2.2. For the purpose of this comparison, we selectthesixAurigagalaxiesatlevel3(highspatialresolution)since they provide the best intrinsic spatial resolution, and thus the most stringent test of the effects of beam-smearing. We list the properties of these six galaxies in Table 1. We describe the pipeline we use to create the mock observations inSection3.1,andprovidedetailsofournoisemodelinSection3.2. We then demonstrate that the mock observations faithfully recover the real correlation lengths in the simulations (Section 3.3), and that MNRAS000, 1–12 (2024)4 Z. Li et al. 10 3 10 2 10 1 Noise (10 16 erg s 1 cm 2) H 10 6  10 5  10 4  10 3  10 2  10 1  100 101 Signal (10 16 erg s 1 cm 2) 10 3 10 2 10 1 Noise (10 16 erg s 1 cm 2) [NII] 6584 Figure 2.Distribution of signal versus noise for the H𝛼 (upper panel) and [Nii]𝜆6584 (lower panel) lines over all pixels extracted from four sample galaxies in AMUSING++; see main text for details. The contours represent the areas that enclose 39% (1𝜎 in 2D Gaussian), 68% (2𝜎), and 86% (3𝜎) ofthedata;outsidetheoutermostcontour,weshowindividualpixelsasblack points. The black solid lines show our best flat-plus-linear fit, while the gray dashed lines denote a fixed signal-to-noise ratio of 3. thesecorrelationlengthsareinreasonableagreementwiththevalues expected for𝑧=0 galaxies (Section 3.4). 3.1 Pipeline for mock observations Given our extracted region, we generate mock observations using a pipeline that consists of the following five steps; the final three of these steps closely follow the analysis method described by Li et al. (2023).WealsoillustratethesestepsinFigure1,usingthesimulated galaxy Au6 as an example. (i) In order to replicate observational errors on the simulated metallicitymap,wefirstcreatemockemissionlinemapsforthelines H𝛼and[Nii]𝜆6584,requiredby thecommonly-usedPettini&Pagel (2004)metallicitydiagnostic(hereafterPPN2).WechoosePPN2be- causeitisoneofthesimplestdiagnosticsandrequiresonlytwoemis- sion lines. To produce synthetic maps in the two required lines we firstderiveanH 𝛼mapfromthestarformationratemapusingthecal- ibrationsuggestedbyKennicutt&Evans(2012),SFR /(M⊙yr−1)= 5 ×10−42𝐿H𝛼/(erg s−1). Next, we use the metallicity map and the PPN2diagnostictopredictthe[N ii]𝜆6584/H𝛼ratioineachpixelby solvingtheequation 12+log(O/H)=9.37+2.03𝑥+1.26𝑥2 +0.32𝑥3, where 𝑥 =[Nii]𝜆6584/H𝛼 in each pixel. Multiplying the resulting value of𝑥 by the H𝛼 map produces an [Nii]𝜆6584 line map. We showtheresultingH 𝛼and[Nii]linemapsintheleftcolumn,toptwo rows of Figure 1, along with the true metallicity map (left column, bottom row). Empty pixels correspond to locations where the simu- lations include no fluid elements dense enough to be star-forming. (ii) Next we convolve the simulated flux maps with a Gaussian PSF and add noise. We convolve the two emission line maps using a Gaussian beam with a full width at half maximum (FWHM) of 1.′′0, which is the median PSF size for the AMUSING++ sample (Li et al. 2023). We then generate a noise map using the method described in Section 3.2, and add this to the beam-convolved line maps. At this point our maps represent a reasonable facsimile of the data products to which we have access from the observations; we illustrate these line flux maps, together with the metallicities one would derive by directly applying the PPN2 diagnostic to them, in the second column in Figure 1. Blank pixels in the metallicity map correspond to locations where, as a result of noise, the flux of either the H𝛼or [Nii] line is negative and thus it is not possible to derive a metallicity. (iii) Our third step is to mimic the data quality cuts that Li et al. (2023) apply to the AMUSING++ datafor our simulated maps; they showthatthesequalitycutsarerequiredinordertoextractareliable correlation length from the metallicity field, since in their absence the correlation is corrupted by noise. Specifically, we divide our simulated line and noise maps from step (ii) to produce maps of signal-to-ratio (S/N) for both lines, and we mask pixels where the S/N is below 3. We show the masked maps in the third column of Figure 1. (iv) Again following the procedure described in Li et al. (2023) we apply the adaptive binning algorithmadabin to the [Nii]𝜆6584 map, which is the weaker of the two lines in the diagnostic. This algorithm recursively groups pixels into larger and larger blocks in ordertoincreasetheS/N;eachregioniscoarseneduntilitreachesthe target S/N of 3. We refer readers to Li et al. (2023) for full details of thealgorithm.Oncewehavere-binnedthe[N ii]map,wereconstruct the H𝛼 map with the same binning in order to ensure that we only ever compute line ratios at matched spatial resolution. We show the binnedmapsandthemetallicitymapderivedfromtheminthefourth column of Figure 1. (v) Our final step is to mask the adaptively binned maps to avoid computing metallicities in locations that go beyond the true bound- ariesofionisedgasemissioninthetargetgalaxy.Toachievethis,we mask pixels where in the original, unbinned H𝛼map, the fluxes are detected at S/N< 3. We show the final, masked maps in the final column of Figure 1. We estimate the true correlation length in the KT18 model on the metallicity maps at step (i) using a least-squares fit, as discussed in Section 2.2. For the metallicity map at step (v) we instead follow Li et al. (2023) by adopting an MCMC approach that includes two ad- ditionalparameterstodescribeobservationalnuisanceeffects:beam size 𝜎beam and the noise factor𝑓. Beam size has a Gaussian prior, centred at the known beam size and with dispersion varying in dif- ferentobservations;itspurposeinthemodelistoaccountforthefact that beam-smearing introduces artificial correlations in the inferred metallicity at small scales. The𝑓 factor accounts for the decrease of two-point correlations at non-zero separations caused by noise. Its effect is to reduce the correlation at non-zero separations by a factor MNRAS000, 1–12 (2024)Metallicity correlations in Auriga simulations 5 Table 1.Globalpropertiesandcorrelationlengths(inboththeoriginalsimulationsandmockobservationsforinteriorcomparison)ofthesixAurigasimulations at level 3 (high spatial resolution). Columns are as follows: (1) Auriga ID; (2) stellar mass; (3) star formation rate from stellar cells in the recent 30 Myr; (4) star formation rate from star-forming gas cells, SFRg = 𝑥c𝑚g/𝑡sf , assuming a typical mass fraction of cold star formation clouds of𝑥c =0.9 (see details in the bottom panel of Springel & Hernquist’s Fig. 1) and a star formation timescale𝑡sf =2.2 Gyr (Grand et al. 2017); (5) half-stellar-mass radius (effective radius); (6) correlation length in the original simulation; (7) correlation length in the mock observation. For correlation lengths the central value is the 50th percentile of the posterior PDF, and the error bars show the 16th to 84th percentile range. Auriga ID 𝑀∗ SFR∗ SFRg 𝑅𝑒 𝑙corr,true 𝑙corr,mock (1010M⊙) (M ⊙yr−1) (M ⊙yr−1) (kpc) (kpc) (kpc) (1) (2) (3) (4) (5) (6) (7) 6 7.0 2.7 2.2 5.2 1.873 1.351+0.016 −0.016 16 10.6 4.2 2.9 11.9 1.990 3.388+0.068 −0.066 21 10.2 7.1 6.1 8.8 2.377 2.642+0.021 −0.019 23 9.5 4.6 4.2 8.5 1.133 1.199+0.012 −0.011 24 9.7 4.5 3.2 9.4 1.554 1.312+0.022 −0.021 27 11.0 4.2 3.8 7.1 1.263 1.741+0.017 −0.018 of 𝑓,which,asLietal.(2021)show,ishownoiseaffectsameasured two-point correlation function. In an extreme case where the noise is significantly larger than signal, the two-point correlation function approaches a𝛿function, the two-point correlation function of pure noise. 3.2 Noise estimates The mock pipeline described in the previous section requires esti- mates of the noise level in the line flux maps. Here we describe the process by which we construct these maps. We first choose four galaxiesfromAMUSING++(López-Cobáetal.2020)spanningdis- tances ranging from 127 Mpc to 131 Mpc, where the spatial reso- lution of MUSE (0.′′2) matches the 125 pc resolution of the Auriga simulations;thechosengalaxiesareSDSSJ102131.91+082419.8(la- beled as ASASSN14ba in the AMUSING++ catalogue), MCG-03- 07-040 (labeled as SN2005lu), ESO584-7 (labeled as SN2007ai), and NGC539 (labeled as SN2008gg). For each of these galaxies we extract one spectrum per spaxel, from which we obtain the flux in- tensity and its uncertainty for both the H𝛼and [Nii]𝜆6584 emission lines. In total this yields4 ×320 ×320 ≈4 ×105 distinct fluxes and uncertainties, which we use to estimate the typical noise properties given their exposure time (∼1 hour). Figure 2showsthe distributionof signaland noisefor thesespax- els. To model the relation between signal and noise at the given distance and spatial resolution, we fit the noise as a function of the signal with a function of the form 𝑁 = \u001a 𝑁0, 𝑆 < 𝑆 0 𝑁0 (𝑆/𝑆0)1/2, 𝑆 ≥𝑆0 , (1) where 𝑁0 and 𝑆0 are parameters to be fit. The motivation for this functional form, which is consistent with the distribution shown in Figure2,isthataroughlyconstantbackgroundnoiseleveldominates whenthesignalisweak,whilePoissonnoiseshoulddominatewhen the signal is strong. Performing a simple least-squares fit of this model to the measured S/N data yields best-fit values 𝑁0 = (5.1,4.6)×10−19erg s−1 cm−2 (2) 𝑆0 = (6.5,1.0)×10−18erg s−1 cm−2, (3) where the first number in parentheses is for the H𝛼 line and the second for the [Nii] line. We show these fits by the solid lines in Figure 2. To generate our synthetic noise maps, we use these fits to predict thenoiselevelineverypixelofboththeH 𝛼and[Nii]maps,andthen we draw a noise value for that pixel from a Gaussian distribution with zero mean and a dispersion equal to the noise level. 3.3 Comparison between mock observations and true correlation lengths Wecomparethecorrelationlengthspresentintheoriginalmetallicity mapstothosewerecoverfrommockobservedmetallicitymaps(the bottom left and the bottom right panels in Figure 1); our goal is to establishthatthecorrelationlengthsrecoveredfromobservedmetal- licitymapsarereasonablyaccurateestimatesofthetruecorrelations, despitetheeffectsoffinitetelescoperesolutionandsensitivity.InFig- ure3weshowthetwo-pointcorrelationfunctionsoftheboththetrue andmock-observedmetallicitymapsforsixexamplehalos.Figure3 illustratesthatmakingamockobservationofthesimulationresultsin suppressing the two-point correlations at all separations larger than zero due to noise, which artificially de-correlates the true metallic- ity map. It is a common phenomenon that appears when comparing theoretical two-point correlations and real ones. Due to inevitable noise, the two-point correlation function of an observed metallicity map will deviate from that of the true metallicity map. The overall effect of noise is to decrease the correlation function by a constant factor at all separations larger than zero. The𝑓 factor in the MCMC fit captures this effect, which is why it is critical to include it. InTable1thecolumn 𝑙corr,true reportsthetruecorrelationlengths measured directly from the simulations; these are reported without errors,becausetheycomefromthe 𝜒2 fittingandwedonothavethe uncertaintiesoftheoriginalmetallicitymaps.Thecolumn 𝑙corr,mock reports the result from the MCMC fitting, and the reported uncer- tainties correspond to the 16th to 84th percentile range. Table 1 demonstrates that in most cases the value of𝑙corr derived from the mock observations are within±40% of the true ones. We can there- MNRAS000, 1–12 (2024)6 Z. Li et al. 0.0 0.2 0.4 0.6 0.8 1.0(r) Au6 z = 0.0 (true) z = 0.0 (mock observation) Au16 z = 0.0 (true) z = 0.0 (mock observation) 0.0 0.2 0.4 0.6 0.8 1.0(r) Au21 z = 0.0 (true) z = 0.0 (mock observation) Au23 z = 0.0 (true) z = 0.0 (mock observation) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 r (kpc) 0.0 0.2 0.4 0.6 0.8 1.0(r) Au24 z = 0.0 (true) z = 0.0 (mock observation) 0.0 0.5 1.0 1.5 2.0 2.5 3.0 r (kpc) Au27 z = 0.0 (true) z = 0.0 (mock observation) Figure 3.The two-point correlation functions of the six example simulated galaxies, and the comparison between the two-point correlation functions of the pure simulations (blue dots) and those of mock observations (red dots). The blue solid lines and red solid lines represent the best estimate of the parameters in the KT18 model. Specifically the effects from the mock observations are shown as the discontinuity between the first two red dots. forehaveconfidencethatthecorrelationlengthsreturnedfrommock observations are reasonably close to reality. 3.4 Comparison between mock observations and real observations To examine how well our sample Auriga galaxies compare to obser- vations, we show the relationship between𝑙corr (from mock obser- vations) and𝑀∗, SFR, and𝑅𝑒 for the six example halos in Figure 4. The results demonstrate that the Auriga simulations are consistent with observational results, in that the Auriga simulations at𝑧 = 0 have correlation lengths well within the range observed for galaxies of similar properties. We warn that, because all the Auriga galaxies are chosen to be Milky Way analogues, this test covers only a lim- iteddynamicrangeingalaxyproperties.However,itisinterestingto note that, despite this limited dynamic range, we do recover hints of the correlations seen in the real data, i.e., the Auriga halos with the MNRAS000, 1–12 (2024)Metallicity correlations in Auriga simulations 7 108 109 1010 1011 Stellar mass (M ) 10 1 100 101 Correlation length (kpc) CALIFA (Li21, PPN2) AMUSING++ (Li23) Mock values (this work) 10 2  10 1  100 101 SFR (M  yr 1) CALIFA (Li21, PPN2) AMUSING++ (Li23) Mock values (this work) 100 101 Re (kpc) CALIFA (Li21, PPN2) AMUSING++ (Li23) Mock values (this work) 1.4 1.2 1.0 0.8 0.6 0.4 0.2 0.0 Log probability density Figure 4.Correlation length versus stellar mass (left), SFR (middle), and𝑅𝑒 (right). The blue, cyan, green, yellow, orange, and red (in some panels hidden by the other two symbols) stars show the correlation lengths from the mock observations of the six halos, Au6, Au16, Au21, Au23, Au24, and Au27, respectively. The background heat map shows the distributions derived from 100 galaxies in the CALIFA survey (Li et al. 2021, using the same PPN2 metallicity diagnostic) and the circles show those of 219 galaxies in the AMUSING++ survey (Li et al. 2023). The upper limit on the bottom right shows a scale below which the physics is modified by numerical smoothing at level 3 in the Auriga simulations. largeststellarmass,SFR,andeffectiveradiusalsotendtohavelarger correlation lengths. In Figure 4 we also indicate by downward arrows our estimate of themetalinjectionscaleinalevel3Aurigasimulation,whichrepre- sentsthescalebelowwhichthecorrelationlengthwillbedominated by numerical resolution; we see that all the measured simulation correlation lengths lie well above this value. To estimate this scale, we first note that in the Auriga simulations metals are injected into the 43 cells closest to a supernova event, so the characteristic size of a metal injection region is4𝑙g, where𝑙g is the length scale of a gas cell. Cell sizes in Auriga are adaptive, so𝑙g is not fixed, but we can estimate it by considering a gas cell with a density at the star formation threshold𝜌0 =0.13 H / cm−3 (mentioned in Section 2.1) and with the typical baryonic mass resolution at level 3 of the simu- lations, 𝑚b =6 ×103 M⊙(Grand et al. 2017). Since𝜌0𝑙3g = 𝑚b, a typical cell satisfying these conditions has𝑙g ∼125 pc, and thus we estimate the metal injection scale to be∼500 pc. 4 ANALYSIS: THE EVOLUTION OF METALLICITY CORRELATIONS OVER COSMIC TIME Having established that the metallicity distributions in the Auriga galaxies are a reasonable match to those observed in comparable galaxies at 𝑧 = 0, we now use the Auriga sample to examine the evolution of the two-point correlation function of metallicities over cosmic time. Since we have also established that simulated obser- vations of the Auriga simulations yield good approximations to the true correlation length, at least for observational parameters typical of current local Universe measurements, the subsequent discussion will only focus on the true two-point correlation functions, not on simulated observations – that is, we base all analysis from this point forward on true metallicity maps such as the one shown in the first column,bottomrowofFigure1.Wehave128suchmapsperAuriga halo, at time intervals of≈160 Myr. For each map of this form, we fit a noise-free KT18 model, following the procedure outlined in Section 3.1, to extract a correlation length𝑙corr. We therefore obtain the history of𝑙corr as a function of redshift for each Auriga galaxy. Figure 5 shows the star formation histories (SFHs) and𝑙corr evo- lution over cosmic time for 28 Auriga halos at level 4 resolution. Correlation lengths are smoothed in the adjacent three snapshots (corresponding to intervals of≈±160 Myr on either side of a given time). We also indicate as grey vertical bands in the figure periods oftimewhenmassivemergersaretakingplace,usingthemergerlist identified by Gargiulo et al. (2019). The intervals shown indicate all merger events between the primary Auriga galaxy being simulated andsatellitesofmass 𝑀sat >1010M⊙2.Thewidthofeachgreyband is 160 Myr, close to the interval between two snapshots. As shown in Figure 5, correlation length fluctuations correlate roughly with star formation rate fluctuations, but in general are of largeramplitude.Noristhecorrelationperfect:whilefluctuationsof the correlation length in some galaxies resemble amplified versions oftheSFRfluctuation,forexampleinAu16,thereareothergalaxies wheretrendsin 𝑙corr andSFRappeartobeweaklycorrelatedatbest, for example in Au3 and Au19. During merger periods correlation lengths tend to be larger on average than during non-merger peri- ods; we illustrate these averages as solid horizontal lines for merger periods and dashed horizontal lines for non-merger periods. This is consistent with the findings of Li et al. (2021) and Li et al. (2023), whonotethatcorrelationlengthstendtobelargerinobservedgalax- iesthatshowsignsofbeingintheprocessofmerging.Thisincreasein correlation length during merger periods is particularly pronounced in some galaxies, for instance Au7, Au21, Au29, and Au30. SFR is also highly sensitive to mergers, which can trigger shock waves through tidal forces, and the common response to𝑙corr and SFR to merger events clearly accounts for at least some of the correlation between them; Au24 and Au27 provide clear examples. However, it is also clearly not always the case that mergers induce large changes in SFRs or correlation lengths, and sometimes one responds but not the other. For example both Au18 and Au19 show enhanced SFRs duringmergers,butnocorrespondingincreasesincorrelationlength. 2 Gargiuloetal.(2019)onlyprovidelistsofmergerswithafixedmasscutoff, ratherthanafixedmassratio.However,sincealltheAurigahaloshaveafinal stellar mass𝑀∗∼1011 M⊙, this choice corresponds roughly to a 10:1 mass ratio, near the commonly-adopted threshold for a minor merger. MNRAS000, 1–12 (2024)8 Z. Li et al. 0 20 40SFR (M  yr 1) Au2 00.5123 z Au3 00.5123 z Au4 00.5123 z Au5 00.5123 z Au6 00.5123 z Au7 00.5123 z 0 5 10 lcorr (kpc)Au8 00.5123 z 0 20 40SFR (M  yr 1) Au9 Au10 Au12 Au13 Au14 Au15 0 5 10 lcorr (kpc)Au16 0 20 40SFR (M  yr 1) Au17 Au18 Au19 Au20 Au21 Au22 0 5 10 lcorr (kpc)Au23 024681012 tlookback (Gyr) 0 20 40SFR (M  yr 1) Au24 024681012 tlookback (Gyr) Au25 024681012 tlookback (Gyr) Au26 024681012 tlookback (Gyr) Au27 024681012 tlookback (Gyr) Au28 024681012 tlookback (Gyr) Au29 024681012 tlookback (Gyr) 0 5 10 lcorr (kpc)Au30 Figure 5.Cosmological evolution of star formation rates (blue solid lines) and correlation lengths (red dashed lines) for all the halos. The grey vertical bands indicate periods when major mergers take place. The horizontal solid and dashed lines represent the averaged values of correlation length during merger and non-merger periods, respectively, and are plotted only for galaxies that experience at least one major merger. 100 101 SFR (M  yr 1) 100 101 Correlation length (kpc) z [2.6, 1.5] z [1.5, 0.5] z [0.5, 0.0] 86% (3 ) 39% (1 ) 86% (3 ) 39% (1 ) 100 101 SFR (M  yr 1) 100 101 Correlation length (kpc) Au16 Au17 0 2 4 6 8 10 tlookback (Gyr) Figure 6.Left: contours showing the distribution of all the halos in the SFR-𝑙corr plane, divided up in bins of redshift. To construct this figure, we take the𝑙corr and SFR values from all snapshots in three temporal bins –𝑧 ∈[2.6, 1.5](cosmic noon),𝑧 ∈[1.5, 0.5](disc settling), and𝑧 ∈[0.5, 0.0](disc settled) – and count the frequency in a 2D histogram in this plane with bins that are 0.14 dex wide in SFR and 0.12 dex wide in𝑙corr. We then construct the contours shown from the 2D histograms. Each contour represents the boundary of the area within which 86% (darker and outer) and 39% (lighter and filled) data are included – these contour levels correspond to2𝜎 and 1𝜎 for a 2D Gaussian. The contours are coloured according to the centres of the corresponding time intervals on the colour bar to the right.Right: the trajectory of two example halos, Au16 (solid) and Au24 (dashed), in the SFR-𝑙corr plane. Colours on the tracks indicate lookback time as shown on the colour bar to the right. The starting and ending positions are marked in circles. MNRAS000, 1–12 (2024)Metallicity correlations in Auriga simulations 9 0.5 0.0 0.5 Cross correlation Au2 SFR            lcorr leading      leading Au3 SFR            lcorr leading      leading Au4 SFR            lcorr leading      leading Au5 SFR            lcorr leading      leading Au6 SFR            lcorr leading      leading Au7 SFR            lcorr leading      leading Au8 SFR            lcorr leading      leading 0.5 0.0 0.5 Cross correlation Au9 Au10 Au12 Au13 Au14 Au15 Au16 0.5 0.0 0.5 Cross correlation Au17 Au18 Au19 Au20 Au21 Au22 Au23 -1.0 0.0 1.0 Time lag (Gyr) 0.5 0.0 0.5 Cross correlation Au24 -1.0 0.0 1.0 Time lag (Gyr) Au25 -1.0 0.0 1.0 Time lag (Gyr) Au26 -1.0 0.0 1.0 Time lag (Gyr) Au27 -1.0 0.0 1.0 Time lag (Gyr) Au28 -1.0 0.0 1.0 Time lag (Gyr) Au29 -1.0 0.0 1.0 Time lag (Gyr) Au30 Figure 7.Cross correlations between SFR and correlation lengths for all the halos. This figure is computed from Figure 5 and equation 4 (SFR as𝑋 and𝑙corr as 𝑌). Positive time lags mean that𝑙corr reacts ahead of SFR. The examples above demonstrate that neither SFR nor𝑙corr are per- fect indicators of merger events likely due to the complex nature of merger (e.g. mass ratio, gas mass ratio, orientation). To explore the co-variation of correlation length and SFR over cosmic time further, in the left panel of Figure 6 we show a contour plot of the distribution of galaxies in the correlation length-SFR planebrokenupintointhreetemporalbins:high-(purple),medium- (cyan), and low-𝑧 (green). The quantity shown is the fraction of lookback time that the galaxy spends at a given combination of SFR and𝑙corr;thusforexampletheouterpurple86%contourinthefigure indicates that, if one chooses a random Auriga galaxy at a random lookbacktimecorrespondingtotheredshiftrange 𝑧∈[2.6,1.5]and plots its coordinates(SFR,𝑙corr), there is an 86% chance that these coordinates will lie within the contour. From high to low redshift, we see that the contours are largely consistent. The distribution of the Auriga galaxies in SFR-𝑙corr does not shift significantly over cosmictime,andingeneralresemblesthesequencetracedoutinthis plane by observed𝑧=0 galaxies (c.f. the middle panel of Figure 4). Thusthe(verybroad)SFR- 𝑙corr relationshipappearstobeessentially invariant over redshift, at least within the set of galaxies sampled by Auriga, and the evolution of individual galaxies appears to consist primarily of wandering about this relationship. At first the fact that there is no overall secular trend in where galaxies live on the SFR- 𝑙corr relationshipwithredshiftmightseemsurprising,butweremind thereaderthatwhatweareplottinghereisthehistoriesofparticular halosthathavebeenselectedspecificallytobeMilkyWay-analogues at 𝑧=0,not the distribution of all galaxies. However, we caution that the Auriga simulations are by construc- tion all Milky Way analogues, and thus cover a limited range of stellar mass and morphology. We cannot determine with certainty if this conclusion will continue to hold over a wider range of galaxy properties.However,weconjecturethatitwill,sincetheobservations doincludegalaxieswithwiderangeofmassesandmorphologiesatz =0,andtheseappeartobepartofthesamecontinuousdistributionas MilkyWay-likegalaxies,withnostrongdependenceonmorphology or other galaxy structural parameters (Li et al. 2021). The right panel of Figure 6 shows the tracks of two example ha- los, Au16 and Au24, in the SFR-𝑙corr plane, with time flowing from purple dots to yellow dots as indicated by the colour bar. The figure demonstrates that, while on average galaxies circulate rather than migratingsystematicallytowardoneendortheotheroftheSFR- 𝑙corr relationship, they do circulate with a clear pattern, one that is repli- cated in many other Auriga halos as well, though we show only two in this figure for clarity. The pattern is that galaxies do not move in both 𝑙corr and SFR simultaneously. Instead,𝑙corr increases first (cor- responding to upwards movement in the figure) and SFR increases next (rightwards movement). The same trend is visible when both of them decrease -𝑙corr decreases first (downwards movement) and SFR decreases next (leftwards movement). Consequently, the track forms a roughly clock-wise cycle in the𝑙corr versus SFR plane. To confirm this visual impression, we examine the cross- correlation of SFR and𝑙corr, defined in the usual way whereby the cross-correlation of two time sequences𝑋(𝑡)and𝑌(𝑡)is given by 𝜁(𝜏)= ⟨(𝑋(𝑡)−⟨ 𝑋⟩)(𝑌(𝑡+𝜏)−⟨𝑌⟩)⟩√︁ (⟨𝑋2⟩−⟨ 𝑋⟩2)(⟨𝑌2⟩−⟨𝑌⟩2) , (4) where ⟨·⟩denotes an average over𝑡. A positive time lag means that 𝑌 reacts first and𝑋 follows. Figure 7 shows the cross-correlations between SFR (as𝑋) and𝑙corr (as𝑌) for all the halos. In most cases, we see that SFR and𝑙corr are positively correlated at zero time lag, consistentwiththeoverallpositively-sloped“mainsequence”visible in the SFR versus𝑙corr plane. However, it is also clear that in most cases the cross-correlations have a positive slope, meaning that the correlation is stronger at positive time lag, corresponding to𝑙corr changing first and the SFR reacting slightly later. This is an interesting phenomenon because we see for the first MNRAS000, 1–12 (2024)10 Z. Li et al. time the causality of the interplay between𝑙corr and SFR. This can- not solely be merger-driven, since we see the same general positive trend in the cross-correlation function for the two halos that have no mergers (Au17 and Au25) as for all the others. The positive time lag is probably a result of the correlation length and SFR reacting to perturbationsondifferenttimescales–thenaturalresponsetimefor 𝑙corr is a galactic orbital period, while the natural response time for the SFR is the SFR timescale of∼2 Gyr, which is much longer than an orbital period. This explains both why𝑙corr fluctuates more, and why its fluctuations tend to lead SFR fluctuations. We note that, while the SFR timescale of∼2 Gyr is hardwired into the Auriga star formation prescription, and the∼200 Myr or- bital timescale is implicitly fixed by the choice to simulate Milky Way analogues, the general physical point we make here is true more generally: all observed galaxies have star formation timescales roughly an order of magnitude longer than their orbital times. This statementisequivalenttothesecondformofthewell-knownKenni- cuttrelation( ΣSFR vs.Σg/𝑡dyn,where ΣSFR,Σg,and 𝑡dyn denotestar formationratesurfacedensity,gassurfacedensity,andlocaldynam- icaltimescale,respectively).Therelationalsoholdsforhigh-redshift galaxies (e.g. Daddi et al. 2010). 5 CONCLUSIONS In this study, we utilise the Auriga simulations to investigate the evolution of the spatial distributions of chemical elements in galax- ies using the two-point correlation function as a tool. To quantita- tively compare the two-point correlations of different galaxies, and ofindividualgalaxiesastheyevolveovertime,westudythecorrela- tion length (𝑙corr) introduced in the Krumholz & Ting (2018) model for 2D galactic abundance distributions. As a parameter that reveals galaxychemicalmixingmechanisms,thecorrelationlengthprovides a unique window into galaxy past evolution history. We first confirm that the Auriga simulations have correlation lengths in galactic metal fields comparable to local observation. We first carry out simulated observations of the𝑧=0 Auriga snapshots, demonstrating that when we add realistic noise and beam smear- ingeffects,theAurigasimulationsproducegalaxieswithcorrelation lengths 𝑙corr consistent with observed values for galaxies of similar properties, indicating that the Auriga simulations capture the dom- inant chemical mixing processes involved in galaxy evolution. We furthershowthatcorrelationlengthsrecoveredfromthesesimulated observations are reasonably close to the true values present in the simulations. Wefindthatforanindividualgalaxythereisnosignificantsecular evolution in its correlation length, or in its correlation length versus starformationrate,overcosmologicaltimescales.Themeancorrela- tionlengthsofapopulationofgalaxiesathigh- 𝑧showsonlymarginal differences from the mean of those same galaxies at low-𝑧, and the galaxy-to-galaxyscatterof 𝑙corr alsoremainsunchangedovercosmic time.Wethereforeconcludethattherelationshipbetweencorrelation lengthandstarformationrateis nottheresultofabuild-upovercos- michistory.Instead,itappearstobeanequilibriumrelationshipthat is established in a time much less than a Hubble time. Galaxies can movealongthisrelationshipinresponsetoperturbationsintheiren- vironments, but the relationship itself is essentially invariant (xxx). This picture is very similar to the one proposed for the origin of galaxy metallicity gradients and their relationship with other galaxy properties such as mass and star formation rate Sharda et al. (2021). Furthermore, our analysis reveals an intriguing trend that fluctua- tionsincorrelationlengthsprecedefluctuationsinstarformationrate. This finding suggests that the scatter of the𝑙corr distribution might bearesultofgalaxiesbeingatdifferentevolutionarystages,whereby galactic correlation lengths react more rapidly to external pertur- bations than star formation rates. The former evolve on timescales comparable to the galactic orbital period, while the latter are change only over many orbital periods, so at a given time where a galaxy lies in the SFR-𝑙corr depends in part on whether its star formation rate has had time to “catch up” to the changes in correlation length induced by whatever perturbed it most recently. Finally,intheAurigasimulationswehavepresented,althoughgas cellsinthestar-formingdisccanbeassmallasafewtensofpc,met- als are injected across 64 cells and therefore the injection width, the otherkeyparameteroftheKT18modelwithtypicalvaluesof ≲ 100 pc (Li et al. 2023) is not resolved everywhere. However, in principle the value of𝑤inj should vary between elements with different nu- cleosynthetic origins (e.g., N versus O). One should be able to find signatures of this effect in both observations and higher resolution simulations.Infutureworkweintendtoapplytwo-pointcorrelations to higher resolution zoom-in simulations, to investigate if they are able to recover the imprints of elements’ differing nucleosynthetic origins on their present-day spatial distributions. ACKNOWLEDGEMENTS RG acknowledges support from an STFC Ernest Rutherford Fel- lowship (ST/W003643/1). EW & JTM acknowledge support by the Australian Research Council Centre of Excellence for All Sky As- trophysics in 3 Dimensions (ASTRO 3D), through project number CE170100013.MRKacknowledgessupportfromtheAustralianRe- search Council through award FL220100020. DATA AVAILABILITY The scripts and plots for this article will be shared on reasonable request to the corresponding author. Thearepo code is publicly available in Weinberger et al. (2020). REFERENCES Belfiore F., et al., 2017, MNRAS, 469, 151 Bellardini M. A., Wetzel A., Loebman S. R., Faucher-Giguère C.-A., Ma X., Feldmann R., 2021, MNRAS, 505, 4586 Bundy K., et al., 2015, ApJ, 798, 7 Ceverino D., Sánchez Almeida J., Muñoz Tuñón C., Dekel A., Elmegreen B. G., Elmegreen D. M., Primack J., 2016, MNRAS, 457, 2605 Chabrier G., 2003, PASP, 115, 763 Colbrook M. J., Ma X., Hopkins P. F., Squire J., 2017, MNRAS, 467, 2421 Croom S. M., et al., 2012, MNRAS, 421, 872 Daddi E., et al., 2010, ApJ, 714, L118 DiMatteoP.,PipinoA.,LehnertM.D.,CombesF.,SemelinB.,2009,A&A, 499, 427 DiMatteoP.,HaywoodM.,CombesF.,SemelinB.,SnaithO.N.,2013,A&A, 553, A102 Erroz-Ferrer S., et al., 2019, MNRAS, 484, 5009 Gargiulo I. D., et al., 2019, MNRAS, 489, 5742 Grand R. J. J., et al., 2016, MNRAS, 460, L94 Grand R. J. J., et al., 2017, MNRAS, 467, 179 Ho I. T., et al., 2018, A&A, 618, A64 Kennicutt Robert C. J., 1998, ARA&A, 36, 189 Kennicutt R. C., Evans N. J., 2012, ARA&A, 50, 531 Kreckel K., et al., 2019, ApJ, 887, 80 Kreckel K., et al., 2020, MNRAS, 499, 193 MNRAS000, 1–12 (2024)Metallicity correlations in Auriga simulations 11 0.6 1.0 2.0 4.0 6.0 10.0 Correlation length (kpc) 0.0 0.2 0.4 0.6 0.8 1.0Cumulative distribution function Level 3 (softening length at 184 pc) Level 4 (softening length at 369 pc) Figure A1. Cumulative distribution function of correlation lengths for the six Auriga halos that are simulated at both level 3 (blue) and level 4 (red) resolution. Krumholz M. R., Ting Y.-S., 2018, MNRAS, 475, 2236 Li Z., Krumholz M. R., Wisnioski E., Mendel J. T., Kewley L. J., Sánchez S. F., Galbany L., 2021, MNRAS, 504, 5496 Li Z., et al., 2023, MNRAS, 518, 286 López-Cobá C., et al., 2020, AJ, 159, 167 Ma X., Hopkins P. F., Feldmann R., Torrey P., Faucher-Giguère C.-A., Kereš D., 2017, MNRAS, 466, 4780 Metha B., Trenti M., 2023, MNRAS, 520, 879 Metha B., Trenti M., Chu T., 2021, MNRAS, 508, 489 Orr M. E., et al., 2023, MNRAS, 521, 3708 Petit A. C., Krumholz M. R., Goldbaum N. J., Forbes J. C., 2015, MNRAS, 449, 2588 Pettini M., Pagel B. E. J., 2004, MNRAS, 348, L59 Planck Collaboration et al., 2014, A&A, 571, A16 Poetrodjojo H., et al., 2018, MNRAS, 479, 5235 Rosales-Ortega F. F., Díaz A. I., Kennicutt R. C., Sánchez S. F., 2011, MN- RAS, 415, 2439 Sánchez-Menguiano L., et al., 2016, ApJ, 830, L40 Sánchez-Menguiano L., et al., 2018, A&A, 609, A119 Sánchez S. F., et al., 2012, A&A, 546, A2 ShardaP.,KrumholzM.R.,WisnioskiE.,ForbesJ.C.,FederrathC.,Acharyya A., 2021, MNRAS, 502, 5935 Springel V., 2010, MNRAS, 401, 791 Springel V., Hernquist L., 2003, MNRAS, 339, 289 Tinsley B. M., 1980, Fundamentals Cosmic Phys., 5, 287 TisseraP.B.,Rosas-GuevaraY.,SilleroE.,PedrosaS.E.,TheunsT.,Bignone L., 2022, MNRAS, 511, 1667 Torrey P., Cox T. J., Kewley L., Hernquist L., 2012, ApJ, 746, 108 Weinberger R., Springel V., Pakmor R., 2020, ApJS, 248, 32 Williams T. G., et al., 2022, MNRAS, 509, 1303 Yang C.-C., Krumholz M., 2012, ApJ, 758, 48 de Avillez M. A., Mac Low M.-M., 2002, ApJ, 581, 1047 APPENDIX A: COMPARISON OF RESULTS DERIVED FROM LEVEL 3 AND LEVEL 4 The majority of the Auriga simulations use “level 4” resolution, for whichthemaximumsofteninglengthis369physicalpc,butasubset of six halos (Au6, Au16, Au21, Au23, Au24, and Au27) were also simulated at a resolution with a maximum softening length of 184 physical pc (“level 3” resolution). In order to test the resolution de- 0.6 1.0 2.0 4.0 6.0 10.0 Correlation length (kpc) 0.0 0.2 0.4 0.6 0.8 1.0Cumulative distribution function Large box Small box Figure B1.Cumulativedistributionfunctionofcorrelationlengthsforthe28 Auriga halos using a larger (40 ×40 kpc2, blue) and smaller (20 ×20 kpc2, red) FoV. pendence of our results, we compute correlation lengths from both sets of simulations. In Figure A1 we show the cumulative distribu- tion function (CDF) of correlation lengths of the six halos that are simulated at both level 3 and level 4; the CDF that we show here is the distribution of correlation lengths over all all snapshots for all six halos, and thus represents the distribution in time. The two distributions are clearly very similar, and a quantitative comparison using a two-sided KS test indicates that we cannot reject the null hypothesis that the level 3 and level 4 simulation results were drawn from the same underlying distribution (𝑝 = 0.083). This suggests that our analysis is robust against changing resolutions. APPENDIX B: COMPARISON OF RESULTS DERIVED FROM DIFFERENT BOX SIZES Asdiscussedinthemaintext,thereissomesubtletyinthechoiceof fieldofview(FoV)aroundeachgalaxytoexaminewhencomputing the two-point correlation function. A larger FoV of40 ×40 kpc2 at the median∼129 Mpc distance of the AMUSING++ sample is well-matched to the MUSE FoV, and thus is well-suited to making mock observations, but we find that a smaller20 ×20 kpc2 FoV is preferable for studying cosmological evolution. To motivate that choice and examine its impact, in Figure B1 we show the CDF of correlationlengthsofallthe28halosatlevel4computedusingboth thelargerandsmallerFoV.WefindthatthelargerFoVchoiceleadsto adistributionthatisshiftedsignificantlytolargercorrelationlengths. Thetwo-sidedKStestindicatesthatwerejectthenullhypothesisthat thelargerandsmallerFoVsamplesweredrawnfromdifferentparent distributions (𝑝=6.6 ×10−16). This suggests that the choice of box size will affect our analysis. GiventhatthechoiceofFoVmatters,itisimportanttounderstand what causes the difference. For this purpose we show a detailed example of Au24 in Figure B2. In this figure we show in the upper panels the metallicity residual maps at four redshifts (𝑧 = 2,1,0.5, and 0), and in the lower panel the star formation history and the correlation length as a function of redshift. We see that at the two lower redshifts,𝑧 = 0 and 0.5, the correlation lengths for the two different FoV sizes are nearly identical, and they differ by only tens MNRAS000, 1–12 (2024)12 Z. Li et al. -10 0 10 x (kpc) 20 10 0 10 20 y (kpc) -10 0 10 x (kpc) -10 0 10 x (kpc) -10 0 10 x (kpc) 0.4  0.2  0.0 0.2 0.4 (O/H) 024681012 Lookback time (Gyr) 0 10 20 30SFR (M  yr 1) z = 2.0 z = 1.0 z = 0.5 z = 0.0 0 5 10 15 Correlation length (kpc) Small box Large box Figure B2.AnillustrationofanexamplehaloAu24.Thefirstrowshowsthemetallicityresidualmapsofa 40×40 kpc2 FoVatfourcosmictimes( 𝑧 =2.0, 1.0, 0.5, and 0.0 from left to right), while the black boxes indicate a20 ×20 kpc2 FoV. The second row illustrates the star formation history in the blue curve and the correlation lengths derived from larger (blue squares) and smaller (red squares) at different cosmic times indicated using the black vertical dashed lines. of percent even at𝑧=1. By contrast, there is a very large difference in correlation length in the𝑧 = 2 snapshot, and examination of the metallicitymapsatstarformationhistorymakesitclearwhy:at 𝑧=2 thegalaxyisonthecuspofamerger,leadingtoasubstantialincrease in star formation rate. Because the merger has not yet occurred, however, we can clearly see in the larger FoV two distinct galaxies with different mean metallicities. When we then compute the two- point correlation of resulting map, we get a very large value that in effect corresponds to the projected separation between the two pre-merger galaxies, rather than describing the metal field within either of them; a similar phenomenon is seen in observations of interacting galaxies (Li et al. 2023). The smaller FoV avoids this effectbecauseitisnotlargeenoughtoincludegasinsatellitegalaxies or metal-poor gas from the circumgalactic medium (CGM). This phenomenondrivesthedifferencesinthedistributionsofcorrelation lengthsvisibleinFigureB1,anditmotivatesustochoosethesmaller box size for our cosmological analysis because we wish to focus on correlation lengths within galaxies, and therefore to the extent possible to avoid contamination by pre-merger interacting systems. This paper has been typeset from a TEX/LATEX file prepared by the author. MNRAS000, 1–12 (2024)",
      "references": [],
      "meta_data": {
        "arxiv_id": "2402.08632v1",
        "authors": [
          "Zefeng Li",
          "Robert J. J. Grand",
          "Emily Wisnioski",
          "J. Trevor Mendel",
          "Mark R. Krumholz",
          "Yuan-Sen Ting",
          "Ruediger Pakmor",
          "Facundo A. Gómez",
          "Federico Marinacci",
          "Ioana Ciucă"
        ],
        "published_date": "2024-02-13T18:00:02Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This study investigates the cosmological evolution of galactic gas-phase metal distributions using two-point correlation functions from the Auriga simulations. Key findings include that simulated mock observations accurately reproduce metal correlation functions from local galaxy surveys, suggesting the simulations capture essential chemical mixing processes. Individual galaxies show no significant systematic evolution in their metal correlation functions from z~3 to the present. The research reveals that fluctuations in metal correlation length are correlated with, but precede, fluctuations in star formation rate (SFR), implying that metal re-arrangement within galaxies occurs more rapidly and is more sensitive to environmental changes like mergers, gas inflows/outflows, and fly-bys. The study concludes that the relationship between correlation length and SFR is an equilibrium phenomenon, established quickly, rather than a cumulative effect over cosmic history.",
        "methodology": "The study utilized 28 simulated galaxies from the Auriga Project, which are high-resolution cosmological zoom-in simulations employing the magnetohydrodynamic (MHD) code AREPO. Two-point correlation functions were used to characterize 2D metallicity maps, and the Krumholz & Ting (KT18) model was applied to fit these functions and extract the characteristic correlation length (l_corr). A mock observations pipeline was developed for z=0 snapshots, involving the creation of synthetic Hα and [NII]λ6584 emission line maps (using the PPN2 diagnostic), convolution with a Gaussian PSF, addition of realistic noise (modeled from the AMUSING++ survey), pruning of low signal-to-noise pixels, and adaptive binning (adabin algorithm). Least-squares fitting was used for true simulation data, while an MCMC approach, incorporating observational nuisance parameters (beam size and noise factor), was used for mock observation data. Cross-correlation analysis was performed to investigate the causal relationship between SFR and l_corr.",
        "experimental_setup": "The research primarily used 28 simulated galaxies from the Auriga Project, with 6 halos simulated at Level 3 resolution (~6 × 10^3 M☉ baryonic mass resolution) for mock observations and all 28 at Level 4 resolution (~5 × 10^4 M☉) for cosmological evolution analysis. The simulations adopted Planck Collaboration et al. (2014) cosmological parameters. They included primordial and metal-line cooling, a uniform UV background, star formation (density threshold 0.13 atoms cm−3, timescale 2.2 Gyr, Chabrier IMF), magnetic fields, AGN, and energetic/chemical feedback from Type II supernovae and AGB stars (tracking 9 elements). Data extraction involved resampling star-forming gas cells into 125x125x125 pc^3 cells and projecting them into 2D face-on metallicity maps (125 pc resolution). A 40x40 kpc^2 box was used for z=0 mock observations (matching MUSE FoV), while a smaller 20x20 kpc^2 box was used for cosmological evolution to avoid contamination from mergers and the circumgalactic medium. Validation included comparing mock observations to local galaxy surveys (CALIFA, AMUSING++), comparing mock l_corr values to true simulation values, and assessing robustness against resolution and box size changes.",
        "limitations": "The study's conclusions are limited by the nature of the Auriga simulations, which are constructed as Milky Way analogues, thus covering a restricted range of stellar mass and morphology. This limits the certainty of generalizing findings to a broader spectrum of galaxy properties. A noted limitation is that the metal injection width, a key parameter of the KT18 model, is not fully resolved everywhere in the simulations due to the characteristic cell size being comparable to or larger than typical injection widths. Additionally, the star formation timescale of ~2 Gyr is hardwired into the Auriga star formation prescription, potentially influencing the observed time lags. The merger identification in the simulations relies on a fixed mass cutoff rather than a fixed mass ratio. Mock observations also introduce artificial correlations at small scales due to beam-smearing and noise, though these effects were accounted for in the fitting process.",
        "future_research_directions": "Future work aims to apply two-point correlations to higher-resolution zoom-in simulations to explore if they can recover the imprints of elements' differing nucleosynthetic origins (e.g., Nitrogen vs. Oxygen, which are expected to have different injection widths) on their present-day spatial distributions. The findings of this study are also intended to guide future observational efforts focused on measuring metallicity correlations in galaxies beyond the local Universe. Expanding the analysis to a wider range of galaxy properties, beyond the Milky Way analogues currently simulated by Auriga, is also a logical extension.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Enhancing LLM’s Cognition via Structurization",
      "full_text": "Application of positivity bounds in asymptotically safe gravity Astrid Eichhorn ∗ and Andreas Odgaard Pedersen † CP3-Origins, University of Southern Denmark, Campusvej 55, DK-5230 Odense M, Denmark Marc Schiffer ‡ Perimeter Institute for Theoretical Physics, 31 Caroline Street North, Waterloo, ON N2L 2Y5 Positivity bounds are bounds on the Wilson coefficients of an effective field theory. They hold, if the ultraviolet completion satisfies unitarity, microcausality, locality and Lorentz symmetry; ac- cordingly their violation signals a violation of at least one of these properties of the ultraviolet completion. We explore whether positivity bounds on four-photon-couplings hold, when the ultra- violet completion is an asymptotically safe gravity-photon theory. By working at sixth order in an expansion in the electromagnetic field strength, we discover indications that positivity bounds hold for effective field theories that are UV completed by the asymptotically safe Reuter fixed point. We also perform various tests of the robustness of our result. This amounts to a nontrivial and critical indication of the physical viability of asymptotically safe gravity. Motivation: In this work, we apply positivity bounds [1–3] in asymptotically safe quantum gravity as a litmus test of causality and unitarity in the theory. Asymp- totically safe quantum gravity [4, 5] is a quantum field theory of gravity. It is based on an enhanced symmetry at small, sub-Planckian distance scales, namely quantum scale symmetry. This symmetry is realized at a fixed point of the Renormalization Group (RG) flow. Such a fixed point addressed the problem of perturbative non- renormalizability, which consists of a loss of predictiv- ity through introducing infinitely many counterterms. A fixed point generates relations between the correspond- ing, infinitely many couplings of the theory. These rela- tions persist down to the infrared (IR), if we start from a quantum scale-symmetric theory in the ultraviolet (UV). This renders the theory predictive despite the presence of higher-order interactions which correspond to the coun- terterms of perturbative renormalization. For gravity, such a fixed point is interacting and known as the Reuter fixed point. Following a proposal by Weinberg [4] and early evidence in 2+ ϵ dimensions [6–8], see also [9–11] for recent studies, a breakthrough came with Reuter’s adaptation of functional Renormalization Group tech- niques [12–14] to quantum gravity [5]. With these tech- niques, compelling evidence for the requisite interacting RG fixed point has been collected, both in pure gravity and in gravity-matter theories, see [15–23] for early ev- idence and [24–32] and references therein for the most recent evidence, as well as [33–41] for recent reviews. There is a critical open question, namely whether the theory is physically viable in that it respects causal- ity and unitarity. Answering this question is techni- cally non-trivial due to the effect of higher-derivative interactions. These are unavoidably present at an in- teracting fixed point, see, e.g., [18, 22–24, 28, 42–45], but may endanger the status of the theory as a viable physical theory. They may introduce ghost degrees of freedom. Ghosts are generically present in simple exam- ples with non-degenerate Hamiltonians and finite many higher-order derivatives, which result in Ostrogradsky instabilities [46]. Such instabilities can be traded for unitarity-violation in the quantum theory. Going beyond the simplest examples, the situation becomes more subtle and examples of healthy theories with higher-order time derivatives and stable time evolution are known, see, e.g., [47–50] for different classes of such theories. There are even examples in which ghost degrees of freedom do not result in runaway instabilities in the time evolution of the classical theory [51, 52]. At the quantum level, it has been shown how ghosts can be avoided and, e.g., traded for potentially harmless violations of microcausality [53– 55]. This mechanism may be relevant for quadratic grav- ity, see also [56–58] for a related proposal and [59, 60] for results on the well-posedness of the equations of motion at the classical level despite fourth-order time derivatives. Against this background, it would be premature to dis- miss asymptotically safe gravity as a physically viable theory based on the presence of higher-derivative terms. In fact, it has been pointed out that the analytic struc- ture of the propagator might not have ghost poles [61–64] and scattering amplitudes might obey unitarity bounds [65]. Studies that employ truncations of higher-derivative terms to finite order can, however, induce spurious ghosts in the propagator even in theories which are known to be unitarity [66, 67]. This makes the assessment of unitarity challenging when working to finite order in a truncation of the gravitational dynamics. In this work, we pioneer1 a different way of probing the physical viability of the theory: we use positivity bounds to place new constraints on asymptotically safe gravity. Positivity bounds: Positivity bounds are bounds on the Wilson coefficients in an effective field theory. They are derived by demanding unitarity, locality, microcausality, 1 During the course of this project, we became aware of the re- lated work [68] by B. Knorr and A. Platania, whom we thank for discussions. arXiv:2405.08862v1  [hep-th]  14 May 20242 and Lorentz invariance, but otherwise remaining agnos- tic about the ultraviolet (UV) completion [1–3]. Wil- son coefficients parameterize the scattering amplitudes in the theory, and unitarity, locality, microcausality and Lorentz invariance in turn constrain the dependence of the scattering amplitudes on the Mandelstamm variables. For instance, Lorentz invariance implies crossing symme- try of the amplitude; unitarity and causality restrict the analytic structure of the amplitude. Positivity bounds arise as part of a broader S-matrix bootstrap, which aims at mapping the space of S-matrices of all consistent quan- tum field theories [69, 70], see also [71] for the relation of S-matrix bootstraps constraints to positivity bounds. By now positivity bounds have been derived in a vari- ety of contexts, including without and with gravity, see, e.g., [72–78] and with [79–87] respectively, and [88] for reviews. The impact of broken Lorentz invariance has been considered in [89–91] and in a quantum gravita- tional context, positivity bounds have been considered in string theory [92–94]. In order to make use of results on photon self-interactions in asymptotically safe gravity [95–97], we focus on pos- itivity bounds for the photon [1, 98–100]. In the pres- ence of gravity, the massless pole in the spin-2-propagator makes the derivation of positivity bounds more involved [83] and, as discussed, e.g., in [101], can lead to Planck- scale suppressed violations of the positivity bounds. Here, we nevertheless focus on the standard positivity bounds, because these can also be derived by imposing causality on the propagation of photons at low energy, i.e., without making use of scattering amplitudes [100]. In theories with massless vector bosons, the Lagrangian of the low-energy EFT can be written in terms of higher powers of the field strength, Lphotons = −1 4FµνFµν (1) +c1 k4 (FµνFµν)2 + c2 k4 FµνFνκFκλFλµ +. . . . Herein k is an energy scale required because the higher- order terms are dimension-eight operators. The interac- tions associated to c1,2 form a basis for the dimension- eight operators at fourth order in field strengths [96, 102, 103]. Positivity bounds for the four-photon interactions have been derived in [1, 98–100] and read 4c1 >−3c2, 4c1 +3c2 ∣4c1 +c2∣ >1. (2) The same bounds can be derived using IR causality [104], i.e., sub-luminal propagation of photons, in the low- energy theory, without reference to high-energy scatter- ing [100]. Four-photon Wilson coefficients from asymptotically safe gravity: To test, whether positivity bounds hold in asymptotically safe quantum gravity, we use functional RG techniques [12–14], see [105] for a review. In func- tional RG setups, the scale k acts as an IR cutoff in the path integral, such that all quantum fluctuations at higher momentum scales have been integrated out. This allows a direct interpretation of ci(k → 0) as the Wilson coefficients of a low-energy theory in which all degrees of freedom have been integrated out. Below, we explain how to obtain predictions for these Wilson coefficients from an asymptotically safe theory of gravity and pho- tons. At an asymptotically safe fixed point, the higher-order interactions in Eq. (2) are already present in the UV [95, 96, 106]. This is a main distinction to, e.g., asymp- totically free theories, in which such higher-order inter- actions are only generated by the RG flow towards the IR, but can be set to zero in the UV. In asymptotically safe gravity, the higher-order interactions are generated by gravitational fluctuations, which we parameterize by the Einstein-Hilbert Lagrangian Lgravity =− k2 16π GR + k4Λ 8πG . (3) In this parameterization, G and Λ are dimensionless and thus given by numbers in the UV. To calculate the UV values of the four-photon cou- plings, we make use of beta functions derived in [95– 97, 106]. The UV (fixed point) values are the zeros of the two beta functions βc1 = 4c1 − 53 18π G c1 − 23 36π c2 G + 5 2G2 +O(c2 1,2), (4) βc2 = 4c2 + 19 18π G c2 + 40 9π c1 G −10G2 +O(c2 1,2), (5) where we set the UV value of the dimensionless cosmo- logical constant to zero, Λ ∗ = 0 (for our numerical re- sults, we use the fixed-point value Λ ∗ = 0.05). At larger values of c1,2 additional terms O(c2 1,2) are present; they are not important for our qualitative discussion, but in- cluded in our numerical results. The terms ∼ G2 imply that βc1,2 = 0 can only be achieved at c1,2 ∗ ≠ 0, i.e., at nonvanishing fixed-point values. To determine the fixed- point values, we supplement the beta functions for the couplings c1,2 by beta functions for the gravitational cou- plings calculated in [107], together with the contribution from photons, as in [108]. We find the Reuter fixed point at G∗ =1.12 , Λ∗ =0.05 , c 1 ∗ =−0.69 , c 2 ∗ =2.62. (6) The two couplings c1 and c2 correspond to irrelevant perturbations of the Reuter fixed point, which can be determined from the critical exponents that character- ize the fixed point. For irrelevant perturbations there is no free parameter that enters the values of the cou- plings, even once the RG flow moves away from the fixed point. Because G and Λ are relevant perturbations of3 the fixed point, their low-energy values can be adjusted to match observational constraints [17, 109]. The tra- jectories c1,2(k) depend on the values of the relevant couplings, but there are no free parameters left after G(k ≪ MPlanck) and Λ (k ≪ MPlanck) are adjusted to their observed values. We therefore obtain unique func- tions c1,2(k) and can thus determine whether or not the inequalities (2) hold in the limit k → 0. For k > MPlanck, it holds that c1,2(k) = c1,2 ∗. Once k drops below the Planck scale, k < MPlanck, the Newton coupling scales as G ∼k2. This is necessary for gravity to behave classically in this regime, because the dimension- ful Newton constant is then given by GN (k2) = G ⋅k2 = const. This fast decrease of G implies that quantum fluc- tuations of gravity decouple dynamically, as one would expect in this regime. At k < MPlanck, the flow of four- photon couplings is driven mainly by quantum fluctua- tions of the photon field. In practise, in this regime, the RG flow of c1,2 is dominated by dimensional terms, be- cause βc1,2 =4c1,2 +O(c2 1,2) and c1,2 scale towards zero.2 To interpret the resulting RG trajectoriesc1,2(k) in the context of positivity bounds, we must connect the calcu- lation of c1,2(k) in a Euclidean regime to the Lorentzian positivity bounds. We thus perform a Wick-rotation, under which the higher-order couplings do not change signs. While a Wick-rotation is in general not avail- able in a fully non-perturbative quantum-gravity setting, see, e.g., [110], there is mounting evidence for a near- perturbative nature of asymptotically safe quantum grav- ity [27, 42, 44, 45, 111–118]. In such a near-perturbative setting, the theory may be dominated by small fluctua- tions of the metric about a Minkowski background, for which an analytical continuation is available, given a suit- able analyticity structure of the propagator. We thus make the assumption that a Wick-rotation of the effective action for photons captures the behavior of an asymptot- ically safe gravity-photon system in Lorentzian signature. Our results are obtained in truncations of the func- tional RG and are thus subject to systematic uncertain- ties. Therefore, it is crucial to test the robustness of our results, which we do in three different ways. First, we include six-photon operators computed in [97], which im- pact the RG running of the four-photon operators. They have a subleading impact on our results at the quanti- tative level and do not impact the qualitative results. This is in line with the expected near-perturbative na- ture of the fixed point, at which higher-order operators contribute at a subleading level. Second, we vary an un- physical parameter in our calculation, namely a gauge parameter in the gravitational sector. Because it is an 2 In our setup, the scaling G ∼ k2 means that a tiny correction to dimensional scaling of c1,2 is present in this regime; in set- tings with charged matter, this would be completely negligible compared to the effect of matter loops. FIG. 1. We show the RG trajectory in the c1,2 plane; the scale dependence of the other couplings is not included in the plot, but enters in the calculation of c1,2(k). The purple dot indicates the Reuter fixed point of asymptotically safe quan- tum gravity Eq. (6), and the colored line emanating from it the RG trajectory towards k → 0. The green shaded region indicates where the positivity bounds (2) are satisfied. We ob- serve that for all finitek the scale-dependent couplings c1,2(k) satisfy the positivity bounds. unphysical parameter, physical results – such as whether causality can be violated – would not depend on it, if we could calculate without introducing approximations through our truncation. The dependence on the gauge parameter is thus another measure for the robustness of our results, see, e.g., for other studies of gauge-parameter dependence [119, 120]. Third, we use the principle of minimum sensitivity for the critical exponents as a func- tion of the gauge parameter to find a preferred value for the gauge parameter. We use this value to present the results in this letter and discuss other values and provide further details in the supplementary material. Positivity bounds in asymptotically safe gravity-photon systems: We find that the inequalities Eq. (2) are satisfied at the asymptotically safe fixed point Eq. (6). However, the decisive test of whether asymptotically safe gravity is compatible with the positivity bounds – and thus the underlying conditions of unitarity, (micro)causality, lo- cality and Lorentz symmetry – is performed by starting the RG flow at the fixed point and integrating towards low k. We find that the positivity bounds are also satis- fied everywhere along the trajectories c1,2(k), cf. Fig. 1. This is a non-trivial indication for the physical viability4 of asymptotically safe gravity. In the supplemental material, we analyze the robustness of this result and find remarkable stability under exten- sions of the truncation and variations of the gauge pa- rameter. In fact, positivity bounds hold in all settings we analyze, except for a small interval of values of the gauge parameter. We argue that this interval is disfavored, see the discussion in the supplemental material. Thus, while our study cannot fully exclude positivity violations, they likely only arise as a consequence of an insufficient trun- cation for the interval of gauge parameter in question. Accordingly, we suggest that infrared causality is preserved in the EFT for photons (see [100]) whose UV completion is an asymptotically safe gravity-photon theory. In addition, the following statement can be made about unitarity, microcausality, locality and Lorentz invariance: because these conditions together imply the positivity bounds, a large violation of positivity bounds implies that at least one of the conditions is violated.3 The converse is not true; if positivity bounds are satisfied, one cannot conclude that all conditions hold, as they might, e.g., be violated in positivity bounds for higher-order Wilson coefficients beyond those we considered. We can thus make the statement that we do not find convincing indications for the violation of unitarity, locality, microcausality or Lorentz-symmetry in asymptotically safe gravity . Next, we explore whether positivity bounds automati- cally hold in asymptotically safe gravity or whether they constrain the theory in a non-trivial way. To that end, we treat the fixed-point values of the gravitational cou- plings G and Λ as free parameters, instead of determining them through the fixed-point condition. This analysis can also be interpreted as a further test of robustness of our results: because the fixed-point values of G and Λ are subject to uncertainties induced by the truncation, it is crucial to know whether slight variations in the fixed- point values could lead to qualitatively different conclu- sions. In the plane spanned by G and Λ, we determine the fixed-point values of c1,2 as functions of G and Λ. We restrict the range of Λ to those values that can be continuously connected to Λ = 0 at low scales, see the supplementary material for details. As a first result, we find that there are regions in the G −Λ-plane for which the positivity bounds are violated for the fixed-point val- ues c1,2 ∗(G, Λ). This does not automatically imply pos- itivity violation in the effective field theory, because the positivity bounds relate to the Wilson coefficients, i.e., the couplings at low k, and not their fixed-point values. To test positivity violation, we numerically solve the beta 3 As discussed in [100, 101], Planck-scale suppressed violations of positivity bounds can occur in the presence of gravity. −0.2 0.0 0.2 0.4 Λ 0.25 0.50 0.75 1.00 1.25 1.50 1.75 G Reuter FP Disfavored strong − gravity regime Positivity-bounds satisfied Positivity- bounds violated 1 FIG. 2. Region in the G −Λ plane, in which the positiv- ity bounds on c1,2 are satisfied at k → 0. The gravitational couplings are treated as input parameters, and their scale- dependence is approximated by simple Heaviside functions, which is a good approximation of the actual scale dependence. The green region indicates where the positivity bounds are satisfied, while the red region indicates where they are vio- lated. The gray region indicates a region where new relevant directions are present in the matter sector. The purple dot indicates the Reuter fixed point obtained within a specific setup. functions to obtain c1,2(k) and calculate c1,2(k → 0). For this analysis we approximate the scale-dependent grav- itational couplings G(k) and Λ(k) with Heaviside func- tions, i.e., they assume the input values (G, Λ) above the Planck-scale, and are zero below the Planck scale. This approximation is justified, since G ∼k2 and Λ(k → 0) ≈0, and hence very quickly approach zero below the Planck scale. We also check explicitly that for the case, where G(k) and Λ(k) are described by fixed-point trajectories, this approximation leads to the same conclusion regard- ing positivity bounds. The result of this analysis is shown in Fig. 2. There is a region in the G−Λ-plane, in which the positivity bounds are satisfied. The Reuter fixed point is located inside this region and well away from its boundaries. There is also a region, in which the positivity bounds are vio- lated, as well as a region in which some truncations do not show a fixed point and other truncations only show fixed points at which higher-order couplings become rel- evant. This region is referred to as “disfavored strong- gravity regime”, because the non-existence of fixed points or presence of extra relevant directions is tied to gravity fluctuations becoming nonperturbatively strong, see [95– 97, 106, 121] for details.5 From Fig. 2, we can conclude that (i) it is nontriv- ial that the IR theory resulting from the Reuter fixed point satisfies positivity bounds, because this is not the case for all values of G and Λ; (ii) that our result on positivity bounds is robust, because the Reuter fixed point is not located close to the boundary of the re- gion which satisfies positivity bounds; (iii) that positivity bounds can be used to place nontrivial constraints on the gravitational parameters. The third is particularly im- portant, because without considering positivity bounds, there would not be any way (within our truncation) to see that the assumptions going into derivations of positivity bounds may be violated for these parameter values. Only once positivity bounds are included, can we understand that not all parameter values G, Λ outside the disfavored strong-gravity regime are actually physically viable, be- cause some of them lead to violations of causality in the propagation of photons [100]. Future perspectives: In this work, we have pioneered a new way to detect violations of unitarity, locality, (micro- ) causality or Lorentz symmetry in asymptotically safe gravity. Because in our setup positivity bounds for the lowest-order four-photon couplings in an asymptotically safe theory hold except in a small (and disfavored, see supplemental material) interval of values of a gravita- tional gauge parameter, we conclude that we do not find convincing evidence for the violation of these properties. Our work shows that positivity bounds can be tested meaningfully in asymptotic safety, even using existing results for higher-order interactions, from which Wilson coefficients can be extracted. Several extensions of our study are technically possible and physically informative. First, the inclusion of couplings that contribute to the 2- 2- photon amplitude at higher level in momenta provides further tests for the robustness of our results, but also leads to additional positivity bounds [98–100]. Second, the (truncated) 2-2-photon scattering amplitude can be calculated from the Wilson coefficients, complementing the first steps towards scattering amplitudes undertaken in [122, 123]. Third, positivity bounds for other matter fields, e.g., scalars [72], for which higher-order terms are already rather well-explored [121, 124–127], can also be explored. Fourth, the constraints on the grav- itational parameter space can be compared with other phenomenological constraints, such as, e.g., viability of the Standard Model of particle physics [128–131]. Fifth, instead of working in a Euclidean regime, beta functions could be calculated either in the presence of a (Euclidean) foliation which ensures that a continuation to Lorentzian signature is more readily available, as in [29, 30, 132–138], or even directly in Lorentzian signature [64], see also [139–142]. By carrying out this program, we can collect evidence as to whether asymptotically safe gravity constitutes a physically viable quantum theory of gravity. Our present work paves the way to achieve this. Acknowledgements: We thank A. Platania, B. Knorr, C. de Rham and A. Tolley for discussions. AE is supported by a grant (no 29405) by VILLUM Fonden. AE also gratefully acknowledges support by the Deutsche Forschungsgemeinschaft (DFG) under Grant No 406116891 within the Research Training Group RTG 2522/1 during an early stage of this work. This work is also supported by the Perimeter Institute for Theoretical Physics. Research at Perimeter Institute is supported in part by the Government of Canada through the Depart- ment of Innovation, Science and Economic Development and by the Province of Ontario through the Ministry of Colleges and Universities.6 SUPPLEMENTARY MATERIAL Here, we investigate the robustness of our results. We use the principle of minimum sensitivity to identify a regime of the gauge parameter in which results are sta- ble under extensions of the truncation and only very weakly dependent on the gauge parameter. Working in this regime, we then show how the removal of F6 opera- tors from our study has only a subleading, quantitative impact on our results, but does not alter the qualita- tive conclusions. Furthermore, we investigate the gauge- dependence of the positivity bounds in our system, show- ing that indeed the violation or non-violation of positiv- ity bounds is stable under small changes of the gauge- parameter in the regime singled out by the principle of minimal sensitivity. Technical setup and gauge choice For completeness, let us first provide some details on the technical setup. We use the functional renormal- ization group (FRG) [5, 12–14] to extract the scale- dependence of the couplings in the photon-gravity sys- tem. The key object of the FRG is the scale-dependent effective action Γ k, which includes all quantum fluctua- tions above the IR-cutoff scale k. The FRG furthermore provides a functional differential equation, k∂kΓk = 1 2Tr[k∂kRk (Γ(2) k +Rk) −1 ] , (7) where Γ (2) k is the second derivative of Γ k with respect to the fields, and where Rk is the IR regulator. In the path integral, it acts akin a momentum-dependent mass, and hence implements the Wilsonian idea of integrating out quantum fluctuations according to their momentum scale. From the flow equation Eq. (7), we can extract the scale dependence of all couplings of the system, by projecting onto suitable field monomials or functionals. While Γk in principle contains all possible operators that are compatible with the symmetries of the system, in practical computations, it is typically truncated to a fi- nite set of operators. Systematic extensions of this set of operators are then necessary to i) estimate the system- atic uncertainties in a given truncation, and ii) to obtain qualitatively reliable results of physical observables. For detailed reviews on the FRG, and its applications in grav- ity see, e.g., [36, 37, 105, 143]. In the present work we approximate the gravitational dy- namics by the Einstein-Hilbert action, see Eq. (3), and the dynamics of the photon sector with Γphotons k =1 4 ∫ d4x√g F2 +∫ d4x√g (c1 k4 (F2)2 + c2 k4 F4) +∫ d4x√g (d1 k8 (F2)3 + d2 k4 F2 F4) , (8) where we define F2 = gµνgκλFµκFνλ , (9) F4 = gµνgκλgρσgζτ FµκFλρFσζ Fτν , (10) which is the leading-order photon EFT-action Eq. (2), supplemented with two six-photon operators, corre- sponding to d1,2. The interactions associated to c1,2 (d1,2) form a basis for the dimension-eight (twelve) op- erators at fourth (sixth) order in field strengths without additional derivatives, which can be shown using the re- lations in [96, 102, 103]. From a phenomenological per- spective, we are mainly interested in the scale dependence of the four-photon interactions c1,2(k) [95, 96, 106], and we will use the six-photon interactions, computed in [97], to assess the robustness of our results. To this end, we will refer to a truncation where we set d1,2 = 0 as ’ F4- truncation’, while we refer to the system including the interactions associated to d1,2 as ’F6-truncation’. To extract the scale dependence of couplings, we ex- pand the action around an auxiliary background, using a linear parameterization of metric fluctuations, i.e., gµν = ¯gµν + √ 16 πk−2G(k)hµν . (11) In order to compute the impact of metric and photon fluctuations, we need to fix the gauge in both sectors. Hence, we introduce the gauge-fixing actions Sgf, h= 1 32 π αh G(k)k−2 ∫ d4x√¯g Fµ¯gµνFν , (12) with Fµ =(δµκ ¯Dλ − 1 +βh 4 δκλ ¯Dµ)hκλ , (13) and Sgf, A= 1 2αA ∫ d4x√¯g ( ¯DνAν)( ¯DµAµ) , (14) where we have introduced a total of three gauge parame- ters αh, αA, and βh. The first two control how strictly the gauge condition is imposed, while βh determines which of the scalar modes in the metric-sector is physical. We choose the Landau gauge in both sectors, i.e., αh → 0 , and αA → 0 , (15) which has been shown to be a preferred choice [120, 144]. We then extract the scale dependence of the photon in- teractions c1,2, and d1,2 as a function of βh, and take7 the scale dependence of G and Λ as a function of βh from [107], supplemented with the photon contribution from [108]. We note that in the presence of gravity, couplings such as c1,2 and d1,2 are automatically in- duced in the UV, and cannot be consistently set to zero, see also Eq. (4). More generally speaking, there is strong evidence that at an asymptotically safe fixed point for gravity and matter, all couplings that respect the symmetries of the system, including matter self- interactions [95, 96, 106, 121, 124–129, 145–148] and non- minimal interactions [116, 126, 127, 129, 149] are non- vanishing. In turn, those operators that break a symme- try of the system can consistently be set to zero, see, e.g., [107, 129, 137, 138, 150–152]. While the flow equation Eq. (7) is in principle exact, within truncation also physical quantities, such as critical exponents, depend on the gauge parameters of the gravi- tational sector. Critical exponents define the universality class of a fixed point and are defined by Θi =−eig [∂βgi ∂gj ]∣ gl=gl,∗ , (16) where gi are all couplings of the system. In order to mini- mize the residual gauge dependence of our truncated sys- tem, we apply the principle of minimal sensitivity (PMS) to the critical exponents of the system in order to choose an appropriate value for βh. The PMS has successfully been employed in the context of the FRG to minimize residual regulator dependence in [153], and in quantum gravity in [28]. In Fig. 3 we show the set of critical ex- ponents in the F4- and the F6-truncation as a function of the gauge parameter βh. Here, Θ 1,2 correspond to the critical exponents of the gravitational couplings. In our system, they are not impacted by the truncation in the matter sector. We see that both the gravitational and the photon critical exponents vary strongly around βh = 0, and become constant towards βh → −∞. This is the case both in the F4- (solid lines), and the F6- trun- cation (dashed lines). This indicates that negative values of βh are preferred by a PMS on the gauge parameter. Hence, we choose βh → −∞, (17) for the analysis presented in this paper. In this gauge, the beta-functions feature poles at Λ = −1 4 and Λ = 1 2 , which correspond to the poles of the scalar and the transverse-traceless mode of metric fluctuations. This pole structure limits the range of admissible fixed-point values to Λ ∈ (−1 4 , 1 2 ), which are the only values that can result in a nearly vanishing cosmological constant at low scales, in the present approximation. Note that a more careful study of gravitational dynamics would treat the avatar of the cosmological constant appearing in the propagator differently from the coupling appear- ing as the momentum-independent part of pure-gravity vertices, see, e.g., [154–156]. FIG. 3. Critical exponents at the fixed point of the gravity- photon system for different values of the gauge parameter βh. Θ1,2 correspond to the critical exponents of the gravitational couplings G and Λ, and are hence independent of the trunca- tion in the matter sector. The solid (dashed) lines at negative Θi indicate the critical exponents of the matter couplings in the F4- and the F6-truncation. Positive critical exponents indicate relevant directions and negative critical exponents irrelevant ones. Anticipating a study of other values of the gauge pa- rameter below, we explore whether our truncation ap- pears equally robust for all values of the gauge parame- ter.4 We do so by comparing the fixed-point value of the Reuter fixed point, G∗, to the value Gcrit, at which the disfavored strong-gravity regime begins. In this regime, our lowest-order truncation does not feature a fixed point for the four-photon couplings, whereas a larger trunca- tion features a fixed point, but with additional relevant couplings. Such drastic changes under extensions of the truncation indicate that more extended truncations are needed to achieve robust results which are (apparently) converged. Furthermore, in scalar-tensor theories, it has been explicitly explored that truncations close to such a disfavored regime are not converged yet in small trunca- tions, see [121]. By comparing Gcrit/G∗ in the F4 and F6 truncation, we see that the system appears stable under extensions of the truncation at large negative βh, but not stable for βh ≳ −6, cf. Fig. 4, because the ratio Gcrit/G∗ changes strongly between the two truncations in this regime. Stability under an extension of the truncation To assess the robustness of our analysis, we compare the regions where positivity bounds for c1,2 are satisfied 4 Let us highlight that we explore βh at a fixed value α = 0; a study of other values of α, e.g., α = 1 as used in [68] would also be highly informative and is postponed to future work.8 FIG. 4. We show the ratio Gcrit(Λ∗)/G∗. At Gcrit, the fixed point for the four-photon couplings is lost to a fixed point collision in the F4 truncation (blue continuous line) and ac- quires a relevant direction in the F6 truncation (red dashed line). Because the difference between a fixed-point collision and a real fixed point with an extra relevant direction is ma- jor, we consider values G > Gcrit unreliable in the present truncation. Thus, if the fixed-point value G∗ gets too close to Gcrit, we no longer fully trust the results obtained in these comparatively small truncations. The jumps are caused by jumps in Gcrit., or the overall disappearence of disfavored re- gions for specific gauge parameters βh. and violated in the F4- and F6-truncation, and within the Heaviside approximation for the scale-dependence of G and Λ. The F4-truncation is the lowest order at which positivity bounds in the photon sector of asymptotically safe quantum gravity can be tested. In computations based on the FRG, the scale dependence of n-point func- tions generically depends on n +1 and n +2-point func- tions, which in this case includes six-photon interactions. These additional interactions impact c1,2 in two ways: first, the asymptotically safe fixed point changes, and second, the flow towards k → 0 is modified, resulting in a modified trajectory of the RG-flow, ultimately resulting in changed Wilson coefficients at k → 0. Hence, if the regions where positivity bounds are sat- isfied change significantly under the inclusion of F6 op- −0.2 0.0 0.2 0.4 Λ 0.25 0.50 0.75 1.00 1.25 1.50 1.75 G Reuter FP 1 FIG. 5. Comparison of regions where positivity bounds are satisfied or violated in theF4- and the F6-truncations. If a re- gion has changed its status from the F4-to the F6-truncation, it is indicated in dark green (red) if the positivity bound is satisfied (violated) in the F6- truncation. Light colors indi- cate regions where both truncations lead to the same result, and light green (red) indicate where the positivity bounds for c1,2 are satisfied (violated) in both truncations. erators, this indicates that our truncation is not yet con- verged and any statements on positivity bounds being satisfied or violated come with a large systematic uncer- tainty. Conversely, if the status of positivity bounds only changes in small regions in theG-Λ plane when increasing the truncation, this indicates some amount of apparent convergence, and reliability of our truncation. In Fig. 5, we show which regions in the G-Λ plane change their status regarding positivity bounds from the F4- to the F6-truncation. Here, dark green (red) in- dicates that positivity bounds are satisfied (violated) in the F6-truncation, but have not been satisfied (vio- lated) in that region in the F4-truncation. Light col- ors (green, red, blue), indicate regions where the sta- tus of positivity bounds has not changed from the F4- to the F6-truncation. We observe that a large region in the gravitational parameter space is colored lightly, meaning that the status of positivity bounds is stable under extensions of the truncation. Only narrow bands have changed their status. Among those regions, only a small band around Λ ≈−0.2 violates positivity in the F6- truncation, while positivity bounds are satisfied in the F4-truncation in that region. In the other regions, pos- itivity either changes from violated to satisfied, or from disfavored strong-gravity regions to either satisfaction or violation of positivity bounds in the F6-truncation.9 FIG. 6. We show the RG trajectory in the c1,2 plane; the scale dependence of the other couplings is not included in the plot, but enters in the calculation of c1,2(k). The markers in- dicates the Reuter fixed point of asymptotically safe quantum gravity Eq. (6) for different choices of the gauge parameter βh. The colored solid (blue dashed) line indicates the RG trajectory towards k → 0 for βh → −∞ (βh = 1). The green shaded region indicates where both positivity bounds Eq. (2) are satisfied. The red region indicates where one of the posi- tivity bounds is violated. We observe that for all finite k, the scale-dependent couplings c1,2(k) either satisfy or violate the positivity bounds, depending on the quadrant the fixed-point value lies in. Stability under changes of the gauge parameter As mentioned before, in truncations of the scale depen- dent effective action Γk, also physical quantities, like crit- ical exponents, depend on unphysical parameters, such as the gauge parameter βh. We have argued before that a principle of minimal sensitivity applied on the critical ex- ponents of the Reuter fixed point singles out βh → −∞ as a preferred gauge choice, and all results presented in the main part of this paper were obtained for that choice. Here we focus on another physical quantity, namely the question whether positivity bounds (at k → 0) are satis- fied or not for theories emanating from the Reuter fixed point. In Fig. 6 we show the RG trajectories emanating from the Reuter fixed point for different choices of βh in the c1,2 plane. The markers indicate the location of the Reuter fixed point, while the colored solid line (the blue dashed line) indicates the trajectories for βh → −∞ (βh =1). We see that c1∗ and c2∗ exchange their sign at about βh ≈ −1. Furthermore, we see that for all shown −100 −80 −60 −40 −20 0 βh 0 20 40 4c1 + 3c2 |4c1 + c2| F 4 − trunc. F 6 − trunc. 1 FIG. 7. We show the value of the stronger positivity condi- tion in Eq. (2) at k → 0 for trajectories emanating from the Reuter fixed point as a function of the gauge parameter βh in the F4- (purple dashed line) and theF6-truncation (blue solid line). For the scale dependence of the gravitational couplings we used the Heaviside approximation. The green region in- dicates 4c1+3c2 ∣4c1+c2∣ >1, i.e., satisfied positivity bounds, while the red region marks 4c1+3c2 ∣4c1+c2∣ <1. FIG. 8. We show the ratio of Wilson coefficients c1,2 at k → 0 for trajectories emanating from the Reuter fixed point as a function of the gauge parameter βh. For the scale de- pendence of the gravitational couplings we used the Heaviside approximation. We see that this ratio is very stable as a func- tion of βh, except for a divergence around βh ≈ −1, where c1 and c2 change sign shortly after one another. Since the sign- change does not occur at exactly the same value for βh, the ratio diverges. We also see quantitative agreement between the F4- and the F6-truncation, both in the value of the ratio, as well as in the location of the sign-change. values of βh, the Reuter fixed point results in an almost constant value for c1,∗ c2,∗ . Correspondingly, for βh ≳ −1, both positivity-bounds Eq. (2) are violated along the entire RG-trajectory, while for βh ≲ −1 the positivity bounds are satisfied along the entire RG-trajectory. In Fig. 7 we show the value of the stronger positiv- ity bound at k → 0 for trajectories emanating from the10 Reuter fixed point, and as a function of βh. We employed the Heaviside approximation for the scale dependence of the gravitational couplings. We see that indeed the com- bination 4c1+3c2 ∣4c1+c2∣ is very stable for negative βh, and only changes significantly around βh ≈−4, where it then drops below 1 around βh ≈ −1, indicating the violation of pos- itivity bounds. This behavior, namely the stability for large negative βh, and rapid changes for βh ≈ 0 are in line with the behavior of critical exponents, see Fig. 3. While this holds independently for the F4- and the F6- truncation, we see a large quantitative difference between both truncations. However, as we see in Fig. 8, this it not caused by a large truncation-dependence of the Wilson coefficients c1,2, but by a near-cancellation in the denom- inator: we find that the approximation 4 c1 ≈−c2 is very stable as a function of βh, up to βh ≈ −1. Furthermore, we see that this approximation becomes more accurate in the F6-truncation. Since the combination 4 c1 +c2 is pre- cisely the denominator of the positivity bound Eq. (2), the value of the positivity condition, which is shown in Fig. 7, increases significantly, due to the denominator being close to zero. We conclude that there are gauges where RG trajec- tories emanating from the corresponding Reuter fixed points violate the positivity bounds, but that those gauge choices lie within (or very close to) the range that we have conservatively excluded above. ∗ eichhorn@sdu.dk † anpe519@student.sdu.dk ‡ mschiffer@perimeterinstitute.ca [1] A. Adams, N. Arkani-Hamed, S. Dubovsky, A. Nico- lis, and R. Rattazzi, JHEP 10, 014 (2006), arXiv:hep- th/0602178. [2] T. N. Pham and T. N. Truong, Phys. Rev. D 31, 3027 (1985). [3] M. R. Pennington and J. Portoles, Phys. Lett. B 344, 399 (1995), arXiv:hep-ph/9409426. [4] S. Weinberg, “ULTRAVIOLET DIVERGENCES IN QUANTUM THEORIES OF GRAVITATION,” in General Relativity: An Einstein Centenary Survey (1980) pp. 790–831. [5] M. Reuter, Phys. Rev. D 57, 971 (1998), arXiv:hep- th/9605030. [6] R. Gastmans, R. Kallosh, and C. Truffin, Nucl. Phys. B 133, 417 (1978). [7] S. M. Christensen and M. J. Duff, Phys. Lett. B 79, 213 (1978). [8] H. Kawai and M. Ninomiya, Nucl. Phys. B 336, 115 (1990). [9] R. Martini, A. Ugolotti, F. Del Porro, and O. Zanusso, Eur. Phys. J. C 81, 916 (2021), arXiv:2103.12421 [hep- th]. [10] R. Martini, G. P. Vacca, and O. Zanusso, “Per- turbative Approaches to Nonperturbative Quantum Gravity,” in Handbook of Quantum Gravity , edited by C. Bambi, L. Modesto, and I. Shapiro (2024) pp. 1–46, arXiv:2210.13910 [hep-th]. [11] R. Martini, D. Sauro, and O. Zanusso, Phys. Rev. D 109, 065014 (2024), arXiv:2302.14804 [hep-th]. [12] C. Wetterich, Phys. Lett. B 301, 90 (1993), arXiv:1710.05815 [hep-th]. [13] T. R. Morris, Int. J. Mod. Phys. A 9, 2411 (1994), arXiv:hep-ph/9308265. [14] U. Ellwanger, Z. Phys. C 62, 503 (1994), arXiv:hep- ph/9308260. [15] W. Souma, Prog. Theor. Phys. 102, 181 (1999), arXiv:hep-th/9907027. [16] O. Lauscher and M. Reuter, Phys. Rev. D 65, 025013 (2002), arXiv:hep-th/0108040. [17] M. Reuter and F. Saueressig, Phys. Rev. D 65, 065016 (2002), arXiv:hep-th/0110054. [18] O. Lauscher and M. Reuter, Phys. Rev. D 66, 025026 (2002), arXiv:hep-th/0205062. [19] D. F. Litim, Phys. Rev. Lett. 92, 201301 (2004), arXiv:hep-th/0312114. [20] A. Codello and R. Percacci, Phys. Rev. Lett. 97, 221301 (2006), arXiv:hep-th/0607128. [21] P. F. Machado and F. Saueressig, Phys. Rev. D 77, 124045 (2008), arXiv:0712.0445 [hep-th]. [22] A. Codello, R. Percacci, and C. Rahmede, Annals Phys. 324, 414 (2009), arXiv:0805.2909 [hep-th]. [23] D. Benedetti, P. F. Machado, and F. Saueressig, Mod. Phys. Lett. A24, 2233 (2009), arXiv:0901.2984 [hep-th]. [24] K. Falls, N. Ohta, and R. Percacci, Phys. Lett. B 810, 135773 (2020), arXiv:2004.04126 [hep-th]. [25] A. Baldazzi and K. Falls, Universe 7, 294 (2021), arXiv:2107.00671 [hep-th]. [26] B. Knorr, SciPost Phys. Core 4, 020 (2021), arXiv:2104.11336 [hep-th]. [27] Y. Kluth and D. F. Litim, Phys. Rev. D 106, 106022 (2022), arXiv:2202.10436 [hep-th]. [28] A. Baldazzi, K. Falls, Y. Kluth, and B. Knorr, (2023), arXiv:2312.03831 [hep-th]. [29] F. Saueressig and J. Wang, JHEP 09, 064 (2023), arXiv:2306.10408 [hep-th]. [30] G. Korver, F. Saueressig, and J. Wang, (2024), arXiv:2402.01260 [hep-th]. [31] B. B¨ urger, J. M. Pawlowski, M. Reichert, and B.-J. Schaefer, (2019), arXiv:1912.01624 [hep-th]. [32] A. Pastor-Guti´ errez, J. M. Pawlowski, and M. Reichert, SciPost Phys. 15, 105 (2023), arXiv:2207.09817 [hep- th]. [33] A. Eichhorn, Front. Astron. Space Sci. 5, 47 (2019), arXiv:1810.07615 [hep-th]. [34] A. Bonanno, A. Eichhorn, H. Gies, J. M. Pawlowski, R. Percacci, M. Reuter, F. Saueressig, and G. P. Vacca, Front. in Phys. 8, 269 (2020), arXiv:2004.06810 [gr-qc]. [35] A. Eichhorn and M. Schiffer, (2022), arXiv:2212.07456 [hep-th]. [36] F. Saueressig, “The Functional Renormalization Group in Quantum Gravity,” (2023) arXiv:2302.14152 [hep- th]. [37] J. M. Pawlowski and M. Reichert, (2023), arXiv:2309.10785 [hep-th]. [38] B. Knorr, C. Ripken, and F. Saueressig, (2022), arXiv:2210.16072 [hep-th]. [39] T. R. Morris and D. Stulga, “The Functional f(R) Ap- proximation,” (2023) arXiv:2210.11356 [hep-th]. [40] C. Wetterich, “Quantum Gravity and Scale Symmetry11 in Cosmology,” (2023) arXiv:2211.03596 [gr-qc]. [41] A. Platania, “Black Holes in Asymptotically Safe Grav- ity,” (2023) arXiv:2302.04272 [gr-qc]. [42] K. Falls, D. F. Litim, K. Nikolakopoulos, and C. Rahmede, (2013), arXiv:1301.4191 [hep-th]. [43] H. Gies, B. Knorr, S. Lippoldt, and F. Saueressig, Phys. Rev. Lett. 116, 211302 (2016), arXiv:1601.01800 [hep- th]. [44] T. Denz, J. M. Pawlowski, and M. Reichert, Eur. Phys. J. C 78, 336 (2018), arXiv:1612.07315 [hep-th]. [45] K. Falls, C. R. King, D. F. Litim, K. Nikolakopou- los, and C. Rahmede, Phys. Rev. D 97, 086006 (2018), arXiv:1801.00162 [hep-th]. [46] R. P. Woodard, Scholarpedia 10, 32243 (2015), arXiv:1506.02210 [hep-th]. [47] N. Barnaby and N. Kamran, JHEP 02, 008 (2008), arXiv:0709.3968 [hep-th]. [48] J. Gleyzes, D. Langlois, F. Piazza, and F. Vernizzi, Phys. Rev. Lett. 114, 211101 (2015), arXiv:1404.6495 [hep-th]. [49] D. Langlois and K. Noui, JCAP 02, 034 (2016), arXiv:1510.06930 [gr-qc]. [50] L. Buoninfante, G. Lambiase, Y. Miyashita, W. Takebe, and M. Yamaguchi, Phys. Rev. D 101, 084019 (2020), arXiv:2001.07830 [hep-th]. [51] C. Deffayet, S. Mukohyama, and A. Vikman, Phys. Rev. Lett. 128, 041301 (2022), arXiv:2108.06294 [gr- qc]. [52] C. Deffayet, A. Held, S. Mukohyama, and A. Vikman, JCAP 11, 031 (2023), arXiv:2305.09631 [gr-qc]. [53] J. F. Donoghue and G. Menezes, Phys. Rev. D 100, 105006 (2019), arXiv:1908.02416 [hep-th]. [54] J. F. Donoghue and G. Menezes, Phys. Rev. Lett. 123, 171601 (2019), arXiv:1908.04170 [hep-th]. [55] J. F. Donoghue and G. Menezes, Phys. Rev. D 104, 045010 (2021), arXiv:2105.00898 [hep-th]. [56] D. Anselmi and M. Piva, Phys. Rev. D 96, 045009 (2017), arXiv:1703.05563 [hep-th]. [57] D. Anselmi, JHEP 02, 141 (2018), arXiv:1801.00915 [hep-th]. [58] D. Anselmi and M. Piva, JHEP 11, 021 (2018), arXiv:1806.03605 [hep-th]. [59] A. Held and H. Lim, Phys. Rev. D 104, 084075 (2021), arXiv:2104.04010 [gr-qc]. [60] A. Held and H. Lim, Phys. Rev. D 108, 104025 (2023), arXiv:2306.04725 [gr-qc]. [61] L. Bosma, B. Knorr, and F. Saueressig, Phys. Rev. Lett. 123, 101301 (2019), arXiv:1904.04845 [hep-th]. [62] B. Knorr and M. Schiffer, Universe 7, 216 (2021), arXiv:2105.04566 [hep-th]. [63] A. Bonanno, T. Denz, J. M. Pawlowski, and M. Re- ichert, SciPost Phys. 12, 001 (2022), arXiv:2102.02217 [hep-th]. [64] J. Fehre, D. F. Litim, J. M. Pawlowski, and M. Reichert, Phys. Rev. Lett. 130, 081501 (2023), arXiv:2111.13232 [hep-th]. [65] T. Draper, B. Knorr, C. Ripken, and F. Saueressig, Phys. Rev. Lett. 125, 181301 (2020), arXiv:2007.00733 [hep-th]. [66] A. Platania and C. Wetterich, Phys. Lett. B811, 135911 (2020), arXiv:2009.06637 [hep-th]. [67] A. Platania, JHEP 09, 167 (2022), arXiv:2206.04072 [hep-th]. [68] A. Platania and B. Knorr, to appear (2024). [69] M. F. Paulos, J. Penedones, J. Toledo, B. C. van Rees, and P. Vieira, JHEP 11, 133 (2017), arXiv:1607.06109 [hep-th]. [70] M. Kruczenski, J. Penedones, and B. C. van Rees, (2022), arXiv:2203.02421 [hep-th]. [71] J. Elias Miro, A. Guerrieri, and M. A. Gumus, JHEP 05, 001 (2023), arXiv:2210.01502 [hep-th]. [72] C. de Rham, S. Melville, A. J. Tolley, and S.-Y. Zhou, Phys. Rev. D 96, 081702 (2017), arXiv:1702.06134 [hep- th]. [73] C. de Rham, S. Melville, A. J. Tolley, and S.-Y. Zhou, JHEP 03, 011 (2018), arXiv:1706.02712 [hep-th]. [74] A. J. Tolley, Z.-Y. Wang, and S.-Y. Zhou, JHEP 05, 255 (2021), arXiv:2011.02400 [hep-th]. [75] L. Alberte, C. de Rham, S. Jaitly, and A. J. Tol- ley, Phys. Rev. D 103, 125020 (2021), arXiv:2012.05798 [hep-th]. [76] J. Davighi, S. Melville, and T. You, JHEP 02, 167 (2022), arXiv:2108.06334 [hep-th]. [77] B. Bellazzini, M. Riembau, and F. Riva, Phys. Rev. D 106, 105008 (2022), arXiv:2112.12561 [hep-th]. [78] M. Chala and J. Santiago, Phys. Rev. D 105, L111901 (2022), arXiv:2110.01624 [hep-ph]. [79] C. de Rham, S. Melville, and A. J. Tolley, JHEP 04, 083 (2018), arXiv:1710.09611 [hep-th]. [80] C. de Rham, S. Melville, A. J. Tolley, and S.-Y. Zhou, JHEP 03, 182 (2019), arXiv:1804.10624 [hep-th]. [81] L. Alberte, C. de Rham, A. Momeni, J. Rumbutis, and A. J. Tolley, JHEP 03, 097 (2020), arXiv:1910.11799 [hep-th]. [82] S. Melville and J. Noller, Phys. Rev. D 101, 021502 (2020), [Erratum: Phys.Rev.D 102, 049902 (2020)], arXiv:1904.05874 [astro-ph.CO]. [83] L. Alberte, C. de Rham, S. Jaitly, and A. J. Tol- ley, Phys. Rev. D 102, 125023 (2020), arXiv:2007.12667 [hep-th]. [84] C. de Rham, S. Melville, and J. Noller, JCAP 08, 018 (2021), arXiv:2103.06855 [astro-ph.CO]. [85] L. Alberte, C. de Rham, S. Jaitly, and A. J. Tolley, Phys. Rev. Lett. 128, 051602 (2022), arXiv:2111.09226 [hep-th]. [86] M. Herrero-Valea, A. S. Koshelev, and A. Tokareva, Phys. Rev. D 106, 105002 (2022), arXiv:2205.13332 [hep-th]. [87] Y. Hamada, R. Kuramochi, G. J. Loges, and S. Naka- jima, JHEP 05, 076 (2023), arXiv:2301.01999 [hep-th]. [88] C. de Rham, S. Kundu, M. Reece, A. J. Tolley, and S.- Y. Zhou, in Snowmass 2021 (2022) arXiv:2203.06805 [hep-th]. [89] T. Grall and S. Melville, Phys. Rev. D 105, L121301 (2022), arXiv:2102.05683 [hep-th]. [90] S. Melville and J. Noller, JCAP 06, 031 (2022), arXiv:2202.01222 [hep-th]. [91] P. Creminelli, O. Janssen, and L. Senatore, JHEP 09, 201 (2022), arXiv:2207.14224 [hep-th]. [92] A. Guerrieri, J. Penedones, and P. Vieira, Phys. Rev. Lett. 127, 081601 (2021), arXiv:2102.02847 [hep-th]. [93] A. Guerrieri, H. Murali, J. Penedones, and P. Vieira, JHEP 06, 064 (2023), arXiv:2212.00151 [hep-th]. [94] J. Berman, H. Elvang, and A. Herderschee, JHEP 03, 021 (2024), arXiv:2310.10729 [hep-th]. [95] N. Christiansen and A. Eichhorn, Phys. Lett. B 770, 154 (2017), arXiv:1702.07724 [hep-th]. [96] A. Eichhorn, J. H. Kwapisz, and M. Schiffer, Phys. Rev.12 D 105, 106022 (2022), arXiv:2112.09772 [gr-qc]. [97] M. Schiffer, to appear (2024). [98] J. Henriksson, B. McPeak, F. Russo, and A. Vichi, JHEP 06, 158 (2022), arXiv:2107.13009 [hep-th]. [99] K. H¨ aring, A. Hebbar, D. Karateev, M. Meineri, and J. a. Penedones, (2022), arXiv:2211.05795 [hep-th]. [100] M. Carrillo Gonz´ alez, C. de Rham, S. Jaitly, V. Pozsgay, and A. Tokareva, (2023), arXiv:2307.04784 [hep-th]. [101] J. Henriksson, B. McPeak, F. Russo, and A. Vichi, JHEP 08, 184 (2022), arXiv:2203.08164 [hep-th]. [102] W. Dittrich and H. Gies, Probing the quantum vacuum. Perturbative effective action approach in quantum elec- trodynamics and its application , Vol. 166 (2000). [103] B. Knorr, Asymptotic safety in QFT: from quantum gravity to graphene, Ph.D. thesis, Jena U. (2017). [104] M. Carrillo Gonzalez, C. de Rham, V. Pozsgay, and A. J. Tolley, Phys. Rev. D 106, 105018 (2022), arXiv:2207.03491 [hep-th]. [105] N. Dupuis, L. Canet, A. Eichhorn, W. Metzner, J. M. Pawlowski, M. Tissier, and N. Wschebor, Phys. Rept. 910, 1 (2021), arXiv:2006.04853 [cond-mat.stat-mech]. [106] A. Eichhorn and M. Schiffer, Phys. Lett. B 793, 383 (2019), arXiv:1902.06479 [hep-th]. [107] A. Eichhorn and S. Lippoldt, Phys. Lett. B 767, 142 (2017), arXiv:1611.05878 [gr-qc]. [108] P. Don` a, A. Eichhorn, and R. Percacci, Phys. Rev. D 89, 084035 (2014), arXiv:1311.2898 [hep-th]. [109] G. Gubitosi, R. Ooijer, C. Ripken, and F. Saueressig, JCAP 12, 004 (2018), arXiv:1806.10147 [hep-th]. [110] A. Baldazzi, R. Percacci, and V. Skrinjar, Class. Quant. Grav. 36, 105008 (2019), arXiv:1811.03369 [gr-qc]. [111] M. R. Niedermaier, Phys. Rev. Lett. 103, 101303 (2009). [112] K. Falls, D. F. Litim, K. Nikolakopoulos, and C. Rahmede, Phys. Rev. D 93, 104022 (2016), arXiv:1410.4815 [hep-th]. [113] A. Eichhorn, P. Labus, J. M. Pawlowski, and M. Re- ichert, SciPost Phys. 5, 031 (2018), arXiv:1804.00012 [hep-th]. [114] K. G. Falls, D. F. Litim, and J. Schr¨ oder, Phys. Rev. D 99, 126015 (2019), arXiv:1810.08550 [gr-qc]. [115] A. Eichhorn, S. Lippoldt, J. M. Pawlowski, M. Re- ichert, and M. Schiffer, Phys. Lett. B 792, 310 (2019), arXiv:1810.02828 [hep-th]. [116] A. Eichhorn, S. Lippoldt, and M. Schiffer, Phys. Rev. D 99, 086002 (2019), arXiv:1812.08782 [hep-th]. [117] A. Eichhorn and M. Pauly, Phys. Rev. D 103, 026006 (2021), arXiv:2009.13543 [hep-th]. [118] M. Becker, A. Kurov, and F. Saueressig, (2024), arXiv:2402.01075 [hep-th]. [119] H. Gies, B. Knorr, and S. Lippoldt, Phys. Rev. D 92, 084020 (2015), arXiv:1507.08859 [hep-th]. [120] B. Knorr and S. Lippoldt, Phys. Rev. D 96, 065020 (2017), arXiv:1707.01397 [hep-th]. [121] G. P. de Brito, B. Knorr, and M. Schiffer, Phys. Rev. D 108, 026004 (2023), arXiv:2302.10989 [hep-th]. [122] T. Draper, B. Knorr, C. Ripken, and F. Saueressig, JHEP 11, 136 (2020), arXiv:2007.04396 [hep-th]. [123] B. Knorr, S. Pirlo, C. Ripken, and F. Saueressig, (2022), arXiv:2205.01738 [hep-th]. [124] A. Eichhorn, Phys. Rev. D 86, 105021 (2012), arXiv:1204.0965 [gr-qc]. [125] G. P. de Brito, A. Eichhorn, and R. R. L. d. Santos, JHEP 11, 110 (2021), arXiv:2107.03839 [gr-qc]. [126] C. Laporte, A. D. Pereira, F. Saueressig, and J. Wang, JHEP 12, 001 (2021), arXiv:2110.09566 [hep-th]. [127] B. Knorr, (2022), arXiv:2204.08564 [hep-th]. [128] A. Eichhorn, A. Held, and J. M. Pawlowski, Phys. Rev. D 94, 104027 (2016), arXiv:1604.02041 [hep-th]. [129] A. Eichhorn and A. Held, Phys. Rev. D 96, 086025 (2017), arXiv:1705.02342 [gr-qc]. [130] A. Eichhorn and A. Held, Phys. Lett. B777, 217 (2018), arXiv:1707.01107 [hep-th]. [131] A. Eichhorn and A. Held, Phys. Rev. Lett. 121, 151302 (2018), arXiv:1803.04027 [hep-th]. [132] E. Manrique, S. Rechenberger, and F. Saueressig, Phys. Rev. Lett. 106, 251302 (2011), arXiv:1102.5012 [hep- th]. [133] S. Rechenberger and F. Saueressig, JHEP 03, 010 (2013), arXiv:1212.5114 [hep-th]. [134] J. Biemans, A. Platania, and F. Saueressig, Phys. Rev. D 95, 086013 (2017), arXiv:1609.04813 [hep-th]. [135] J. Biemans, A. Platania, and F. Saueressig, JHEP 05, 093 (2017), arXiv:1702.06539 [hep-th]. [136] W. B. Houthoff, A. Kurov, and F. Saueressig, Eur. Phys. J. C 77, 491 (2017), arXiv:1705.01848 [hep-th]. [137] B. Knorr, Phys. Lett. B 792, 142 (2019), arXiv:1810.07971 [hep-th]. [138] A. Eichhorn, A. Platania, and M. Schiffer, Phys. Rev. D 102, 026007 (2020), arXiv:1911.10066 [hep-th]. [139] R. Banerjee and M. Niedermaier, Nucl. Phys. B 980, 115814 (2022), arXiv:2201.02575 [hep-th]. [140] E. D’Angelo, N. Drago, N. Pinamonti, and K. Re- jzner, Annales Henri Poincare 25, 2295 (2024), arXiv:2202.07580 [math-ph]. [141] E. D’Angelo and K. Rejzner, (2023), arXiv:2303.01479 [math-ph]. [142] E. D’Angelo, Phys. Rev. D 109, 066012 (2024), arXiv:2310.20603 [hep-th]. [143] M. Reichert, PoS 384, 005 (2020). [144] D. F. Litim and J. M. Pawlowski, JHEP 09, 049 (2002), arXiv:hep-th/0203005. [145] A. Eichhorn and H. Gies, New J. Phys. 13, 125012 (2011), arXiv:1104.5366 [hep-th]. [146] J. Meibohm and J. M. Pawlowski, Eur. Phys. J. C 76, 285 (2016), arXiv:1601.04597 [hep-th]. [147] G. P. de Brito, A. Eichhorn, and M. Schiffer, Phys. Lett. B 815, 136128 (2021), arXiv:2010.00605 [hep-th]. [148] G. P. de Brito, A. Eichhorn, and S. Ray, (2023), arXiv:2311.16066 [hep-th]. [149] A. Eichhorn, S. Lippoldt, and V. Skrinjar, Phys. Rev. D 97, 026002 (2018), arXiv:1710.03005 [hep-th]. [150] G. Narain and R. Percacci, Class. Quant. Grav. 27, 075001 (2010), arXiv:0911.0386 [hep-th]. [151] R. Percacci and G. P. Vacca, Eur. Phys. J. C 75, 188 (2015), arXiv:1501.00888 [hep-th]. [152] A. Eichhorn, Y. Hamada, J. Lumma, and M. Yamada, Phys. Rev. D 97, 086004 (2018), arXiv:1712.00319 [hep- th]. [153] I. Balog, H. Chat´ e, B. Delamotte, M. Marohnic, and N. Wschebor, Phys. Rev. Lett. 123, 240604 (2019), arXiv:1907.01829 [cond-mat.stat-mech]. [154] N. Christiansen, B. Knorr, J. M. Pawlowski, and A. Rodigast, Phys. Rev. D 93, 044036 (2016), arXiv:1403.1232 [hep-th]. [155] N. Christiansen, B. Knorr, J. Meibohm, J. M. Pawlowski, and M. Reichert, Phys. Rev. D 92, 121501 (2015), arXiv:1506.07016 [hep-th].13 [156] J. M. Pawlowski and M. Reichert, Front. in Phys. 8, 551848 (2021), arXiv:2007.10353 [hep-th].",
      "references": [
        "ULTRAVIOLET DIVERGENCES IN QUANTUM THEORIES OF GRAVITATION",
        "Perturbative Approaches to Nonperturbative Quantum Gravity",
        "The Functional Renormalization Group in Quantum Gravity",
        "The Functional f(R) Approximation",
        "Quantum Gravity and Scale Symmetry in Cosmology",
        "Black Holes in Asymptotically Safe Gravity",
        "Probing the quantum vacuum. Perturbative effective action approach in quantum electrodynamics and its application",
        "Asymptotic safety in QFT: from quantum gravity to graphene"
      ],
      "meta_data": {
        "arxiv_id": "2405.08862v1",
        "authors": [
          "Astrid Eichhorn",
          "Andreas Odgaard Pedersen",
          "Marc Schiffer"
        ],
        "published_date": "2024-05-14T18:00:01Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This research investigates the physical viability of asymptotically safe quantum gravity (ASG) by applying positivity bounds to four-photon couplings. The main problem addressed is whether ASG respects fundamental principles like unitarity and causality, which can be threatened by the higher-derivative interactions inherent to the theory. The key contribution is the finding that positivity bounds are satisfied for effective field theories whose ultraviolet completion is the asymptotically safe Reuter fixed point, providing a non-trivial indication of ASG's physical viability. The study also demonstrates that positivity bounds can impose significant constraints on gravitational parameters (G and Λ), ruling out regions in the parameter space that would otherwise appear consistent but lead to causality violations.",
        "methodology": "The study applies positivity bounds, which are constraints on Wilson coefficients in effective field theories, derived from demanding unitarity, locality, microcausality, and Lorentz invariance. Specifically, it focuses on four-photon interactions and their associated Wilson coefficients (c1 and c2), using the bounds 4c1 > -3c2 and |4c1 + 3c2| / |4c1 + c2| > 1. Functional Renormalization Group (FRG) techniques are employed to calculate the scale-dependent Wilson coefficients c1(k) and c2(k) in an asymptotically safe gravity-photon system. Beta functions for these couplings, along with gravitational couplings G and Λ, are solved to determine their fixed-point values and trace their Renormalization Group (RG) flow from the ultraviolet (UV) fixed point down to the infrared (IR). A Wick-rotation is assumed to connect the Euclidean FRG calculations to the Lorentzian positivity bounds, justified by the near-perturbative nature of ASG.",
        "experimental_setup": "The research uses an asymptotically safe gravity-photon theory framework. The gravitational dynamics are approximated by the Einstein-Hilbert action, and the photon sector by a low-energy effective field theory (EFT) Lagrangian incorporating dimension-eight operators with coefficients c1 and c2. Beta functions for c1, c2, G, and Λ, derived from previous works, are utilized to find the Reuter fixed point at G* = 1.12, Λ* = 0.05, c1* = -0.69, c2* = 2.62. Robustness tests involve two levels of truncation: an \"F4-truncation\" (four-photon operators) and an \"F6-truncation\" (including additional six-photon operators d1, d2). The gravitational gauge parameter (βh) is varied, and a preferred value (βh → -∞) is chosen based on the principle of minimum sensitivity for critical exponents. For some analyses, the scale-dependent gravitational couplings G(k) and Λ(k) are approximated by Heaviside functions.",
        "limitations": "The study acknowledges several limitations, primarily stemming from the use of truncations in the functional Renormalization Group (FRG), which introduce systematic uncertainties. A key assumption is the validity of a Wick-rotation to connect Euclidean FRG results to Lorentzian positivity bounds, which is generally not guaranteed in fully non-perturbative quantum gravity. The results exhibit a residual dependence on the unphysical gravitational gauge parameter (βh) due to truncations, although a \"preferred\" gauge choice is identified. Violations of positivity bounds are observed in a specific, disfavored interval of βh values, attributed to insufficient truncation. Furthermore, satisfying the considered lowest-order positivity bounds does not definitively prove the adherence to all underlying conditions (unitarity, microcausality, locality, Lorentz symmetry), as violations could occur in higher-order Wilson coefficients not included in this analysis.",
        "future_research_directions": "Future research can extend this study in several ways: 1) Include higher-order couplings that contribute to the 2-2-photon amplitude at higher momentum levels, leading to additional and more stringent positivity bounds. 2) Calculate the truncated 2-2-photon scattering amplitude directly from the derived Wilson coefficients. 3) Explore positivity bounds for other matter fields, such as scalars, where higher-order interactions are already well-understood. 4) Compare the constraints on the gravitational parameter space obtained from positivity bounds with other phenomenological constraints, such as the viability of the Standard Model of particle physics. 5) Pursue direct calculations of beta functions in a Lorentzian signature or in Euclidean foliations that allow for a more robust analytical continuation, thereby avoiding the assumption of a Wick-rotation. This comprehensive program aims to gather further evidence regarding the physical viability of asymptotically safe gravity.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "From Heuristic to Analytic: Cognitively Motivated Strategies for Coherent Physical Commonsense Reasoning",
      "full_text": "Novel Methods for Load Estimation in Cell Switching in HAPS-Assisted Sustainable 6G Networks Maryam Salamatmoghadasi, Metin Ozturk, Halim Yanikomeroglu Abstract—In the evolving landscape of vertical heterogeneous networks (vHetNets), the practice of cell switching—particularly for small base stations (SBSs)—faces a significant challenge due to the lack of accurate data on the traffic load of sleeping SBSs. This information gap is crucial as it hinders the feasibility and applicability of existing power consumption optimization methods; however, the studies in the literature predominantly assume perfect knowledge about the traffic load of sleeping SBSs. Addressing this critical issue, our study introduces innovative methodologies for estimating the traffic load of sleeping SBSs in a vHetNet including the integration of a high altitude platform (HAPS) as a super macro base station (SMBS) into the terres- trial network. We propose three distinct spatial interpolation- based estimation schemes: clustering-based, distance-based, and random neighboring selection. Employing a real data set for empirical validations, we compare the estimation performance of the developed traffic load estimation schemes and assess the impact of estimation errors. Our findings demonstrate that accurate estimation of sleeping SBSs’ traffic loads is essential for making network power consumption optimization methods both feasible and applicable in vHetNets. Index Terms—HAPS-SMBS, vHetNet, traffic load estimation, cell switching, power consumption, sustainability I. Introduction The rapid escalation of mobile traffic demand, spurred by the proliferation of multimedia content and the internet of things (IoT), necessitates the densification of radio access networks (RANs). With the advent of the sixth generation of cellular networks (6G) networks, which are expected to sup- port an unprecedented number of devices per square kilometer, thequestforenhancedconnectivitybecomesincreasinglyvital. However, this expansion is not without challenges, most no- tably the significant rise in energy consumption within RANs. Base stations (BSs), in particular, are major contributors to this energy usage in cellular networks, as highlighted in [1]. Addressing this challenge necessitates a re-evaluation of current operational practices. Traditionally, the practice of keeping all BSs—especially small BSs (SBSs)—continuously active, irrespective of actual user demand, leads to notable This research has been sponsored in part by the NSERC Create program entitled TrustCAV and in part by The Scientific and Technological Research Council of Türkiye (TUBITAK) under the TUBITAK-2219 program. All the authors are with Non-Terrestrial Networks Lab, Depart- ment of Systems and Computer Engineering, Carleton University, Ot- tawa, ON K1S5B6, Canada. Metin Ozturk is also with Electrical and Electronics Engineering, Ankara Yildirim Beyazit University, Ankara, 06010, Turkiye. emails: maryamsalamatmoghad@cmail.carleton.ca, metin.ozturk@aybu.edu.tr, halim@sce.carleton.ca. energy wastage [2]. This problem is exacerbated by the in- herently fluctuating nature of cellular network traffic, which varies both temporally and spatially. A strategic solution lies in the deactivation of BSs, i.e., cell/BS switching off [1]. This method, which involves switching off SBSs or putting them into sleep mode during periods of low activity, offers a feasible strategy for enhancing energy efficiency and network sustainability. However, the implementation of effective cell switching approaches faces a significant hurdle: the lack of precise traffic load information for sleeping SBSs. This ab- sence of data renders current methods for optimizing power consumption impractical and unworkable, since the majority of the works in the literature assume perfect knowledge on the traffic loads of sleeping BSs [3]–[7]. The key to making cell switching strategies viable is addressing the challenge of estimating the traffic loads of sleeping SBSs. Tackling this issue not only bridges the gap between theory and practice but also unlocks the full potential of cell switching approaches for significantly improving sustainability in vertical heterogeneous networks (vHetNets) [8]. The literature on cell switching extensively investigates policies for more efficient BS deactivation, with the primary goal of reducing overall network energy consumption. For example, the research presented in [3] explored a vHetNet model, which incorporated a high altitude platform station (HAPS) functioning as a super macro base station (SMBS), in addition to a macro BS (MBS) and several SBSs. The focus was on optimizing the sleep mode management of SBSs and leveraging the capabilities of HAPS-SMBS to reduce energy consumption in vHetNets, while ensuring the maintenance of user quality-of-service (QoS). However, a pivotal limitation in this study is the assumption of complete knowledge of sleeping SBSs’ traffic loads—an assumption that necessitates accurate estimation in reality. In a similar vein, the study presented in [4] explored the cell switching conundrum, particularly in the presence of HAPS-SMBS. The authors focused on offloading the traffic loads from deactivated BSs to HAPS- SMBS. A notable strategy employed in this investigation involves the use of a sorting algorithm that prioritizes BSs with relatively lower traffic loads for switching off. The reliance of their model on traffic load data highlights the existing issue of accurate load estimation for these studies. In [5], a tiered sleep mode system for BSs was proposed, where sleep depth varied with the number of devices in sleep mode. The authors implemented a decentralized control, arXiv:2402.04386v1  [cs.NI]  6 Feb 2024allowing each BS to independently manage its sleep strategy, thus keeping the system scalable and computationally efficient. A simplified 𝑄-learning algorithm was used, although the issue of accurate traffic load estimation for sleeping cells remained unaddressed. Another interesting perspective was proposed in [9] and [6], where the authors implemented a genetic algorithm to optimize energy savings by switching off BSs within a heterogeneous network (HetNet) environment. This approach primarily revolved around user association, following which BSs are deactivated in accordance with a deterministic algorithm. The study also considered the power consumption implications of transitioning between different BS states, namely ON, OFF, and sleep, and integrated the control data separated architecture (CDSA) into its framework. Similar to previous studies, the issue of accurately estimating the traffic load of sleeping cells is also a challenge here. Moreover, the authors in [7] proposed an innovative approach to cell switching in ultra dense networks by employing a value function approximation (VFA)-based reinforcement learning (RL) algorithm. The results demonstrated potential energy savings while maintaining the QoS, suggesting a scalable and less complex solution compared to exhaustive search methods. However, the challenge of traffic load estimation for sleeping BSs remains unaddressed, limiting the practical application of these studies and their contribution towards achieving sustainability goals of 6G networks as mentioned in the 6G framework of International Telecommunication Union (ITU) [10]. Addressing this gap, our work stands as a pioneering effort to investigate and evaluate various spatial interpolation meth- ods for estimating the traffic load of sleeping cells. Without properly addressing the novel cell load estimation problem presented in this work, the applicability and feasibility of the existing literature about cell switching approached are always questionable. In this regard, we conduct a comparative analysis of the estimated values against their actual counterparts and evaluate the implications of any discrepancies on the network’s power consumption within a vHetNet, as analyzed in [3]. Our study makes significant contributions in the following areas: • Introducing and investigating the sleeping SBSs’ load es- timation problem for cell switching and traffic offloading in a vHetNet. • Developing three innovative spatial interpolation algorithms—𝑘-means clustering, distance-based, and randomneighboringselection—fortrafficloadestimation. • Utilizing a real data set [11] to enhance the realism and reliability of our approach. • Evaluating the impact of traffic load estimation errors on the network power consumption outcomes studied in [3]. The rest of the paper is organized as follows: Section II presents the system model, including network and power consumption models as well as data processing, while the problem formulation is elaborated in Section III. The proposed cell load estimation techniques are introduced in Section IV, followed by discussing the obtained results in Section V. MBS SBS Active SBS Sleep SBS Fig. 1. A vHetNet model with an MBS, multiple SBSs, and a HAPS-SMBS. Lastly, Section VI concludes the paper. II. System Model In this section, we present an overview of the network model, including its components and characteristics. We also detail the metrics used to measure network power consumption and describe the parameters of the data set employed for empirical validation. A. Network Model Our study examines a vHetNet as illustrated in Fig. 1, which consists of a macro cell (MC), encompassing a MBS and a clusterofSBSswithinitscoveragearea.Additionally,aHAPS- SMBS is integrated into the network, potentially serving multiple MCs. The primary function of SBSs is to deliver data services and address user-specific requirements, while MBS and HAPS-SMBS are tasked with providing consistent network coverage and managing control signals, along with offering data services. A key role of the HAPS-SMBS in this architecture is to coordinate the offloading of traffic from SBSs, particularly during low-traffic periods. This is facili- tated by HAPS’s higher capacity and extensive line of sight, enabling efficient monitoring of SBSs’ traffic loads [12]. Based on this monitoring, strategic decisions are made regarding which SBSs to switch off, enhancing the network’s operational efficiency. These decisions take into account the current traffic capacity of both the MBS and HAPS-SMBS to ensure efficient network operation. B. Network Power Consumption The power consumption of each BS in the network is calculated-based on the energy-aware radio and network tech- nologies (EARTH) power consumption model [13]. For the 𝑖-th BS, the power consumption at any given time, denoted as 𝑃𝑖, is expressed as [14] 𝑃𝑖 = \u001a 𝑃O,𝑖 +𝜂𝑖𝜆𝑖𝑃T,𝑖, 0 <𝜆𝑖 <1, 𝑃S,𝑖, 𝜆 𝑖 =0, (1) where 𝑃O,𝑖 represents the operational circuit power consump- tion, 𝜂𝑖 is the power amplifier efficiency,𝜆𝑖 is the load factor, 𝑃T,𝑖 is the transmit power, and𝑃S,𝑖 is the sleep circuit powerconsumption. The total instantaneous power consumption of the network, denoted as𝑃, is given by 𝑃=𝑃H +𝑃M + 𝑠∑︁ 𝑗=1 𝑃𝑗, (2) where 𝑃H and 𝑃M denote the power consumption of HAPS- SMBS and MBS at any given moment, respectively, which are calculated based on the (0 < 𝜆𝑖 < 1) case in (1) as HAPS-SMBS and MBS are always active in our modeling. Meanwhile, 𝑃𝑗 represents the power consumption of the SBSs, which is determined using both 𝜆𝑖 cases of (1), depending on their operational status. The variable𝑠 signifies the total number of SBSs within the network. C. Data Set and Data Processing To calculate the power consumption using (1), the load factor 𝜆𝑖 for each BS is required. For this purpose, we utilize a real call detail records (CDR) data set provided by Telecom Italia [11]. The data set divides the city of Milan into 10,000 square-shaped grids, each measuring235×235 meters. Within each grid, levels of user activity—including calls, text messages, and internet usage—were recorded at 10-minute intervals over two months (November and December 2013). In processing this data, we first amalgamate these distinct activity types into a singular measure. Then, for each SBS, a grid is randomly selected, and the combined activity levels from that grid are normalized to represent the traffic load for the corresponding cell. III. Problem Formulation This section outlines the power consumption optimization problem in vHetNets and addresses the impact of traffic load estimation errors on the optimization policy. A. vHetNet Power Consumption Optimization Problem Our objective is to minimize the total power consumption of the vHetNet, as defined in (2). We achieve this by iden- tifying the most efficient power-saving state. The state vector Δ= {𝛿1,𝛿2,...,𝛿 s}represents the status (ON or OFF) of each SBS at time𝑡, where𝛿j ∈{0,1}indicates the state of the𝑗-th SBS (0 for OFF, 1 for ON). It is assumed that MBS and HAPS- SMBS are always ON (𝛿M = 𝛿H = 1). For an SBS, when𝛿j changes from 1 to 0 at time𝑡, either MBS or HAPS-SMBS could allocate its traffic. On the other hand, when𝛿j changes from 0 to 1, MBS or HAPS-SMBS offload some traffic to the 𝑗-th SBS. The optimization problem can be formulated as minimize 𝚫 𝑃(Δ) (3a) s.t. 𝜆M≤1, (3b) 𝜆H≤1, (3c) 𝛿𝑗 ∈{0,1}, 𝑗=1,...,𝑠, (3d) where 𝑃(Δ)=(𝑃O,H +𝜂H𝜆H𝑃T,H)+( 𝑃O,M +𝜂M𝜆M𝑃T,M) +( 𝑠∑︁ 𝑗=1 (𝑃O,𝑗 +𝜂𝑗𝜆𝑗𝑃T,𝑗)𝛿𝑗 +𝑃S,𝑗 (1 −𝛿𝑗)). (4) In the constraints (3b) and (3c),𝜆M = [0,1]and 𝜆H = [0,1] are defined as positive real numbers, R+. The constraints ensure that the operational capacities of both the MBS and HAPS-SMBS are never exceeded, thereby upholding the QoS requirements. Importantly, the optimal state vector,Δopt, min- imizes 𝑃(Δ), which is a function of the load factors,𝜆𝑗, of SBSs. Therefore, any errors in estimating these load factors can alter the optimal state vector, impacting the total power consumption of the vHetNet. B. Estimation of Sleeping SBS Traffic Load The optimization problem defined in (3) aims to minimize the vHetNet’s total power consumption while adhering to spe- cific constraints. The decision to choose the best state vector, Δopt, on the traffic loads of the SBSs,𝜆𝑗. A critical question arises: How can the traffic load of a sleeping SBS, which was previously set to sleep mode, be known for determining its status in the next time slot? In other words, obtaining𝜆𝑗 for sleeping SBSs is a problematic aspect, as existing literature on cell switching often assumes perfect knowledge of these traffic loads [3]– [7]. This assumption is impractical and unrealistic, as traffic loads should be estimated, inevitably introducing estimation errors. In the subsequent section, we propose three different methods to estimate the traffic load of sleeping SBSs, thereby addressing this critical gap in the field. IV. Proposed Load Estimation Scheme This paper introduces novel methodologies for estimating the traffic load of sleeping BSs, a critical factor in optimizing network power consumption. In this regard, we propose three distinct spatial interpolation approaches, which will be elab- orated in the following paragraphs, that are used to estimate the traffic loads of the sleeping SBSs. More specifically, the base methodology is to use spatial interpolation to estimate the traffic loads, and the three approaches we develop are utilized to select the cells (i.e., cell selection approaches) that would be involved in this interpolation process. A. Clustering-Based Traffic Load Estimation The first approach involves clustering SBSs based on their traffic loads and estimating the traffic load of a sleeping SBS using the average load of active SBSs within the same cluster. We employ the𝑘-means algorithm, an unsupervised machine learning technique, for clustering. The number of clusters, a crucial hyper-parameter, is determined using the elbow method [15], which assesses different cluster numbers by calculating the sum of squared errors (SSE) between data points and centroids for each potential cluster count. The SSE is given by [15] 𝑆𝑆𝐸 = 𝐾∑︁ 𝑘=1 ∑︁ 𝑥𝑚 ∈𝐶𝑘 (𝑥𝑚−𝑐𝑘) 2 . (5)where 𝐾 represents the optimal number of clusters, 𝑐𝑘 the centroid of each cluster𝐶𝑘, and 𝑥𝑚 each sample in𝐶𝑘. The optimal cluster number is identified at the point where the SSE curve forms an \"elbow\" before flattening. 1) Multi-level Clustering-Based Traffic Load Estimation: The multi-level clustering (MLC) approach involves repeated clustering of SBSs based on their traffic loads to estimate the traffic load of offloaded SBSs. This method employs the elbow method to determine the optimum number of clusters, followed by the application of the 𝑘-means algorithm for clustering. The traffic load of a sleeping SBS is then estimated based on the average load of active SBSs in the same cluster. This clustering process, which is detailed in Algorithm 1, is recursively applied to form multi-level clusters, thereby enhancing the accuracy of the estimations. This iterative approach, as outlined in Algorithm 1, ensures progressively refined clustering with each layer, leading to more precise traffic load estimations for sleeping SBSs. Algorithm 1Multi-Level Clustering (MLC) using𝑘-means Require: Traffic loads of SBSs 𝜆𝑖,𝑗 , maximum number of layers 𝐿 Ensure: Clustered SBSs with estimated traffic loads 1: procedure MLC_k_means(𝜆, 𝐿) 2: Determine the optimal number of clusters𝑘 using the elbow method 3: Initialize layer count𝑙 =1 4: while 𝑙 ≤𝐿 do 5: Perform 𝑘-means clustering on𝜆to form𝑘clusters 6: for each cluster𝐶𝑘 do 7: Calculate the mean traffic load𝜇m 8: for each sleeping SBS in𝐶𝑘 do 9: Estimate the traffic load as𝜇m 10: end for 11: end for 12: Update 𝜆 with estimated ones for sleeping SBSs 13: Increment the layer count𝑙 by 1 14: end while 15: return The final clusters with estimated traffic loads 16: end procedure B. Geographical Distance-Based Traffic Load Estimation This method considers the proximity of neighboring cells to estimate the traffic load of a sleeping SBS. It includes two sub-methods based on the presence of a weighting mechanism: distance-based without and with weighting. 1) Distance-Based without Weighting:In this approach, the traffic load of a sleeping SBS is estimated by averaging the traffic loads of its neighboring cells, arranged incrementally based on proximity. All neighboring cells contribute equally to the estimation, regardless of their distance. The estimated traffic load, ˆ𝜆𝑖, is calculated as ˆ𝜆𝑖 = 1 𝑁 𝑁∑︁ 𝑗=1 𝜆𝑗, (6) where 𝜆𝑗 represents the traffic load of the𝑗-th neighboring cell, and 𝑁 is defined as the number of neighboring cells included in the estimation process. 2) Distance-Based with Weighting:This method refines the previous approach by assigning different weights to neighbor- ing cells based on their distance from the sleeping SBS. The closer a cell is, the more it influences the estimated traffic load. The weighted traffic load,ˆ𝜆𝑖, is calculated as ˆ𝜆𝑖 = 𝑁Í 𝑗=1 𝜆𝑗 ×𝑤𝑖,𝑗 𝑁Í 𝑗=1 𝑤𝑖,𝑗 . (7) The weighting factor,𝑤𝑖,𝑗 , is defined as 𝑤𝑖,𝑗 = 𝑑max 𝑑𝑛 𝑖,𝑗 , 𝑛 ∈{1,2,...,𝑘 } (8) where 𝑑max is the maximum distance between the sleeping SBS and its neighboring cells included in the estimation, and 𝑑𝑖,𝑗 is the distance between the sleeping SBS𝑖 and the neighboring SBS 𝑗. C. Random Cell Selection Traffic Load Estimation This approach utilizes a random selection of surrounding cells for traffic load estimation. It comprises two variations: random selection without weighting and random selection with weighting. 1) Random Selection without Weighting: The traffic load of a sleeping SBS is estimated based on the average traffic load of randomly selected surrounding SBSs. The estimation formula is similar to (6), but the selection of𝑗 is random. 2) Random Selection with Weighting:This variation applies a weighting mechanism to the randomly selected surrounding cells, same as (8), where𝑗 is selected randomly. The weighting factors vary based on the distance to the sleeping SBS, enhancing the accuracy of the traffic load estimation. V. Performance Evaluation In this section, we assess the efficacy of our proposed methods for estimating the traffic load of sleeping SBSs. The evaluation is conducted using the Milan data set, as described earlier in Section II-C. The parameters used for simulations are outlined in Table I. Our data collection process involves averaging the traffic for each SBS at each time slot over a month, followed by applying the proposed algorithms with a randomized selection of SBSs for each iteration. The primary metric for evaluating the performance of our methods is the traffic load estimation error, defined as the ratio of the difference between the actual and estimated traffic load to the actual traffic load. Fig. 2 presents the relationship between traffic load esti- mation error and the number of neighboring SBSs for the distance-based estimation method with weighting. The results demonstrate how the estimation error is affected by varyingTABLE I SIMULATION PARAMETERS Parameters Value Number of SBSs 5000 Number of time slots 144 Time slot duration 10 m Number of days 30 Number of iteration 300 Optimal 𝐾 using elbow method 3 50 100 150 200 5 10 15 20 25 30 35 40 45Estimation Error (%) Fig. 2. Estimation error for distance-based cell selection with weighting. the power of distance,𝑛, in (8), with values of 1, 3, 5, and 10. Two key observations emerge from this analysis: • An increase in the number of neighboring SBSs leads to a decrease in the accuracy of the traffic load estimation. This trend suggests that incorporating too many neigh- boring SBSs diminishes the relevance of each individual cell’s data. • The performance of the distance-based method with weighting improves as𝑛 increases. This improvement is attributable to the method’s emphasis on the proximity of neighboring cells; closer neighbors have a more signifi- cant impact on the estimation than those farther away. For example, the estimation error rises to 45% with𝑛=1 for a large number of neighboring SBSs, but it drops below 15% for𝑛 =5. This trend is even more pronounced for 𝑛=10, where the estimation error appears less dependent on the number of contributing neighbors. Fig.3 comparesthe performanceof allthree proposedmethods in terms of the load estimation error. Given the different nature of the variable in𝑘-means clustering compared to the other methods, Fig. 3 features two𝑥-axes. As the number of layers in 𝑘-means clustering increases, indicating a more homogeneous traffic load within each cluster, the estimation error decreases. The results show that it is possible to achieve near 0% estimation error with seven layers of clustering. The trend for the distance-based method without weighting is similar to the weighted case, but the estimation error increases more rapidly with the number of neighboring cells, as distance is 1 2 3 4 5 6 7 Number ofk-means Layers 0 50 100 150 200 Estimation Error (%) 0 50 100 150 200 250 Number of Neighbouring SBSs 0 50 100 150 200Estimation Error (%) Fig. 3. The estimation error for different methods. Two𝑥- and two𝑦-axes are considered: blue one is for clustering based while black one is for the rest of the approaches. not a factor in the estimation. Conversely, the random selection method with weighting shows a decreasing trend in estimation error. This improvement is attributed to the increased number of contributing SBSs and the emphasis on closer neighbors, enhancing the accuracy of the traffic load estimation. The results for the random selection without weighting are not presented, as the estimation errors were found to be high and fluctuated significantly due to the random nature of neighbor selection and the lack of distance consideration. Fig. 4 illustrates the discrepancies between the actual and estimated state vectors in the context of 𝑘-means cluster- ing with varying numbers of layers. The results in Fig. 4 demonstrate that an increase in the number of SBSs leads to greater variation in the state vector. Further, for a given number of SBSs, augmenting the number of layers in the MLC process results in a reduction of this discrepancy, which can be attributed to the inverse relationship between estimation error and the number of layers in𝑘-means clustering, as observed in Fig. 3. This discrepancy is particularly pronounced when there are fewer SBSs in the vHetNet but diminishes as the number of SBSs increases. In a densely populated vHetNet with 70 SBSs, the difference becomes negligible, leading to a decision change rate of 98%-99%. Fig. 5 depicts the network power consumption as a function of the number of SBSs, 𝑠, comparing the results for actual versus estimated traffic loads of sleeping SBSs. The estimation scenario, as described in Fig. 4, is conducted using the 𝑘- means clustering algorithm with varying numbers of layers, 𝐿. Consistent with expectations, the results demonstrate that network power consumption increases with𝑠, in both actual and estimated scenarios. Another key observation is that, for any given𝑠, an increase in𝐿 results in the estimated network power consumption more closely aligning with the actual consumption. This convergence is attributed to the enhanced accuracy of the traffic load estimation with higher layers in5 0 0 15 0 0 55 51 33 99 99 98 10 30 50 70 0 20 40 60 80 100Decision Change (%) Fig. 4. Decision change for different number of layers in MLC. the MLC process. Interestingly, despite substantial changes in the state vector for larger𝑠, the results indicate no significant difference in network power consumption. This observation can be explained by the efficacy of the offloading algorithm proposed in [3], which effectively manages the network power consumption even with changes in the state vector. It is important to acknowledge that the results presented in this study are specific to the Milan dataset. While different datasets may yield different outcomes, we intuitively believe that the findings offer generic insights and trends applicable to the developed solutions. Such an acknowledgment underscores the relevance of our methodologies across varying network scenarios, despite the inherent dataset-specific limitations. 10 30 50 70 1000 1500 2000 2500 3000 3500 4000 Fig. 5. Network power consumption for different number of layers in MLC. VI. Conclusion This research tackles a significant challenge in vHet- Nets—accurately estimating traffic loads for SBSs in sleep mode. The lack of reliable data on the traffic load of sleeping SBSs poses a critical obstacle, affecting the effectiveness of power consumption optimization strategies in vHetNets. By focusing on the cell load estimation problem, our study highlights its significance and the pressing need for effective solutions. To address this issue, we introduced three innova- tive spatial interpolation methods: clustering-based, distance- based, and random neighboring selection, which aim to fill the data gap and improve power consumption management in vHetNets. Using the Milan data set for validation, our analysis confirms the effectiveness of these methods in ac- curately estimating traffic loads. This accuracy is essential for implementing power optimization techniques, ensuring that energy-efficient strategies are both feasible and practical. References [1] M. Feng, S. Mao, and T. Jiang, “Base station on-off switching in 5G wireless networks: Approaches and challenges,” IEEE Wireless Communications, vol. 24, pp. 46–54, Aug. 2017. [2] F. Ahmed, M. Naeem, W. Ejaz, and M. Haneef, “Energy cooperation with sleep mechanism in renewable energy assisted cellular HetNets,” Wireless Pers. Commun., vol. 116, no. 1, p. 105–124, Aug. 2021. [3] M. Salamatmoghadasi, A. Mehrabian, and H. Yanikomeroglu, “Energy sustainability in dense radio access networks via high altitude platform stations,” IEEE Networking Letters, pp. 1–1, Nov. 2023. [4] T. Song, D. Lopez, M. Meo, N. Piovesan, and D. Renga, “High altitude platform stations: the new network energy efficiency enabler in the 6G Era,” arXiv e-prints, p. arXiv:2307.00969, Jul. 2023. [5] A. E. Amine, J.-P. Chaiban, H. A. H. Hassan, P. Dini, L. Nuaymi, and R. Achkar, “Energy optimization with multi-sleeping control in 5G het- erogeneous networks using reinforcement learning,”IEEE Transactions on Network and Service Management, vol. 19, no. 4, pp. 4310–4322, Mar. 2022. [6] M. W. Kang and Y. W. Chung, “An efficient energy saving scheme for base stations in 5G networks with separated data and control planes using particle swarm optimization,” Energies, vol. 10, no. 9, 2017. [Online]. Available: https://www.mdpi.com/1996-1073/10/9/1417 [7] M. Ozturk, A. I. Abubakar, J. P. B. Nadas, R. N. B. Rais, S. Hussain, and M. A. Imran, “Energy optimization in ultra-dense radio access networks via traffic-aware cell switching,”IEEE Transactions on Green Communications and Networking, vol. 5, no. 2, pp. 832–845, Feb. 2021. [8] M. Alzenad and H. Yanikomeroglu, “Coverage and rate analysis for ver- tical heterogeneous networks (vhetnets),”IEEE Transactions on Wireless Communications, vol. 18, no. 12, pp. 5643–5657, Sep. 2019. [9] H. Fourati, R. Maaloul, L. Fourati, and M. Jmaiel, “An efficient energy- saving scheme using genetic algorithm for 5G heterogeneous networks,” IEEE Systems Journal, vol. 17, no. 1, pp. 589–600, Apr. 2023. [10] “The ITU-R Framework for IMT-2030,” ITU-R Working Party 5D, International Telecommunication Union (ITU), Tech. Rep., Jul. 2023. [Online]. Available: https://www.itu.int/en/ITU-R/study-groups/ rsg5/rwp5d/imt-2030/Pages/default.aspx [11] T. Italia, “Telecommunications - SMS, Call, Internet - MI,” 2015. [Online]. Available: https://doi.org/10.7910/DVN/EGZHFV [12] G. Karabulut Kurt, M. G. Khoshkholgh, S. Alfattani, A. Ibrahim, T. S. J. Darwish, M. S. Alam, H. Yanikomeroglu, and A. Yongacoglu, “A vision and framework for the high altitude platform station (HAPS) networks of the future,”IEEE Communications Surveys & Tutorials, vol. 23, no. 2, pp. 729–779, 1st Quart., 2021. [13] G. Auer, V. Giannini, C. Desset, I. Godor, P. Skillermark, M. Olsson, M. A. Imran, D. Sabella, M. J. Gonzalez, O. Blume, and A. Fehske, “How much energy is needed to run a wireless network?”IEEE Wireless Communications, vol. 18, no. 5, pp. 40–49, Oct. 2011. [14] H. Wu, X. Xu, Y. Sun, and A. Li, “Energy efficient base station on/off with user association under C/U split,” 2017 IEEE Wireless Communications and Networking Conference, pp. 1–6, May 2017. [15] H. Zhao, “Research on improvement and parallelization of k-means clustering algorithm,” 2021 IEEE 3rd International Conference on Frontiers Technology of Information and Computer (ICFTIC), pp. 57– 61, Dec. 2021.",
      "references": [
        "Base station on-off switching in 5G wireless networks: Approaches and challenges",
        "Energy cooperation with sleep mechanism in renewable energy assisted cellular HetNets",
        "Energy sustainability in dense radio access networks via high altitude platform stations",
        "High altitude platform stations: the new network energy efficiency enabler in the 6G Era",
        "Energy optimization with multi-sleeping control in 5G heterogeneous networks using reinforcement learning",
        "An efficient energy saving scheme for base stations in 5G networks with separated data and control planes using particle swarm optimization",
        "Energy optimization in ultra-dense radio access networks via traffic-aware cell switching",
        "Coverage and rate analysis for vertical heterogeneous networks (vhetnets)",
        "An efficient energy-saving scheme using genetic algorithm for 5G heterogeneous networks",
        "The ITU-R Framework for IMT-2030",
        "Telecommunications - SMS, Call, Internet - MI",
        "A vision and framework for the high altitude platform station (HAPS) networks of the future",
        "How much energy is needed to run a wireless network?",
        "Energy efficient base station on/off with user association under C/U split",
        "Research on improvement and parallelization of k-means clustering algorithm"
      ],
      "meta_data": {
        "arxiv_id": "2402.04386v1",
        "authors": [
          "Maryam Salamatmoghadasi",
          "Metin Ozturk",
          "Halim Yanikomeroglu"
        ],
        "published_date": "2024-02-06T20:44:25Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "Addresses the significant challenge in vertical heterogeneous networks (vHetNets) regarding the lack of accurate traffic load data for sleeping small base stations (SBSs), which hinders the feasibility of existing power consumption optimization methods. The study introduces and investigates this sleeping SBSs' load estimation problem for cell switching and traffic offloading. It develops three innovative spatial interpolation algorithms—k-means clustering, distance-based, and random neighboring selection—for traffic load estimation. The research utilizes a real dataset for empirical validation and evaluates the impact of estimation errors on network power consumption outcomes.",
        "methodology": "The study proposes three distinct spatial interpolation approaches to estimate the traffic loads of sleeping SBSs in a vHetNet, which includes a High Altitude Platform (HAPS) acting as a Super Macro Base Station (SMBS). The methods are: 1) Clustering-Based Traffic Load Estimation, using the k-means algorithm and multi-level clustering (MLC) to estimate a sleeping SBS's load by averaging active SBSs in the same cluster. 2) Geographical Distance-Based Traffic Load Estimation, which averages neighboring cell loads, with variations for equal weighting or distance-based weighting. 3) Random Cell Selection Traffic Load Estimation, which randomly selects surrounding cells and can apply distance-based weighting. Network power consumption is calculated using the energy-aware radio and network technologies (EARTH) model.",
        "experimental_setup": "The methods were evaluated using a real call detail records (CDR) dataset from Telecom Italia, which details user activity (calls, text messages, internet usage) in 10,000 square-shaped grids (235x235 meters) across Milan over two months (November and December 2013). Data processing involved amalgamating distinct activity types into a singular measure and normalizing combined activity levels to represent traffic load for randomly selected SBS grids. Simulations were conducted with 5000 SBSs over 144 time slots (10 minutes each) for 30 days, with 300 iterations. The optimal number of clusters for k-means was determined as 3 using the elbow method. Performance was assessed based on traffic load estimation error (ratio of difference between actual and estimated load to actual load) and the discrepancy between actual and estimated state vectors (decision change rate).",
        "limitations": "The primary limitation acknowledged is that the results are specific to the Milan dataset used for validation. While the findings offer generic insights and trends, the authors highlight that different datasets may yield different outcomes. The study also implicitly addresses the limitation of existing literature, which predominantly assumes perfect knowledge about the traffic load of sleeping SBSs, rendering current power consumption optimization methods impractical in real-world scenarios due to the inevitable estimation errors.",
        "future_research_directions": "The paper strongly emphasizes the critical need for accurate traffic load estimation for sleeping SBSs to make power consumption optimization methods feasible and applicable in vHetNets. Future research could focus on improving the accuracy and robustness of the proposed spatial interpolation methods, exploring their applicability in diverse geographical regions and traffic patterns, and integrating these estimation techniques with more advanced and dynamic power management and cell switching algorithms to further enhance network sustainability in 6G networks and beyond.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Position: Principles of Animal Cognition to Improve LLM Evaluations",
      "full_text": "Quadrupole-Octupole Residual Interaction in the Proxy-SU (3) Scheme A. Restrepo ∗ and J. P. Valencia † Instituto de F´ ısica, Universidad de Antioquia, Calle 70 No. 52-21, Medell´ ın, Colombia (Dated: August 26, 2024) The octupole deformation of atomic nuclei is a relevant research area given its implications in the nuclear structure and fundamental physics, however, inclusion of octupole degrees of freedom in the nuclear interaction has been explored little in SU (3) symmetry-based models. In this article we expand the octupole operator O3 µ in the SU (3) second quantization formalism and use it to formulate an octupole-octupole residual interaction O3 ·O 3 which is incorporated in Elliott’s model Hamiltonian. We compute the matrix elements of this extended Hamiltonian and use them to calculate energy levels and analyse the B(E3; 3− 1 → 0+ 1 ) transition strength of the isotope 224Th using the semi-microscopic algebraic quartet model (SAQM) based on the proxy- SU (3) scheme. I. INTRODUCTION The atomic nucleus is a primary component of ordi- nary matter and its structure is one of the main subjects of study in quantum mechanics. Since the nucleus is an extended body, it is a legitimate area of research to investigate its geometrical shape [1–3], the interactions influencing it [4] and its variation across isotopes as well as between excited states [5, 6]. It is known that some nuclei organize in spherical shapes while others lower its energy through deformation. This phenomenon is de- scribed theoretically using the multipole moments where the most relevant for the atomic nucleus are the first four, viz. dipole ( Q1), quadrupole ( Q2), octupole ( Q3) and hexadecapole (Q4) moments. Over the years, several non-zero values of electric quadrupole moments have been reported [7] which in- dicate axially symmetric ellipsoidal deformations of the nucleus. Negative values correspond to oblate defor- mations while positive ones to prolate deformations, with a clear prolate dominance observed across all nu- clei [4, 8]. The largest quadrupole values are generally found in half-filled shells defined by the magic numbers 2, 8, 20, 28, 50, 82, and 126. This breaking of spherical symmetry allows for rotational degrees of freedom to populate excited states observed as characteristic energy level patterns called rotational bands. For certain configurations of proton ( Z) and neutron (N) numbers around ≈ 34, 56, 88, and 134 [9], the nu- cleus lowers its energy further through a static octupole deformation [10, 11]. This combined with a prolate quadrupole moment gives the nucleus a so-called “pear shape”[12, 13]. Such mirror asymmetric shape causes a displacement between the centers of charge and mass, creating an electric dipole moment as well. Although few direct measurements of the Q3 moments have been reported in the literature, certain low energy spectra, en- hanced E3 and E1 transition probabilities serve as indi- ∗ alejandro.restrepo19@udea.edu.co † patricio.valencia@udea.edu.co rect indicators of octupole deformation [9]. In particular, for even-even nuclei, the presence of a negative parity band LΠ = 1−, 3−, 5−, ...near the ground state band LΠ = 0 +, 2+, 4+, ...is a characteristic spec- trum of “pear shape” deformation along with enhanced 3− 1 → 0+ 1 and 1 − 1 → 0+ 1 transitions. In figure 1 are shown the energies of the lowest 1− 1 and 3− 1 excited states for even-even nuclei. It can be observed that the lowest values are found in the actinide region for the isotopes 224Ra, 224Th, 226Th, 222Ra, 226Ra and 228Th in increas- ing order up to 0.4MeV. In figure 2 are shown the eval- uated B(E3; 3− 1 → 0+ 1 ) values where a maximum can be found around that region. Previous calculations [14] with relativistic Hartree–Bogoliubov (RHB) theory predicted stable octupole deformations for isotopes 222−226Ra and 224−228Th suggesting them as the best even-even candi- dates for research on “pear shape” deformation. One of the main interests of studying octupole collec- tivity in nuclei goes further than the nuclear structure itself, having implications in the strong nuclear interac- tion and the symmetries of nature [15, 16]. Experimen- tal efforts are being made to measure Q1, Q3 and E3 transitions that will provide more insight about funda- mental physics [17, 18]. In this article we continue the work on the isotope 224Th in the semi-microscopic alge- braic quartet model (SAQM) [19] based on the proxy- SU (3) scheme [20, 21] started in [22, 23] by including a residual octupole-octupole interaction in Elliott’s Hamil- tonian. An analysis of octupole effective charges and B(E3; 3− 1 → 0+ 1 ) transition are made as well. We be- gin explaining the two-shell model space of the system required for the residual octupole interaction to be de- fined, then show the expansion of the octupole operator in terms of SU (3) irreducible tensors and the calculations for the low-energy spectrum of 224Th along comparison with observed energy levels. II. MODEL SPACE Since the octupole operator r3Y 3 µ (Ω) has negative par- ity, at least two consecutive shells are required for its min- arXiv:2408.08386v2  [nucl-th]  23 Aug 20242 FIG. 1: Lowest 1 − (left) and 3− (right) energy levels in even-even nuclei reported in ENSDF [24]. FIG. 2: Evaluated B(E3) values in even-even nuclei reported in ENSDF [24]. The vertical lines indicate the octupole magic numbers obtained in [25]. imal realization [26]. We will focus on heavy even-even nuclei in the actinide region so that proton ( π) and neu- tron (ν) degrees of freedom must be distinguished. These two statements imply that the full model space has to be composed of two-shells which represent the valence and excited shells states for each proton and neutron spaces (ηπ = 5 , 6 and ην = 6 , 7 respectively) along with their couplings to total irreducible representations (irreps). The spectrum generating algebra (SGA) [27] will be U(2(Ωσ− + Ωσ+)) for each proton and neutron spaces where σ = π, ν, the sub-indexes − and + indicate lower and upper shells respectively and Ω = ( η + 1)(η + 2)/2. Since nucleons are fermions, their SGA irreps must be totally anti-symmetric given by {1Nσ } where Nσ repre- sents the number of particles. This algebra can be sepa- rated into spin and spatial parts as a product n U(Ωσ−)⊗ U(Ωσ+) o ⊗ n SUσ−(2) ⊗ SUσ+(2) o . The corresponding spatial representations will be denoted as [ f]σ− ⊗ [f]σ+ . In the SAQM, the ground state (0 ℏω) will be given by setting the upper shell empty in a configuration like [2I]σ− ⊗ [0]σ+ where I denotes the number of quartets [19, 22]. Excited (1 ℏω) states will be given by occupying one orbital in the excited shell in the proton or neutron space (not simultaneously) denoted as [2I−1, 1]σ−⊗[0]σ+. These irreps guarantee that the quartet symmetry [4 I] for the total proton-neutron space is maintained. The U(2) spin irreps are the conjugates [ ¯f]σ− ⊗ [ ¯f]σ+ from which the spins of each shell Sσ− ⊗ Sσ+ can be obtained along with the total coupling to spin Sσ = 0 by means of3 regular angular momentum coupling methods. The total nuclear spin S is obtained straightforwardly as well. The next step in defining the nuclear state is the reduction of the spatial algebras into trivial algebras. The proxy- SU (3) requires such reduction to include SU (3) which can be done by solving the branching prob- lem U(Ω) ⊃ SU (3) [28]. This decomposition has to be performed independently for each shell involved in the model, i.e, U(Ωσ−) ⊗ U(Ωσ+) ⊃ α−SU (3)σ− ⊗ α+SU (3)σ+ where the α represent multiplicities. The total nucleon irrepsρσ(λ, µ)σ where ρσ represent the mul- tiplicities arises from the irreps in the Kronecker product (λ, µ)σ− ⊗ (λ, µ)σ+. Then, in a similar way the total nucleus irreps ρ(λ, µ) are obtained from the Kronecker product ( λ, µ)π ⊗ (λ, µ)ν. The physical basis decom- position SU (3) ⊃ SO(3) is adopted in the subsequent branching in order to obtain the total orbital angular momentum L. Recall that the step SU (3) ⊃ SO(3) is non-canonical so that an additional label K appears that accounts for the multiplicities of L in a given irrep (λ, µ) [29]. This quantum number is usually called Elliott label which is used to denote rotational bands and its values are K = min(λ, µ), min(λ, µ) − 2, min(λ, µ) − 4, ...,0 or 1 from which the Vergados label ¯χ is obtained by orthog- onalization [30]. The final coupling involves the product of total orbital angular momentum L and total spin S which form the total angular momentum J and its pro- jection M. The algebra chain and the corresponding state can be seen in equations 1 and 2 respectively. For conve- nience reasons, the state will be denoted sometimes as |Ψπ+ν⟩ = |Ω; J, M⟩ where the label Ω encompasses all quantum numbers of higher algebras than those of total angular momentum SUJ(2) and its projection UM (1). U (2(Ωπ− + Ωπ+)) ⊗ U (2(Ων− + Ων+)) ⊃(n\u0010 U (Ωπ−) ⊗ U (Ωπ+) \u0011 ⊃ \u0010 SUπ−(3) ⊗ SUπ+(3) \u0011 ⊃ SUπ(3) o ⊗ n\u0010 SUπ−(2) ⊗ SUπ+(2) \u0011 ⊃ SUπ(2) o) ⊗ (n\u0010 U (Ων−) ⊗ U (Ων+) \u0011 ⊃ \u0010 SUν−(3) ⊗ SUν+(3) \u0011 ⊃ SUν(3) o ⊗ n\u0010 SUν−(2) ⊗ SUν+(2) \u0011 ⊃ SUν(2) o) ⊃ ( SU(3)π+ν ⊃ SOπ+ν(3) ) ⊗ SUπ+ν(2) ⊃ SUJ(2) ⊃ UM(1), (1) |Ψπ+ν ⟩ = \f\f\f\f\f n [1Nπ ], [fπ−] ⊗ [fπ+ ], απ−(λ, µ)π− ⊗ απ+ (λ, µ)π+ ; ρπ (λ, µ)π , Sπ− ⊗ Sπ+ ; Sπ o ⊗ n [1Nν ], [fν−] ⊗ [fν+ ], αν−(λ, µ)ν− ⊗ αν+ (λ, µ)ν+ ; ρν (λ, µ)ν , Sν− ⊗ Sν+ ; Sν o ; ρ(λ, µ), K, L, S; J, M + . (2) III. OCTUPOLE OPERATOR The octupole operator is defined as a sum of proton and neutron contributions O3 µ = O 3 πµ + O 3 νµ , (3) with the single nucleon term given by O 3 σµ = Z(N)X i=1 r3 σ(i)Y 3 µ (ˆrσ(i)), (4) where σ = π, ν. Following the methodology in [23], the mathematical form of the operator in terms of one-body SU (3) tensors is given by equation 5 where the expansion coefficients VLo=3 σ are displayed in table I. These coeffi- cients condense the products of SUJ(2) reduced matrix elements [31] and coupling coefficients. Notice that the sum over η, η′ runs over the two shells where the opera- tor is defined; these are 5,6 and 6,7 for the proton and neutron cases respectively. O Lo=Jo=3 σ M o = X η,η′,(λo,µo),Ko VLo=3 σ (η, η′, λo, µo, Ko) ×\ba† (η′,0)1 2 ˜a(0,η)1 2 \tρo=1,(λo,µo),Ko,Lo=Jo=3 σ M o . (5) The irrep ( λo, µo) arises from the Kronecker product between the single particle irreps ( η′, 0) ⊗ (0, η). A sum over the label Ko must be explicit since the angular mo- mentum Lo = 3 may have several multiplicities in a single (λo, µo) as indicated in table I. Notice that the coefficients Vσ of the irreps ( λo, µo) and (µo, λo) are equal.4 TABLE I: Expansion coefficients VLo=3 σ (η, η′, λo, µo, Ko) of O 3 σµ for actinides. (λo, µo) (6,7),(7,6) (5,6),(6,5) (4,5),(5,4) (3,4),(4,3) (2,3),(3,2) (1,2),(2,1) Ko 0 2 1 3 0 2 1 3 0 2 1 V3 π 9.7515 -9.9778 -5.1484 5.8693 -7.2727 13.1012 16.0658 -7.1607 0.2676 V3 ν 2.2092 -16.0225 1.6038 -8.1959 -6.6155 15.1021 -2.6772 15.8482 22.1137 -14.1431 -5.3526 IV. HAMILTONIAN AND MATRIX ELEMENTS Elliott’s original model included a mean field and a residual quadrupole-quadrupole interaction in the nu- clear Hamiltonian which accounted for collective excita- tions of nuclei. We include an octupole-octupole two- body residual term in order to consider such degree of freedom as well. This extended Hamiltonian is given by H = ℏωˆn + χQ2 · Q2 + τO3 · O3 − Hg.s., (6) where the first term is the harmonic oscillator mean field with ˆn the U(3) Casimir operator of order 1 that accounts for the number of particles in the excited shell and the strength ℏω = 45A−1/3−25A−2/3 MeV. The second term represents the quadrupole residual interaction which in SU (3) symmetry-based models is given by the relation Q2 ·Q2 = 4C2(λ, µ)−3L2 with the SU (3) Casimir opera- tor of order two given byC2(λ, µ) = λ2+µ2+λµ+3(λ+µ) and the total orbital angular momentum denoted by L. The third term is the octupole residual interaction which is constructed as a scalar product of the expansion in equation 5. The last term Hg.s. is the energy eigenvalue of the ground state which sets it to 0 MeV. The param- eters χ and τ are obtained from fitting procedure to ex- perimental energy levels. The reduced matrix elements (RME) of the term O3 · O3 are given by equation 7 where an identityP Ψ′′ π+ν |Ψ′′ π+ν⟩⟨Ψ′′ π+ν| had to be introduced and the ten- sor expansion of equation 5 was replaced explicitly. The RME on the right hand side of equation 7 can be com- puted with equation 10 of reference [32], which sepa- rates the proton and neutron spaces in products of re- coupling coefficients and triple-barred RME. The later factors however, require an additional mathematical pro- cedure for their calculation as explained in appendix A. Computer codes were developed for the calculation of these octupole interaction double-barred RME. ⟨Ω;J||O3 · O3||Ω′;J′⟩ = X σ,σ′ (λo,µo),Ko,η,η′ (λ′ o,µ′ o),K′ o,η′′,η′′′ Ω′′;J′′ (−1)J+J′′ δJJ′ r2J + 1 2J′′ + 1Vσ(λo, µo, Ko, η, η′) Vσ′(λ′ o, µ′ o, K′ o, η′′, η′′′) × D Ω;J \f\f\f \f\f\f\ba† (η′,0)1 2 ˜a(0,η)1 2 \t(λo,µo),Ko,Lo=Jo=3 σ \f\f\f \f\f\fΩ′′;J′′ ED Ω′′;J′′ \f\f\f \f\f\f\ba† (η′′′,0)1 2 ˜a(0,η′′)1 2 \t(λ′ o,µ′ o),K′ o,Lo=Jo=3 σ′ \f\f\f \f\f\fΩ′;J′ E . (7) V. 224Th SPECTRUM As stated above, the isotope 224Th is one of the best candidates for research on “pear shape” deformation. Its low energy spectrum has been studied previously in [22] where a Hamiltonian involving the SU (3) Casimir oper- ators of orders two and three was formulated. In this section we take the Hamiltonian in 6 and use equation 7 to calculate its matrix elements which are degenerate with respect to the total angular momentum projection M. The harmonic oscillator strength has the value of ℏω = 6.7318 MeV. This isotope has 16 particles in the valence shell; 8 pro- tons and 8 neutrons which allow for the formation of 4 quartets. Thus, its ground state is given by the spatial representation [24]π− ⊗[0]π+ ⊗[24]ν− ⊗[0]ν+ . The reduc- tion U(N) ⊃ SU (3) is given by the leading [33] irreps (26, 4)π− ⊗(0, 0)π+ ⊗(34, 4)ν− ⊗(0, 0)ν+ which couple to a leading (60, 8). The band KΠ = 0+ contained in this ir- rep corresponds to the ground state band observed exper- imentally. As explained above, the excited state has two contributions; the proton excitation given by [2 3, 1]π− ⊗ [1]π+ ⊗[24]ν− ⊗[0]ν+ and the neutron excitation given by [24]π− ⊗ [0]π+ ⊗ [23, 1]ν− ⊗ [1]ν+ . The SU (3) reductions are the leading (25 , 2)π− ⊗ (6, 0)π+ ⊗ (34, 4)ν− ⊗ (0, 0)ν+ and (26, 4)π− ⊗(0, 0)π+ ⊗(32, 2)ν− ⊗(7, 0)ν+ respectively, both of which couple to total leading (65 , 6). The band KΠ = 0− contained in this irrep corresponds to the oc- tupole excited band observed experimentally. The Hamiltonian matrix elements to be calculated are those between the states observed experimentally which can be used to fit the parameters χ and τ. The ground state band has angular momentum JΠ = 0+, 2+, ...,18+ and those of the excited band are JΠ = 1−, 3−, ...,17−. They are a total of 28 states distinguishing between the5 proton and neutron excited states. The intermediate states |Ψ′′ π+ν⟩ of equation 7 are too many to consider them all in a realistic calculation, thus we propose a trun- cation of the full model space to the ground and excited bands (60 , 8)0+ and (65 , 6)0−. These are a total of 97 intermediate states, even though not all of them will con- tribute because many will cancel the one-body RME due to angular momentum selection rules. Adopting these re- strictions, there will be 37 non-zero independent matrix elements of the Hamiltonian, 9 of which are non-diagonal. In this restricted space the products O3 π ·O 3 π and O3 ν ·O 3 ν will contribute to the diagonal terms while the O3 π · O3 ν and O3 ν · O3 π will form the non-diagonal matrix elements. These matrix elements are shown in appendix B. The Elliott Hamiltonian fit to 224Th spectrum re- sults in a quadrupole-quadrupole interaction strength of χ = −0.0032 and a mean square error (MSE) of 0.1771 MeV 2 to the experimental energy levels. The two-parameter Hamiltonian proposed in [22] obtains an improvement with a MSE of 0.1180 MeV 2. The states of the negative parity band have two contributions from the model, taken into account as a superposition |ΨJ⟩ =√α|Ψ(π) J ⟩+ √1 − α|Ψ(ν) J ⟩, where the proton and neutron excited states are denoted as |Ψ(π) J ⟩, |Ψ(ν) J ⟩ respectively and the parameter α expresses the contribution of each nucleon type wavefunction to the total nuclear wavefunc- tion. While the matrix elements of the ground state band are diagonal, the energy of the negative parity band states are be given by ⟨ΨJ|H|ΨJ⟩ = ℏω + χ   4C2(65, 6) − 3L(L + 1) ! + τ   α⟨Ψ(π) J |O3 · O3|Ψ(π) J ⟩ + 2 p α(1 − α)⟨Ψ(π) J |O3 · O3|Ψ(ν) J ⟩ + (1 − α)⟨Ψ(ν) J |O3 · O3|Ψ(ν) J ⟩ ! − Hg.s. (8) Computer codes were developed to calculate these Hamiltonian expectation values and fit its three pa- rameters resulting in χ = −0.0032, τ = −0.0016 and α = 0.2398. It is equivalent to a simultaneous diagonal- ization and fit of the parameters. This model has a MSE of 0.1739 MeV2 and its spectrum is shown in figure 3. VI. B(E3; 3− 1 → 0+ 1 ) STRENGTH Using the scheme and equations shown above, we can study the 3 − 1 → 0+ 1 electromagnetic transition strength. The electric octupole transition operator is [34] T3 µ(E) = b3 0 \u0010 eπO3 µπ + eνO3 µπ \u0011 , (9) where the harmonic oscillator amplitude parameter is b0 = A1/6 fm, the proton and neutron effective charges 224 90 Th134 4+ 4+ 8+ 8+ 10+ 10+ 12+ 12+ 14+ 14+ (3)- 3-(1)- (5)- 5- (7)- 7- (9)- 9- (11)- 11- (13)- 13- (15)- 15- (17)- 17- 1- 18+ 16+ 0(60,8)0+ 1(65,6)0- 18+ 16+ 2+ 2+ 6+ 6+ Eproxy 0+ Eexp. 0+ E(Mev) 0 1 2 3 FIG. 3: Experimental and theoretical low energy spectra of 224Th. The energy is measured in MeV. The labels of each band are n(λ, µ)KΠ. are eπ and eν respectively. The reduced transition prob- ability is be given by B(E3; 3− 1 → 0+ 1 ) = 1 7|⟨ΨJ=0|| T3(E) ||Ψ′ J=3⟩|2, (10) with the reduced matrix element ⟨ΨJ=0|| T3(E) ||Ψ′ J=3⟩ =b3 0 \u0010 a⟨ΨJ=0|| O3 π ||Ψ′(π) J=3⟩ +b⟨ΨJ=0|| O3 ν ||Ψ′(ν) J=3⟩ \u0011 , (11) where |Ψ(σ) J=3⟩ represents the excited state of nucleon σ according to the scheme of equation 2, parameters a, b involve the effective charges and superposition parame- ter α in the total excited state |ΨJ=3⟩ as a = eπ √α and b = eν √1 − α [23]. Recall that the latest evaluated ex- perimental values of the transition 3 − 1 → 0+ 1 are shown in figure 2 From the collective model the following relation can be demonstrated [9, 35] B(E3; 3− 1 → 0+ 1 ) = 1 7   3ZeR3β3 4π !2 , (12) using the experimental value for quadrupole deformation β2 = 0.174(6) [24], the calculation for octupole deforma- tion β3 = −0.153 [36] for 224Th and the approximation for the nuclear radius R ≈ 1.2A1/3 fm, results in a tran- sition strength of 77.6001 W.u. This value is close to the one reported in [37, 38] by different methods.6 Assuming b ≈ −a/2 according to the results in [23] and considering the collective transition strength, one can es- timate the value of a ≈ 0.7805e. Under these assump- tions and the value forα obtained in the previous section, minimums for eπ and eν are set in the model. A direct measurement of the B(E3; 3− 1 → 0+ 1 ) transition for 224Th is required for further analysis. VII. CONCLUSIONS We obtained the mathematical form of the octupole operator r3Y 3 µ (Ω) in terms of SU (3) tensors for the ac- tinide region which is minimally defined in the shells 5,6 in the proton space and 6,7 in the neutron space. The scalar operator O3 · O3 is constructed from this expan- sion and its reduced matrix elements in the two-shell states formalism are computed in terms of products of recoupling coefficients and single-shell reduced matrix el- ements. Computer codes were developed that integrate several SU (3) libraries [28, 39–41] in order to perform the calculations of this work. These are available under request to the authors. Essential algebraic expressions for this work are presented in appendix A along with identities used to check the computer codes developed. The Hamiltonian proposed carries three parame- ters χ, τ and α that account for the strength of the quadrupole, octupole residual interactions and nucleon contributions respectively. This model was applied to the low-lying spectrum of the isotope224Th obtaining a slight improvement with respect to the original Elliott model. Increasing the number of parameters of the model would improve the fit but decrease its explanatory capabilities. A possible improvement might be achieved by a weaker truncation of the intermediate states in equation 7, for the influence of the octupole-octupole interaction is too little in this reduced space. An analysis on the transition B(E3; 3− 1 → 0+ 1 ) involving previous works on collective models, density functional theory and the scheme in this article allows to estimate the octupole effective charges of the model under certain assumptions, however, the lack of experimental data is challenging for this theoretical as- pect. We hope to motivate research on this isotope from the experimental side as well. The octupole deformation of nuclei is a research area that will help physicists obtain a better understanding of the nuclear structure and gain insight of the fundamen- tal symmetries of nature. With this work, we expect to increase interest in this topic from the SU (3)-symmetry based models and await the results of experimental efforts from novel radioactive beams accelerators which will al- low the exploration of previously inaccessible nuclei. ACKNOWLEDGMENTS We would like to thank professors J. G. Hirsch, V. K. B. Kota, D. Langr and J. Cseh for very valuable discus- sions. We thank the Institute of Physics at University of Antioquia for supporting this research. Appendix A: SU (3) one-body triple-barred reduced matrix elements in two-shell states The one-body double-barred reduced matrix elements (RME) of equation 7 can be computed with equation 10 of reference [32], which separates the proton an neutron spaces obtaining a mathematical expression in terms of recoupling coefficients and triple-barred RME. The for- mer can be computed by SU (3) libraries [40] while the latter require an additional mathematical calculation. The proton and neutron spaces are formed each by two shells; a lower shell denoted by sub-index − and an up- per one denoted by sub-index +, with the creation an de- struction operators acting on different quantum numbers. A separation of these two shells [42] can be performed as shown in equation A4 where two situations can happen; a† creates a particle in the − shell and ˜a destroys a par- ticle in + shell, or a† creates a particle in the + shell and ˜a destroys a particle in shell −. The first case is a de- excitation of the nucleus while the second is an excitation process. These two cases will imply a minimal change in their mathematical expressions which are distinguished with the labels s and t as follows: s = + and t = − identify the de-excitation process and s = − and t = + identifies the excitation process as indicated in A4. The triple-barred RME appearing on the right hand side of equation A4 are in a single shell which can be evaluated using computer packages [41, 43]. A computer program was developed to perform the calculation of equation A4 depending on the case as explained above. The following relation is useful  \ba† (η′0)1 2 ˜a(0η)1 2 \tρ0=1,(λ0µ0),Ko,Lo,S0 MLo,MSo !† = (−1)η−η′−Lo−MLo−MSo \ba† (η0)1 2 ˜a(0η′)1 2 \tρ0=1,(λ0µ0),Ko,Lo,S0 −MLo,−MSo , (A1) and the next identities [30, 44] must be hold which were used to check the computer codes D L′, S′ \f\f\f \f\f\fTLo,So \f\f\f \f\f\fL, S E = (−1)L−L′+S−S′ s (2L+ 1)(2S+ 1) (2L′ + 1)(2S′ + 1) D L, S \f\f\f \f\f\fTLo,So \f\f\f \f\f\fL′, S′ E∗ , (A2) ⟨(λ1, µ1)|||T(µ2,λ2)|||(λ3, µ3)⟩ ⟨(λ3, µ3)|||T(λ2,µ2)|||(λ1, µ1)⟩ = (−1) 1 2 (µ1+µ2−µ3−λ1−λ2+λ3) s dim(λ3, µ3) dim(λ1, µ1). (A3)7 D [f′ −] ⊗[f′ +],(λ′ −µ′ −) ⊗(λ′ +µ′ +), ρ′(λ′µ′), S′ \f\f\f \f\f\f \f\f\f \ba† (η′0)1 2 ˜a(0η)1 2 \tρ0=1,(λ0µ0)S0 \f\f\f \f\f\f \f\f\f[f−] ⊗[f+],(λ−µ−) ⊗(λ+µ+), ρ(λµ), S E ¯ρ =    (λ−, µ−) ( α, β) ( λ′ −, µ′ −) ρ = 1 (λ+, µ+) ( γ, δ) ( λ′ +, µ′ +) ρ = 1 (λ, µ) ( λo, µo) ( λ′, µ′) ¯ ρ ρ ρ = 1 ρ′    χ    S− 1 2 S′ − S+ 1 2 S′ + S S o S′   (−1)ϕ s dim(λsµs) dim(λ′sµ′s) √2Ss + 1p2S′s + 1 × D [f′ t],(λ′ tµ′ t), S′ t \f\f\f \f\f\f \f\f\fa† (η′0)1 2 \f\f\f \f\f\f \f\f\f[ft],(λtµt)St E ρ=1 D [fs],(λsµs), Ss \f\f\f \f\f\f \f\f\fa† (η0)1 2 \f\f\f \f\f\f \f\f\f[f′ s],(λ′ sµ′ s), S′ s E ρ=1 , (A4)    ϕ = η + 1 2 + S+ − S′ + + λ′ + − λ+ + µ′ + − µ+, (α, β) = (η′, 0) and (γ, δ) = (0, η) if s = + and t = − ϕ = η′ + 3 2 + S− − S′ − − So + λ′ − − λ− + µ′ − − µ− − λo − µo, (α, β) = (0, η) and (γ, δ) = (η′, 0) if s = − and t = + . Appendix B: O · Omatrix elements In this section we present the matrix elements of the operator O · Oin table II as defined in equation 7. [1] K. Heyde and J. L. Wood, Nuclear shapes: from earli- est ideas to multiple shape coexisting structures, Physica Scripta 91, 083008 (2016). [2] R. S. Mackintosh, The shape of nuclei, Reports on Progress in Physics 40, 731 (1977). [3] S. Nilsson and I. Ragnarsson, Shapes and Shells in Nu- clear Structure (Cambridge University Press, 1995). [4] N. Tajima and N. Suzuki, Prolate dominance of nuclear shape caused by a strong interference between the effects of spin-orbit and l2 terms of the nilsson potential, Phys. Rev. C 64, 037301 (2001). [5] K. Heyde and J. L. Wood, Shape coexistence in atomic nuclei, Rev. Mod. Phys. 83, 1467 (2011). [6] A. Martinou, D. Bonatsos, T. J. Mertzimekis, K. E. Karakatsanis, I. E. Assimakis, S. K. Peroulis, S. Saran- topoulou, and N. Minkov, The islands of shape coexis- tence within the elliott and the proxy-SU(3) models, The European Physical Journal A 57, 84 (2021). [7] N. Stone, Table of nuclear electric quadrupole moments, Atomic Data and Nuclear Data Tables 111-112, 1 (2016). [8] D. Bonatsos, I. E. Assimakis, N. Minkov, A. Marti- nou, S. Sarantopoulou, R. B. Cakirli, R. F. Casten, and K. Blaum, Analytic predictions for nuclear shapes, prolate dominance, and the prolate-oblate shape transi- tion in the proxy-su(3) model, Phys. Rev. C 95, 064326 (2017). [9] P. A. Butler, Octupole collectivity in nuclei, Journal of Physics G: Nuclear and Particle Physics 43, 073002 (2016). [10] I. Ahmad and P. A. Butler, Octupole shapes in nuclei, Annual Review of Nuclear and Particle Science 43, 71 (1993). [11] P. A. Butler and W. Nazarewicz, Intrinsic reflection asymmetry in atomic nuclei, Rev. Mod. Phys. 68, 349 (1996). [12] P. A. Butler, Pear-shaped atomic nuclei, Proceedings of the Royal Society A: Mathematical, Physical and Engi- neering Sciences 476, 20200202 (2020). [13] S. Pancholi, Pear-shaped Nuclei (World Scientific Pub- lishing Company, 2020). [14] K. Nomura, D. Vretenar, T. Nikˇ si´ c, and B.-N. Lu, Mi- croscopic description of octupole shape-phase transitions in light actinide and rare-earth nuclei, Phys. Rev. C 89, 024312 (2014). [15] M. Pospelov and A. Ritz, Electric dipole moments as probes of new physics, Annals of Physics318, 119 (2005), special Issue. [16] W. C. Griffith, M. D. Swallows, T. H. Loftus, M. V. Romalis, B. R. Heckel, and E. N. Fortson, Improved limit on the permanent electric dipole moment of 199Hg, Phys. Rev. Lett. 102, 101601 (2009). [17] L. P. Gaffney, P. A. Butler, M. Scheck, A. B. Hayes, F. Wenander, M. Albers, B. Bastin, C. Bauer, A. Blazhev, S. B¨ onig, N. Bree, J. Cederk¨ all, T. Chupp, D. Cline, T. E. Cocolios, T. Davinson, H. De Witte, J. Diriken, T. Grahn, A. Herzan, M. Huyse, D. G. Jenk- ins, D. T. Joss, N. Kesteloot, J. Konki, M. Kowalczyk, T. Kr¨ oll, E. Kwan, R. Lutter, K. Moschner, P. Napi- orkowski, J. Pakarinen, M. Pfeiffer, D. Radeck, P. Reiter, K. Reynders, S. V. Rigby, L. M. Robledo, M. Rudigier, S. Sambi, M. Seidlitz, B. Siebeck, T. Stora, P. Thoele, P. Van Duppen, M. J. Vermeulen, M. von Schmid, D. Voulot, N. Warr, K. Wimmer, K. Wrzosek-Lipska, C. Y. Wu, and M. Zielinska, Studies of pear-shaped nu- clei using accelerated radioactive beams, Nature497, 199 (2013).8 TABLE II: Reduced matrix elements of O · O. The notation of the states JΠ σ indicates the angular momentum, parity and the excited nucleon σ. In the ground state band, this last label is omitted. 0+ 2+ 4+ 6+ 8+ 10+ 12+ 14+ 16+ 18+ 1−π 3−π 5−π 7−π 9−π 11−π 13−π 15−π 17−π 1−ν 3−ν 5−ν 7−ν 9−ν 11−ν 13−ν 15−ν 17−ν 0+ 4.5946 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2+ 0 25.8974 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4+ 0 0 36.6437 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6+ 0 0 0 32.0281 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8+ 0 0 0 0 30.4360 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 10+ 0 0 0 0 0 29.4780 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 12+ 0 0 0 0 0 0 28.8096 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 14+ 0 0 0 0 0 0 0 28.3865 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 16+ 0 0 0 0 0 0 0 0 28.2491 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 18+ 0 0 0 0 0 0 0 0 0 28.4693 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1−π 0 0 0 0 0 0 0 0 0 0 0.0583 0 0 0 0 0 0 0 0 0.8057 0 0 0 0 0 0 0 0 3−π 0 0 0 0 0 0 0 0 0 0 0 0.8027 0 0 0 0 0 0 0 0 5.8653 0 0 0 0 0 0 0 5−π 0 0 0 0 0 0 0 0 0 0 0 0 0.7432 0 0 0 0 0 0 0 0 3.9895 0 0 0 0 0 0 7−π 0 0 0 0 0 0 0 0 0 0 0 0 0 1.0362 0 0 0 0 0 0 0 0 4.1776 0 0 0 0 0 9−π 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.4556 0 0 0 0 0 0 0 0 4.6520 0 0 0 0 11−π 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1.9908 0 0 0 0 0 0 0 0 5.2846 0 0 0 13−π 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2.6439 0 0 0 0 0 0 0 0 6.0400 0 0 15−π 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3.4203 0 0 0 0 0 0 0 0 6.9057 0 17−π 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.3272 0 0 0 0 0 0 0 0 7.8818 1−ν 0 0 0 0 0 0 0 0 0 0 0.8057 0 0 0 0 0 0 0 0 12.7929 0 0 0 0 0 0 0 0 3−ν 0 0 0 0 0 0 0 0 0 0 0 5.8653 0 0 0 0 0 0 0 0 50.6621 0 0 0 0 0 0 0 5−ν 0 0 0 0 0 0 0 0 0 0 0 0 3.9895 0 0 0 0 0 0 0 0 32.9312 0 0 0 0 0 0 7−ν 0 0 0 0 0 0 0 0 0 0 0 0 0 4.1776 0 0 0 0 0 0 0 0 29.9974 0 0 0 0 0 9−ν 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4.6520 0 0 0 0 0 0 0 0 28.2286 0 0 0 0 11−ν 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5.2846 0 0 0 0 0 0 0 0 26.7378 0 0 0 13−ν 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.0400 0 0 0 0 0 0 0 0 25.3676 0 0 15−ν 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 6.9057 0 0 0 0 0 0 0 0 24.1192 0 17−ν 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7.8818 0 0 0 0 0 0 0 0 23.04709 [18] T. F. Thorsteinsen, K. Nybø, and G. Løvhøiden, Levels in 226ra populated by inelastic deuteron scattering, Physica Scripta 42, 141 (1990). [19] J. Cseh, Algebraic models for shell-like quarteting of nu- cleons, Physics Letters B 743, 213 (2015). [20] D. Bonatsos, I. E. Assimakis, N. Minkov, A. Martinou, R. B. Cakirli, R. F. Casten, and K. Blaum, Proxy-su(3) symmetry in heavy deformed nuclei, Phys. Rev. C 95, 064325 (2017). [21] D. Bonatsos, A. Martinou, S. K. Peroulis, T. J. Mertz- imekis, and N. Minkov, The proxy-su(3) symmetry in atomic nuclei, Symmetry 15, 10.3390/sym15010169 (2023). [22] J. Cseh, Shell-like quarteting in heavy nuclei: Alge- braic approaches based on the pseudo- and proxy-su(3) schemes, Phys. Rev. C 101, 054306 (2020). [23] A. Restrepo and J. P. Valencia, Inter-band b(e1) strengths in heavy nuclei based on the proxy-su(3) scheme (2024), arXiv:2405.04679 [nucl-th]. [24] B. Singh and S. Singh, From ENSDF database as of March 08, 2022. Version available at http://www.nndc. bnl.gov/ensarchivals/. [25] A. Martinou and N. Minkov, Microscopic derivation of the octupole magic numbers from symmetry considera- tions, Physica Scripta 99, 075311 (2024). [26] P. V. Isacker and S. Pittel, Symmetries and deformations in the spherical shell model, Physica Scripta 91, 023009 (2016). [27] A. Frank, J. Jolie, and P. van Isacker, Symmetries in Atomic Nuclei: From Isospin to Supersymmetry , Springer Tracts in Modern Physics (Springer New York, 2008). [28] J. Draayer, Y. Leschber, S. Park, and R. Lopez, Repre- sentations of u(3) in u(n), Computer Physics Communi- cations 56, 279 (1989). [29] F. Sloane, Lie Algebras and Applications , Lecture Notes in Physics (Springer Berlin Heidelberg, 2014). [30] J. Vergados, Su(3) r(3) wigner coefficients in the 2s-1d shell, Nuclear Physics A 111, 681 (1968). [31] M. Moshinsky and Y. Smirnov, The Harmonic Oscillator in Modern Physics , Contemporary concepts in physics (Harwood Academic Publishers, 1996). [32] D. Troltenier, C. Bahri, and J. Draayer, Generalized pseudo-su(3) model and pairing, Nuclear Physics A 586, 53 (1995). [33] V. Kota, SU(3) Symmetry in Atomic Nuclei (Springer Nature Singapore, 2020). [34] P. Brussaard and P. Glaudemans, Shell-model Applica- tions in Nuclear Spectroscopy (North-Holland Publishing Company, 1977). [35] G. Leander and R. Sheline, Intrinsic reflection asymme- try in odd-a nuclei, Nuclear Physics A 413, 375 (1984). [36] P. M¨ oller, A. Sierk, T. Ichikawa, and H. Sagawa, Nuclear ground-state masses and deformations: Frdm(2012), Atomic Data and Nuclear Data Tables 109-110, 1 (2016). [37] L. M. Robledo and P. A. Butler, Quadrupole-octupole coupling in the light actinides, Phys. Rev. C 88, 051302 (2013). [38] L. P. Gaffney, Octupole collectivity in 220Rn and 224Ra, Phd thesis, University of Liverpool (2012), avail- able at https://cds.cern.ch/record/1547569/files/ CERN-THESIS-2012-309.pdf . [39] B. G. Wybourne, Schur Group Theory Software (2022). [40] T. Dytrych, D. Langr, J. P. Draayer, K. D. Launey, and D. Gazda, Su3lib: A c++ library for accurate computa- tion of wigner and racah coefficients of su(3), Computer Physics Communications 269, 108137 (2021). [41] C. Bahri and J. Draayer, Su(3) reduced matrix ele- ment package, Computer Physics Communications 83, 59 (1994). [42] D. J. Millener, Su(3) in shell-model calculations (1991). [43] G. J. Hirsch, C. Bahri, J. Draayer, O. Casta˜ nos, and P. Hess, Reduced matrix elements for the leading spin zero states in the su ( 3 ) scheme * (2007). [44] J. Suhonen, From Nucleons to Nucleus: Concepts of Mi- croscopic Nuclear Theory, Theoretical and Mathematical Physics (Springer Berlin Heidelberg, 2007).",
      "references": [
        "Nuclear shapes: from earliest ideas to multiple shape coexisting structures",
        "The shape of nuclei",
        "Shapes and Shells in Nuclear Structure",
        "Prolate dominance of nuclear shape caused by a strong interference between the effects of spin-orbit and l2 terms of the nilsson potential",
        "Shape coexistence in atomic nuclei",
        "The islands of shape coexistence within the elliott and the proxy-SU(3) models",
        "Table of nuclear electric quadrupole moments",
        "Analytic predictions for nuclear shapes, prolate dominance, and the prolate-oblate shape transition in the proxy-su(3) model",
        "Octupole collectivity in nuclei",
        "Octupole shapes in nuclei",
        "Intrinsic reflection asymmetry in atomic nuclei",
        "Pear-shaped atomic nuclei",
        "Pear-shaped Nuclei",
        "Microscopic description of octupole shape-phase transitions in light actinide and rare-earth nuclei",
        "Electric dipole moments as probes of new physics",
        "Improved limit on the permanent electric dipole moment of 199Hg",
        "Studies of pear-shaped nuclei using accelerated radioactive beams",
        "Levels in 226ra populated by inelastic deuteron scattering",
        "Algebraic models for shell-like quarteting of nucleons",
        "Proxy-su(3) symmetry in heavy deformed nuclei",
        "The proxy-su(3) symmetry in atomic nuclei",
        "Shell-like quarteting in heavy nuclei: Algebraic approaches based on the pseudo- and proxy-su(3) schemes",
        "Inter-band b(e1) strengths in heavy nuclei based on the proxy-su(3) scheme",
        "From ENSDF database as of March 08, 2022. Version available at http://www.nndc. bnl.gov/ensarchivals/.",
        "Microscopic derivation of the octupole magic numbers from symmetry considera- tions",
        "Symmetries and deformations in the spherical shell model",
        "Symmetries in Atomic Nuclei: From Isospin to Supersymmetry",
        "Representations of u(3) in u(n)",
        "Lie Algebras and Applications",
        "Su(3) r(3) wigner coefficients in the 2s-1d shell",
        "The Harmonic Oscillator in Modern Physics",
        "Generalized pseudo-su(3) model and pairing",
        "SU(3) Symmetry in Atomic Nuclei",
        "Shell-model Applica- tions in Nuclear Spectroscopy",
        "Intrinsic reflection asymme- try in odd-a nuclei",
        "Nuclear ground-state masses and deformations: Frdm(2012)",
        "Quadrupole-octupole coupling in the light actinides",
        "Octupole collectivity in 220Rn and 224Ra",
        "Schur Group Theory Software",
        "Su3lib: A c++ library for accurate computa- tion of wigner and racah coefficients of su(3)",
        "Su(3) reduced matrix ele- ment package",
        "Su(3) in shell-model calculations",
        "Reduced matrix elements for the leading spin zero states in the su ( 3 ) scheme *",
        "From Nucleons to Nucleus: Concepts of Mi- croscopic Nuclear Theory, Theoretical and Mathematical Physics"
      ],
      "meta_data": {
        "arxiv_id": "2408.08386v2",
        "doi": "10.1103/PhysRevC.110.054312",
        "authors": [
          "Alejandro Restrepo",
          "José P. Valencia"
        ],
        "published_date": "2024-08-15T19:09:36Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This research expands the octupole operator within the SU(3) second quantization formalism and introduces an octupole-octupole residual interaction (O3 · O3) into Elliott's model Hamiltonian. The main contribution is the development of a theoretical framework to include octupole degrees of freedom in SU(3) symmetry-based models, which has been sparsely explored. The authors compute the matrix elements of this extended Hamiltonian and apply it to calculate energy levels and analyze the B(E3; 3− 1 → 0+ 1 ) transition strength for the isotope 224Th using the semi-microscopic algebraic quartet model (SAQM) based on the proxy-SU(3) scheme. The model provides a slight improvement in fitting the low-lying spectrum of 224Th compared to the original Elliott model and allows for an estimation of octupole effective charges.",
        "methodology": "The study employs the semi-microscopic algebraic quartet model (SAQM) within the proxy-SU(3) scheme. The octupole operator r3Y3µ(Ω) is expanded in terms of one-body SU(3) tensors. A residual octupole-octupole two-body term (τO3 · O3) is incorporated into Elliott’s Hamiltonian, which already includes a harmonic oscillator mean field (ℏωˆn) and a quadrupole-quadrupole interaction (χQ2 · Q2). Reduced matrix elements (RME) of the O3 · O3 term are computed by separating proton and neutron spaces into products of recoupling coefficients and triple-barred RME. Parameters (χ, τ, and α, representing quadrupole strength, octupole strength, and nucleon contribution to excited states, respectively) are determined through a simultaneous diagonalization and fitting procedure against experimental energy levels. The electric octupole transition operator is used to calculate B(E3; 3− 1 → 0+ 1 ) strengths. Custom computer codes integrating various SU(3) libraries were developed for these calculations.",
        "experimental_setup": "The model was applied to the heavy even-even isotope 224Th, a known candidate for 'pear shape' deformation. A two-shell model space was used for protons (ηπ = 5, 6) and neutrons (ην = 6, 7). The ground state was represented by (60, 8)0+, and excited states by (65, 6)0−, corresponding to experimentally observed ground state and octupole excited bands, respectively. The Hamiltonian matrix elements were calculated for angular momentum states JΠ = 0+, 2+, ..., 18+ for the ground band and JΠ = 1−, 3−, ..., 17− for the excited band. The model space for intermediate states in the matrix element calculations was truncated to the ground and excited bands. The harmonic oscillator strength was set to ℏω = 6.7318 MeV. Parameters χ = −0.0032, τ = −0.0016, and α = 0.2398 were obtained by fitting to experimental energy levels, resulting in a Mean Square Error (MSE) of 0.1739 MeV2. Experimental energy levels and B(E3) values were sourced from the ENSDF database.",
        "limitations": "The model faces limitations including a trade-off between increasing the number of parameters for improved fitting and maintaining explanatory capabilities. A significant constraint arises from the truncation of the intermediate states in the calculation of the octupole-octupole interaction matrix elements, which might be too severe, potentially understating the interaction's influence. Furthermore, the lack of direct experimental data for the B(E3; 3− 1 → 0+ 1 ) transition in 224Th and for octupole effective charges poses a challenge for thorough theoretical validation and further analysis.",
        "future_research_directions": "Future research directions include exploring a weaker truncation of intermediate states in the calculation of octupole-octupole interaction matrix elements to potentially improve the model's accuracy. There is a strong call to motivate experimental research on 224Th, particularly for direct measurements of the B(E3; 3− 1 → 0+ 1 ) transition and octupole effective charges, which are crucial for validating theoretical predictions. The authors also hope to increase interest in SU(3)-symmetry based models for studying octupole deformation and anticipate utilizing results from novel radioactive beam accelerators to explore previously inaccessible nuclei, ultimately contributing to a better understanding of nuclear structure and fundamental symmetries of nature.",
        "experimental_code": "",
        "experimental_info": ""
      }
    },
    {
      "title": "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation",
      "full_text": "Data-Driven Stable Neural Feedback Loop Design Zuxun Xiong, Han Wang, Liqun Zhao, and Antonis Papachristodoulou Abstract— This paper proposes a data-driven approach to design a feedforward Neural Network (NN) controller with a stability guarantee for plants with unknown dynamics. We first introduce data-driven representations of stability conditions for Neural Feedback Loops (NFLs) with linear plants, which can be formulated into a semidefinite program (SDP). Subsequently, this SDP constraint is integrated into the NN training process to ensure stability of the feedback loop. The whole NN controller design problem can be solved by an iterative algorithm. Finally, we illustrate the effectiveness of the proposed method compared to model-based methods via numerical examples. I. I NTRODUCTION Decades before the recent rapid development of Artificial Intelligence (AI), pioneering research had already proposed the application of neural networks (NNs) as controllers for systems with unknown dynamics [1], [2], [3]. As related technologies have advanced, this research topic is now rele- vant in feedback control and has achieved success in several applications in recent years, e.g., optimal control [4], model predictive control [5] and reinforcement learning [6]. The ability of NNs to act as general function approxima- tors underpins their impressive performance on control. How- ever, this characteristic also presents challenges in providing rigorous stability and robustness guarantees. To address this issue, research in NN verification focuses on the relationship between inputs and outputs of NNs, ensuring they can operate reliably. In [7], an interval bound propagation (IBP) approach was proposed to calculate an estimate on the output range of every NN layer. This method is relatively simple but also very conservative. Quadratic constraints (QC) were later used to bound the nonlinear activation functions in NNs [8]: the NN verification problem with these QC bounds was formulated as a semidefinite program and solved efficiently. In [9], a tighter bound consisting of two sector constraints for different activation functions with higher accuracy was proposed. A Lipschitz bound estimation method [10] can also be used to evaluate the robustness of NNs. Based on these bounds and stability conditions provided by the NN verification literature, recent results focused on the design problem of NN feedback control. By imposing a Lipschitz bound constraint in the training process, a robust NN controller was designed [11]. In [12], a framework to design a stable NN controller through imitation learning (IL) for linear time-invariant (LTI) systems was proposed. Z. Xiong, H. Wang, Liqun Zhao, and A. Papachristodoulou are with Department of Engineering Science, University of Oxford, Parks Road, Oxford, OX1 3PJ, U.K. {zuxun.xiong, han.wang, liqun.zhao,antonis}@eng.ox.ac.uk AP was supported in part by UK’s Engineering, Physical Sciences Research Council projects EP/X017982/1 and EP/Y014073/1. To ensure that controllers can retain and process long- term memories, recurrent neural network controllers were trained with stability guarantees for partially observed linear systems [13] and nonlinear systems [14]. However, most of the existing studies assume the dynamics of the con- trolled systems are known, or at least partially known. This assumption contradicts the original intention of designing NN controllers, which is to control systems with unknown dynamics. This is exactly the gap we want to fill in this work. Data-driven control method promises to address this prob- lem [15]. A thorough introduction on how to apply data- driven methods to represent linear systems and design stable feedback control systems was given in [16]. Relevant meth- ods were applied to analyse the stability of nonlinear systems [17] using Sum of Squares (SOS) [18]. We also proposed a data-driven verification method to analyze the closed-loop system with an NN controller [19]. In this study, we use the term Neural Feedback Loop (NFL) to represent the closed- loop feedback system with an NN controller. The first contribution of this paper is a data-driven repre- sentation of stability conditions for an NFL with unknown linear plant. Based on the stability conditions, we propose an iterative design algorithm to train a stable NN. Secondly, an NN fine-tuning algorithm is proposed to stabilize an existing NN controller for an unknown system. Both these algorithms require only the collection of data generated by persistently exciting inputs. The rest of this paper is organized as follows. In Section II, we review tools for NFL analysis and the data-driven representation of linear state-feedback systems. In Section III, we formulate a data-driven stability condition for NFLs. Based on these conditions, we propose a stable NFL design algorithm and an NN fine-tuning algorithm in Section IV, and then illustrate two algorithms using numerical examples in Section V. We draw conclusions in the last section. Notation: We use Rn,m, Sn ++ and In to denote n-by-m dimensional real matrices, n-by-n positive definite matrices and n-by-n identity matrices, respectively. We use ||·|| F and tr(·) to denote the Frobenius norm and the trace of a matrix. II. P RELIMINARIES A. Neural Feedback Loop A generic NFL is a closed-loop dynamical system consist- ing of a plant G and an NN controller π(·) ∈ Rnπ . In this work, we consider G to be an LTI system of the form: x(k + 1) = AGx(k) + BGu(k), (1) where AG ∈ Rnx×nx and BG ∈ Rnx×nπ . Here x(k) ∈ Rnx and u(k) ∈ Rnπ are the system state and the control input, arXiv:2405.02100v2  [math.OC]  11 Oct 2024respectively. The controller is designed as a feed-forward fully connected neural network π(k) with l layers as follows: ω0(k) = x(k), (2a) νi(k) = Wiωi−1(k) + bi, for i = 1, . . . , l,(2b) ωi(k) = ϕi(νi(k)), for i = 1, . . . , l, (2c) π(k) = Wl+1ωl(k) + bl+1. (2d) For the ith layer of the NN, we use Wi ∈ Rni×ni−1 , bi ∈ Rni, ni, νi and ωi to denote its weight matrix, bias vector, number of neurons, and corresponding vectorized input and output, respectively. The input of the NN controller is ω0(k) = x(k), which is the state of the plant at time k. ϕi(·) : Rνi → Rνi is a vector of nonlinear activation functions on the ith layer, defined as ϕi(νi) = [φ(νi 1), . . . , φ(νi ni)]⊤, (3) where φ(·) : R → R is an activation function, such as ReLU, sigmoid, and tanh. B. Stability Verification for Neural Feedback Loop To verify stability of an NFL, a commonly used method is to isolate the nonlinear terms introduced by the activation functions, then treat the nonlinearity as a disturbance [12]. The NFL in (2) can be rewritten as: \u0014 π(k) νϕ(k) \u0015 = N \u0014 x(k) ωϕ(k) \u0015 (4a) ωϕ(k) = ϕ(νϕ(k)). (4b) where νϕ(k) ∈ Rnϕ and ωϕ(k) ∈ Rnϕ are vectors formed by stacking νi(k) and ωi(k) of each layer i, i = 1, . . . , l, ϕ(·) : Rnϕ → Rnϕ is the stacked activation function for all layers, where nϕ := Pl i=1 ni. N := \u0014Nπx Nπω Nνx Nνω \u0015 is a matrix consisting of NN weights Wi. It should be noticed that we impose the constraint π(0) = 0 to ensure the equilibrium remains at the origin with the NN controller. To achieve this, we also set all bias bi to be 0. To deal with the nonlinearities in (4b), sector constraints have been proposed to provide lower and upper bounds for different kinds of activation functions of NNs. The interested readers are referred to [8], [9] for a comprehensive review and comparison on different sectors. In this work, we rely on a commonly used local sector: Definition 1: Let α, β, ν, ¯ν ∈ R with α ≤ β and ν ≤ 0 ≤ ¯ν. The function φ : R → R satisfies the local sector [α, β] if (φ(ν) − αν) · (βν − φ(ν)) ≥ 0, ∀ν ∈ [ν, ¯ν]. (5) For example, φ(ν) := tanh( ν) restricted to the inter- val [ −¯ν, ¯ν] satisfies the local sector [α, β] with α = tanh(¯ν)/¯ν >0 and β = 1. By composing the sector bounds for each individual activation function, we can obtain sector constraints for the stacked nonlinearity ϕ(·). One composition method is shown in the following lemma. Lemma 1 ([12, Lemma 1]): Let αϕ, βϕ, ν, ¯ν ∈ Rnϕ be given with αϕ ≤ βϕ, and ν ≤ 0 ≤ ¯ν. Assume ϕ element- wisely satisfies the local sector [αϕ, βϕ] for all νϕ ∈ [ν, ¯ν]. Then, for any λ ∈ Rnϕ with λ ≥ 0, and for all νϕ ∈ [ν, ¯ν], ωϕ = ϕ(νϕ), we have \u0014νϕ ωϕ \u0015⊤ \u0014 −2AϕBϕΛ ( Aϕ + Bϕ)Λ (Aϕ + Bϕ)Λ −2Λ \u0015\u0014 νϕ ωϕ \u0015 ≥ 0, (6) where Aϕ = diag(αϕ), Bϕ = diag(βϕ), and Λ = diag(λ). Putting the result from Lemma 1 into a robust Lyapunov stability analysis framework, we can obtain a sufficient condition for an NFL to be stable. Theorem 1 ([12, Theorem 1]): Consider an NFL with plant G satisfying (1) and NN controller π as in (2) with an equilibrium point x∗ = 0 nx and a state constraint set X ⊆ {x : −¯x ≤ Hx ≤ ¯x}. If there exist a positive definite matrix P ∈ Snx ++, a vector λ ∈ Rnϕ with λ ≥ 0, and a matrix Λ := diag(λ) that satisfy R⊤ V \u0014 A⊤ GP AG − P A ⊤ GP BG B⊤ GP AG B⊤ GP BG \u0015 RV + R⊤ ϕ \u0014 −2AϕBϕΛ ( Aϕ + Bϕ) Λ (Aϕ + Bϕ) Λ −2Λ \u0015 Rϕ ≺ 0, (7a) and \u0014 ¯x2 i H⊤ i Hi P \u0015 ≥ 0, i= 1, . . . , nx, (7b) where RV := \u0014 Inx 0nx×nϕ Nπx Nπω \u0015 , Rϕ := \u0014 Nνx Nνω 0nϕ×nx Inϕ \u0015 , (7c) where H⊤ i is the ith row of the matrix H, then the NFL is locally asymptotically stable around the equilibrium point x∗, and the ellipsoid E(P) := \b x ∈ Rnx : x⊤P x≤ 1 \t is an inner-approximation to the region of attraction (ROA). C. Data-Driven Representation of the System Verifying stability of an NFL using (7) requires AG and BG to be precisely known. We refer to this type of approaches model-based. In contrast to model-based ap- proaches, direct data-driven approaches aim to bypass the identification step and use data directly to represent the system. Consider System (1). We carry out experiments to collect T-long time series of system inputs, states, and successor states as follows: U0,T := \u0002u(0) u(1) . . . u(T − 1)\u0003 ∈ Rnu×T (8a) X0,T := \u0002x(0) x(1) . . . x(T − 1)\u0003 ∈ Rnx×T (8b) X1,T := \u0002x(1) x(2) . . . x(T)\u0003 ∈ Rnx×T (8c) It should be noted that here we use u(t) to denote the in- stantaneous input signal for the open-loop system. The signal is not necessary produced by the NN controller π(·). This data series can be used to represent any T-long trajectory of the linear dynamical system as long as the following rank condition holds [16]: rank \u0012\u0014U0,T X0,T \u0015\u0013 = nu + nx. (9)To satisfy this rank condition, the collected data series should be “sufficiently long”, i.e., T ≥ (nu + 1)nx + nu. Under the rank condition, System (1) with a state-feedback controller u(k) = Kx(k) has the following data-driven representation [16, Theorem 2]: x(k + 1) = (AG + BGK)x(k) = X1,T GKx(k), (10) where GK is a T × nx matrix satisfying \u0014 K Inx \u0015 = \u0014U0,T X0,T \u0015 GK. (11) In our problem, the controller π(·) is a highly nonlinear NN. This makes it challenging to design a stable NFL directly from data. We will demonstrate how to combine this method with the NFL stability condition in the sequel. D. Problem Formulation Our problems of interest are: Problem 1: Consider an unknown plant G as in (1). Design a feed-forward fully connected NN controller π : Rnx → Rnπ that optimises a given loss function and stabilises the plant around the origin. Problem 2: Consider an unknown plant G as in (1) and a given NN controller π : Rnx → Rnπ . Minimally tune the NN to guarantee stability of the NFL around the origin. Throughout this paper, tanh is used as the activation func- tion φ(·) of the NN π(·). Our result could be easily extended to other types of activation functions, using different sectors. III. D ATA-DRIVEN STABILITY ANALYSIS FOR NFL S In this section we provide data-driven stability conditions for NFLs. To obtain convex conditions, a loop transformation is used to normalise the sector constraints [12]. A. Loop Transformation With a given controller π, we can calculate the bounds ν, ¯ν for every layer’s output based on a given state constraint set. If the plant G is also given, then AG, BG, and N are known and the stability condition (7) is convex in matrices P and Λ. Stability can then be verified by solving an SDP. However, for the design problem 1, G is unknown and π is to be designed, which means that AG, BG and N are all decision variables in the problem (7). The condition is then nonconvex. To deal with the nonlinearity, we first utilize a loop transformation to normalise the sector constraints. \u0014 π(k) νϕ(k) \u0015 = ˜N \u0014 x(k) zϕ(k) \u0015 (12a) zϕ(k) = ˜ϕ(νϕ(k)). (12b) The new internal state zϕ(k) is related to ωϕ(k), as follows ωϕ(k) = Bϕ − Aϕ 2 zϕ(k) + Aϕ + Bϕ 2 νϕ(k). (13) Through the transformation, the nonlinearity ˜ϕ is normalised, namely, lies in sector [−1nϕ×1, 1nϕ×1]. Using Lemma 1, we have \u0014νϕ zϕ \u0015⊤ \u0014Λ 0 0 −Λ \u0015\u0014νϕ zϕ \u0015 ≥ 0, ∀νϕ ∈ [ν, ν]. (14) The transformed matrix ˜N can be derived by ˜N = \u0014Nπx + C2(I − C4)−1Nνx C1 + C2(I − C4)−1C3 (I − C4)−1Nνx (I − C4)−1C3 \u0015 (15) where C1 = Nπω Bϕ − Aϕ 2 , C2 = Nπω Aϕ + Bϕ 2 , C3 = Nνω Bϕ − Aϕ 2 , C4 = Nνω Aϕ + Bϕ 2 . (16) We define ˜N := \u0014 ˜Nπx ˜Nπz ˜Nνx ˜Nνz \u0015 . Using the new system representation, the stability condition (7a) and (7c) can be reformulated as: ˜R⊤ V \u0014 A⊤ GP AG − P A ⊤ GP BG B⊤ GP AG B⊤ GP BG \u0015 ˜RV + ˜R⊤ ϕ \u0014 Λ 0 0 −Λ \u0015 ˜Rϕ ≺ 0, (17a) where ˜RV := \u0014 Inx 0 ˜Nπx ˜Nπz \u0015 , ˜Rϕ := \u0014 ˜Nνx ˜Nνz 0 Inϕ \u0015 . (17b) B. Data-Driven Representation of Stability Condition for NFLs The following theorem gives a sufficient condition for an NN controller π to stabilize an NFL with an unknown plant G. Theorem 2: Consider an unknown LTI plant G (1) with an equilibrium point x∗ = 0 nx and a state constraint set X ⊆ {x : −¯x ≤ Hx ≤ ¯x}. Let rank condition (9) hold. Find a matrix Q1 ∈ Snx ++, a diagonal matrix Q2 ∈ Snϕ ++, matrices L1 ∈ RT×nx, L2 ∈ RT×nϕ, L3 ∈ Rnϕ×nx and L4 ∈ Rnϕ×nϕ, such that (7b) is satisfied, and   Q1 0 LT 1 XT 1,T LT 3 0 Q2 LT 2 XT 1,T LT 4 X1,T L1 X1,T L2 Q1 0 L3 L4 0 Q2   ≻ 0. (18a) If we can design an NN to satisfy: ˜NQ = ¯U0,T L, (18b)\u0002I 0\u0003 Q = ¯X0,T L, (18c) where Q := \u0014Q1 0 0 Q2 \u0015 , L := \u0014L1 L2 L3 L4 \u0015 , ¯U0,T := \u0014U0,T 0 0 I \u0015 and ¯X0,T := \u0002X0,T 0\u0003 , then the designed NN controller can make the LTI system locally asymptotically stable around x∗. Besides, the set E(P) is an ROA inner- approximation for the system. Proof: By substituting (17b) into (17a) and then utilis- ing Schur complement, we can obtain stability condition:   P 0 A⊤ G + ˜N⊤ πxB⊤ G ˜N⊤ νx 0 Λ ˜N⊤ πzB⊤ G ˜N⊤ νz AG + BG ˜Nπx BG ˜Nπz P−1 0 ˜Nνx ˜Nνz 0 Λ −1   ≻ 0. (19)Here AG, BG are unknown parameters which can be re- placed using data-driven methods. Based on condition (9), we can find G1 ∈ RT×nx and G2 ∈ RT×nϕ that satisfy: \u0014 ˜Nπx I \u0015 = \u0014U0,T X0,T \u0015 G1, \u0014 ˜Nπz 0 \u0015 = \u0014U0,T X0,T \u0015 G2. (20) Then we can formulate AG + BG ˜Nπx and BG ˜Nπz as: AG + BG ˜Nπx = \u0002BG AG \u0003\u0014 ˜Nπx I \u0015 = \u0002BG AG \u0003\u0014 U0,T X0,T \u0015 G1 = X1,T G1, (21a) BG ˜Nπz = \u0002BG AG \u0003\u0014 ˜Nπz 0 \u0015 = X1,T G2. (21b) Now we can replace the terms containing system parameters with these two terms. Besides, multiplying condition (19) by diag(P−1, Λ−1, Inx, Inϕ) on both left and right, we obtain:   P−1 0 P−1G⊤ 1 X⊤ 1 P−1 ˜N⊤ νx 0 Λ −1 Λ−1G⊤ 2 X⊤ 1 Λ−1 ˜N⊤ νz X1G1P−1 X1G2Λ−1 P−1 0 ˜NνxP−1 ˜Nνz Λ−1 0 Λ −1   ≻ 0. (22) Let Q1 = P−1, Q2 = Λ −1, L1 = G1Q1, L2 = G2Q2, L3 = ˜NνxQ1 and L4 = ˜Nνz Q2. Then stability condition (22) can be expressed as (18a) and condition (20) from data- driven method can be reformulated to (18b) and (18c). Constraint (18a) is a linear matrix inequality (LMI). The nonconvexity comes from the equality constraint (18b). We will propose an iterative approach to efficiently deal with this nonconvexity in the next section. IV. I TERATIVE ALGORITHM FOR STABLE NFL D ESIGN In this section we propose an iterative algorithm to solve Problem 1. We incorporate the data-driven stability condi- tions (18) as hard constraints into an NN training framework with a certain loss function. Based on this algorithm, a fine- tuning algorithm is proposed for solving Problem 2. A. Iterative NFL Design Algorithm with Stability Constraints The stable NFL can be designed by solving the following optimisation problem. min N,Q,L η1L(N) − η2 log det(Q1) (23a) s.t. Q1 ∈ Snx ++, Q2 ∈ Snϕ ++, (23b) (18a), (18c), (23c) f(N)Q = ¯U0,T L, (23d) where f(N) = ˜N represents the loop transformation. The first term of the objective function is to minimise the loss function L(N) for NN prediction. It should be noted that we slightly abuse notation here: the prediction loss function L(·) is actually a function of NN weights W instead of matrix N, which is constructed from W. The second term is to maximise the ROA of the system. η1 and η2 are trade- off weights. Unlike traditional training process, we also add constraints from Theorem 2 for stability guarantee. To address the nonconvexity of optimisation problem (23), we propose an algorithm to solve the problem iteratively. Following the framework proposed by [12], we place the nonconvex loop transformation equality constraints (23d) into the objective function using the augmented Lagrangian method. The augmented loss function is: L1 a(N, Q, L, Y) = η1L(N) − η2 log det(Q1)+ tr(Y T (f(N)Q − ¯U0,T L) + ρ 2||f(N)Q − ¯U0,T L||2 F (24) where Y ∈ R(nu+nϕ)×(nx+nϕ) is the Lagrange multiplier, and ρ is a regularization parameter. Algorithm 1 Iterative design algorithm with hard constraints 1: procedure LINEAR NFL1( U0,T , X0,T , X1,T ) 2: k = 0, initialization 3: while ||f(Nk)Qk − ¯U0,T Lk||2 F > σdo 4: Nk+1 = arg minN L1 a(N, Qk, Lk, Yk) 5: (Q, L)k+1 = arg minQ,LL1 a(Nk+1, Q, L, Yk) s.t. (18a),(18c) 6: Y k+1 = Y k + ρ(f(Nk+1)Qk+1 − ¯U0,T Lk+1) 7: k = k + 1 8: end while 9: return the optimal result ( Nk,∗, Qk,∗, Lk,∗, Yk,∗) 10: end procedure The solving process is shown in Algorithm 1. Here k is used to denote the number of iteration and σ is used as a convergence criterion. In step 4, we use an IL framework to train the NN controller and update N. Then, we guarantee the stability of system by solving an SDP in step 5. B. Fine-Tuning Framework for Existing Unstable Neural Network Controller Another problem of interest is tuning an existing NN controller to render the closed-loop system stable. Due to the lack of stability guarantees for the traditional NN training process, the NFL may not be stable. Herein, we propose a verification and adaptation framework for existing NN controllers. First of all, stability of the NFL is verified by the data-driven stability verification algorithm proposed by our previous work [19]. If the verification SDP is feasible, we conclude that the NFL is already stable. If we want to guarantee stability using this method, the NN controller will need to be fine-tuned, i.e. minimally adapted, to guarantee local stability. The objective function for fine-tuning is shown as: L2 a(Nf , Q, L, Y) = η3||Nf ||2 F − η2 log det(Q1)+ tr(Y T (f( ¯N)Q − ¯U0,T L) + ρ 2||f( ¯N)Q − ¯U0,T L||2 F , (25) where Nf represents the amount that the NN should be fine- tuned. ¯N = N + Nf contains weights and biases of the obtained NN controller after fine-tuning. This equation also guarantees that the original structure of the NN will not be changed. The first term in the objective function reflects the amount of tuning required, while the other terms are the sameas (24). The fine-tuning algorithm is shown as Algorithm 2. Algorithm 2 Fine-tuning algorithm for an existing NN controller 1: procedure FINE -TUNING (N, U0,T , X0,T , X1,T ) 2: Data-driven stability verification [19] 3: if verification SDP is feasible then 4: return ¯N = N, Nf = 0 5: else 6: k = 0, initialization 7: while ||f( ¯Nk)Qk − ¯U0,T Lk||2 F > σdo 8: i = 0: 9: while ||Nf ||2 F > σ′ do 10: ¯Nk i+1 = ¯Nk i + arg minNf L2 a(Nf , Qk, Lk, Yk) 11: Update f( ¯Nk i+1) by (15) and (16) 12: i = i + 1 13: end while 14: ¯Nk+1 = ¯Nk i 15: (Q, L)k+1 = arg minQ,LL2 a( ¯Nk+1, Q, L, Yk) s.t. (18a),(18c) 16: Y k+1 = Y k +ρ(f( ¯Nk+1)Qk+1 − ¯U0,T Lk+1) 17: k = k + 1 18: end while 19: return the result ( ¯Nk,∗, Qk,∗, Lk,∗, Yk,∗) 20: end if 21: end procedure Algorithm 1 uses a gradient descent method in an IL framework to update N. For Algorithm 2, there is no need to retrain the NN. However, the objective function to be solved is nonconvex in ¯N. So we propose an iterative algorithm as shown in step 9 to step 13 to solve this nonconvex problem. In the (i + 1)th iteration, to eliminate the nonconvexity in the optimisation problem of step 10, we calculate the stacked sector bounds Aϕ, Bϕ and C1, C2, C3 and C4 based on the value of ¯Nk i from last iteration. This approximation is based on the assumption that the fine-tuning amount {Nf }k i is small. Then f( ¯Nk i+1) becomes a linear term on variable {Nf }k i+1, and the optimisation problem becomes a QP. After the first iterative algorithm converges, we update Q, Lin the same way as in Algorithm 1. V. N UMERICAL EXAMPLES A. Vehicle Controller Design To demonstrate the effectiveness of our method, we apply Algorithm 1 to the same numerical case as in [12], and compare the performance of our algorithm with the model- based NFL design approach in [12]. The vehicle lateral dynamics have four states, x = [e, ˙e, eθ, ˙eθ]⊤, where e and eθ represents the perpendicular distance to the lane edge, and the angle between the tangent to the straight section of the road and the projection of the longitudinal axis of vehicle, respectively. u ∈ R is the steering angle of the front wheel. For parameters, we use the constraint set X = [−2, 2] × [−5, 5]×[−1, 1]×[−5, 5]. The NN controller for this system has two layers, each with 10 neurons. The expert demon- stration data for IL training comes from an MPC law. Other parameters can be found in [20]. To compare the proposed data-driven control (DDC) method with traditional model- based control (MBC) method, we use exactly the same data set for NN training. We choose ρ = 1000, σ= 0.005 and the maximum iteration number as 20 for the proposed algorithm. To demonstrate the effectiveness of the DDC method, we carry out several simulations with different weight combina- tions. After 10 experiments, the average prediction loss of NN trained with ( η1 = 100, η2 = 100) is 0.128 while that with (η1 = 1000, η2 = 100) is 0.071. However, the ROA of the former is significantly larger than that of the latter. We then compare the proposed DDC approach with the MBC approach under the same settings: ( η1 = 100 , η2 = 100). The average prediction losses after 10 iterations is 0.128 for DDC, and 0.150 for MBC. Also, the ROAs under DDC are slightly bigger than that under MBC in most of the cases (7 times out of 10 experiments). ROAs under two approaches in one experiment are shown in Figure 1. -1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1 -5 -4 -3 -2 -1 0 1 2 3 4 5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 -5 -4 -3 -2 -1 0 1 2 3 4 5 Fig. 1. ROAs of NNs trained by DDC and MBC In this case, the NN trained by our proposed DDC ap- proach shows better performance in terms of both prediction accuracy and the size of the ROA. This may mean that the controller design using data-driven methods can outperform that based on specific models. One intuitive explanation is that the traditional model-based method which contains system identification and control design can be abstracted as a nested bilevel optimisation problem which usually leads to suboptimality. On the contrary, DDC is more flexible, making it more likely to find the “optimal” solution for the control design. Readers interested in this are referred to relevant discussions in [15], [21]. B. Existing NN Controller Fine-Tuning We now consider the same task but in a different scenario: we assume that there is already a trained controller for the vehicle. As there is no stability guarantee in a traditional NN training algorithm, we verify its stability first. If it is proved to be unstable, we modify the NN to make the new NFL stable. As we have no access to the training data and hope its structure and other performances remain unchanged, the Algorithm 2 is applied to fine-tune it.After fine-tuning the NN controller, we simulate the state variation from the same random initial point under input signals of two controllers. The sampling time is 0.02 s and the result can be shown as following figure. 0 1 2 3 4 5 6 7 8 9 10 0 2 4 6 8 Before fine-tuning 0 1 2 3 4 5 6 7 8 9 10 t/s 0 1 2 3 4 5 6 After fine-tuning Fig. 2. Simulation under the control of existing NN and fine-tuned NN Here we use ||x||2 as the value of vertical axis to reflect the variation of the state. Obviously, the NN controller before fine-tuning fails to stabilise the system in a long period, and it cannot be verified as stable by our verification method [19]. After 5 iterations, the proposed algorithm converges and the fine-tuned controller is guaranteed to result in a stable NFL. The fine-tuning process is more efficient compared to training a new NN controller in terms of the running time. The total time for the former is 0.692 s and the average time for NN training is more than 30 mins. All simulations are performed on a laptop with Apple M2 chip. The MOSEK solver is used to solve the SDP and QP here. VI. C ONCLUSIONS AND FUTURE WORKS This paper proposes a data-driven method to design or fine-tune NN controllers for unknown linear systems. The de- sign problem can be formulated as an optimisation problem and then solved by an iterative algorithm. Compared to tra- ditional NN controller design methods, our algorithm solves an SDP that includes stability constraints during training and thus provides stability guarantees. The second algorithm we propose can provide stability guarantee for existing NN controllers by fine-tuning their parameters without changing their structure. Compared to retraining a controller for the system, this method significantly improves efficiency. We use a vehicle lateral control example to verify the proposed methods and compare it with model-based approaches. In our example, the controller designed directly shows better performance in terms of accuracy and the size of ROA compared to that designed based on model. There are several promising directions for future works. First of all, we aim to extend our data-driven design method to NFLs involving nonlinear plants. Additionally, our method should be further developed to design more robust controllers to against disturbance and noise. Previous research has ad- dressed this in LQR design problem by introducing different regularisers to achieve various data-driven parameterisations of the LQR [22]. Finally, more theoretical works should be done to provide rigorous proof on the superiority of DDC for NN controllers design. REFERENCES [1] A. G. Barto, R. S. Sutton, and C. W. Anderson, “Neuronlike adaptive elements that can solve difficult learning control problems,” IEEE transactions on systems, man, and cybernetics , no. 5, pp. 834–846, 1983. [2] C. W. Anderson, “Learning to control an inverted pendulum with connectionist networks,” in 1988 American Control Conference, 1988, Conference Proceedings, pp. 2294–2298. [3] W. Li and J. J. E. Slotine, “Neural network control of unknown nonlinear systems,” in 1989 American Control Conference , 1989, Conference Proceedings, pp. 1136–1141. [4] Y . Chen, Y . Shi, and B. Zhang, “Optimal control via neural networks: A convex approach,” arXiv preprint arXiv:1805.11835 , 2018. [5] F. Fabiani and P. J. Goulart, “Reliably-stabilizing piecewise-affine neural network controllers,” IEEE Transactions on Automatic Control, vol. 68, no. 9, pp. 5201–5215, 2023. [6] E. Kaufmann, L. Bauersfeld, A. Loquercio, M. M ¨uller, V . Koltun, and D. Scaramuzza, “Champion-level drone racing using deep reinforce- ment learning,” Nature, vol. 620, no. 7976, pp. 982–987, 2023. [7] S. Gowal, K. D. Dvijotham, R. Stanforth, R. Bunel, C. Qin, J. Uesato, R. Arandjelovi ´c, T. Mann, and P. Kohli, “On the effectiveness of interval bound propagation for training verifiably robust models,” arXiv preprint, 2019. [8] M. Fazlyab, M. Morari, and G. J. Pappas, “Safety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming,” IEEE Transactions on Automatic Control, vol. 67, no. 1, pp. 1–15, 2022. [9] M. Newton and A. Papachristodoulou, “Neural network verification using polynomial optimisation,” in 2021 60th IEEE Conference on Decision and Control (CDC) , 2021, Conference Proceedings, pp. 5092–5097. [10] M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. Pappas, “Efficient and accurate estimation of lipschitz constants for deep neural networks,” Advances in Neural Information Processing Systems, vol. 32, 2019. [11] P. Pauli, A. Koch, J. Berberich, P. Kohler, and F. Allg ¨ower, “Training robust neural networks using lipschitz bounds,” IEEE Control Systems Letters, vol. 6, pp. 121–126, 2022. [12] H. Yin, P. Seiler, M. Jin, and M. Arcak, “Imitation learning with stability and safety guarantees,” IEEE Control Systems Letters , vol. 6, pp. 409–414, 2022. [13] F. Gu, H. Yin, L. E. Ghaoui, M. Arcak, P. Seiler, and M. Jin, “Recur- rent neural network controllers synthesis with stability guarantees for partially observed systems,” Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 5, pp. 5385–5394, 2022. [14] N. Junnarkar, H. Yin, F. Gu, M. Arcak, and P. Seiler, “Synthesis of stabilizing recurrent equilibrium network controllers,” in 2022 IEEE 61st Conference on Decision and Control (CDC) , 2022, Conference Proceedings, pp. 7449–7454. [15] R. Sepulchre, “Data-driven control: Part one of two [about this issue],” IEEE Control Systems Magazine , vol. 43, no. 5, pp. 4–7, 2023. [16] C. D. Persis and P. Tesi, “Formulas for data-driven control: Stabi- lization, optimality, and robustness,” IEEE Transactions on Automatic Control, vol. 65, no. 3, pp. 909–924, 2020. [17] M. Guo, C. D. Persis, and P. Tesi, “Data-driven stabilization of nonlinear polynomial systems with noisy data,” IEEE Transactions on Automatic Control , vol. 67, no. 8, pp. 4210–4217, 2022. [18] S. Prajna, A. Papachristodoulou, and W. Fen, “Nonlinear control synthesis by sum of squares optimization: a lyapunov-based approach,” in 2004 5th Asian Control Conference (IEEE Cat. No.04EX904) , vol. 1, 2004, Conference Proceedings, pp. 157–165 V ol.1. [19] H. Wang, Z. Xiong, L. Zhao, and A. Papachristodoulou, “Model-free verification for neural network controlled systems,” arXiv preprint arXiv:2312.08293, 2023. [20] H. Yin, P. Seiler, and M. Arcak, “Stability analysis using quadratic constraints for systems with neural network controllers,” IEEE Trans- actions on Automatic Control , vol. 67, no. 4, pp. 1980–1987, 2022. [21] F. D ¨orfler, J. Coulson, and I. Markovsky, “Bridging direct and indirect data-driven control formulations via regularizations and relaxations,” IEEE Transactions on Automatic Control, vol. 68, no. 2, pp. 883–897, 2023. [22] F. D ¨orfler, P. Tesi, and C. De Persis, “On the role of regularization in direct data-driven lqr control,” in 2022 IEEE 61st Conference on Decision and Control (CDC) . IEEE, 2022, Conference Proceedings, pp. 1091–1098.",
      "references": [
        "Neuronlike adaptive elements that can solve difficult learning control problems",
        "Learning to control an inverted pendulum with connectionist networks",
        "Neural network control of unknown nonlinear systems",
        "Optimal control via neural networks: A convex approach",
        "Reliably-stabilizing piecewise-affine neural network controllers",
        "Champion-level drone racing using deep reinforcement learning",
        "On the effectiveness of interval bound propagation for training verifiably robust models",
        "Safety verification and robustness analysis of neural networks via quadratic constraints and semidefinite programming",
        "Neural network verification using polynomial optimisation",
        "Efficient and accurate estimation of lipschitz constants for deep neural networks",
        "Training robust neural networks using lipschitz bounds",
        "Imitation learning with stability and safety guarantees",
        "Recurrent neural network controllers synthesis with stability guarantees for partially observed systems",
        "Synthesis of stabilizing recurrent equilibrium network controllers",
        "Data-driven control: Part one of two [about this issue]",
        "Formulas for data-driven control: Stabilization, optimality, and robustness",
        "Data-driven stabilization of nonlinear polynomial systems with noisy data",
        "Nonlinear control synthesis by sum of squares optimization: a lyapunov-based approach",
        "Model-free verification for neural network controlled systems",
        "Stability analysis using quadratic constraints for systems with neural network controllers",
        "Bridging direct and indirect data-driven control formulations via regularizations and relaxations",
        "On the role of regularization in direct data-driven lqr control"
      ],
      "meta_data": {
        "arxiv_id": "2405.02100v2",
        "authors": [
          "Zuxun Xiong",
          "Han Wang",
          "Liqun Zhao",
          "Antonis Papachristodoulou"
        ],
        "published_date": "2024-05-03T13:51:28Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "This paper proposes a data-driven approach to design feedforward Neural Network (NN) controllers with guaranteed stability for plants with unknown dynamics. The main contributions include introducing data-driven representations of stability conditions for Neural Feedback Loops (NFLs) with linear plants, formulating these conditions into a semidefinite program (SDP), and integrating this SDP constraint into the NN training process. The paper develops an iterative algorithm for stable NN controller design and a fine-tuning algorithm to stabilize existing NN controllers for unknown systems. Both algorithms only require data generated by persistently exciting inputs. Numerical examples demonstrate the effectiveness of the proposed method, showing improved performance in prediction accuracy and Region of Attraction (ROA) size compared to model-based approaches.",
        "methodology": "The methodology involves a data-driven approach to represent stability conditions for Neural Feedback Loops (NFLs). It utilizes a loop transformation to normalize sector constraints of NN activation functions, allowing the stability conditions to be formulated as linear matrix inequalities (LMIs). The unknown linear plant dynamics are replaced using data-driven representations derived from collected input and state trajectories. The stable NN controller design and fine-tuning problems are formulated as optimization problems incorporating these data-driven stability constraints. These nonconvex problems are solved using iterative algorithms based on the augmented Lagrangian method. Algorithm 1 iteratively updates NN parameters by minimizing a loss function and then solves an SDP for stability parameters. Algorithm 2, for fine-tuning existing controllers, iteratively adjusts a small perturbation to NN parameters by solving a quadratic program (QP) under the assumption of small tuning amounts, followed by an SDP update for stability parameters. The tanh function is used as the activation function.",
        "experimental_setup": "The proposed methods are validated using numerical examples, specifically a vehicle lateral dynamics control problem. The plant is a four-state linear time-invariant (LTI) system with one input (steering angle), and a state constraint set X is defined. The NN controller consists of two layers, each with 10 neurons, using tanh activation. Expert demonstration data for imitation learning (IL) training is generated from a Model Predictive Control (MPC) law. The algorithms use specific parameters, including a regularization parameter (ρ = 1000), convergence criteria (σ = 0.005), and a maximum of 20 iterations. Performance is compared against a model-based control (MBC) approach from prior work [12], evaluating prediction loss and the size of the Region of Attraction (ROA). The fine-tuning algorithm's efficiency is also demonstrated by comparing its runtime to retraining a new NN. All simulations are performed on a laptop with an Apple M2 chip, utilizing the MOSEK solver for SDPs and QPs.",
        "limitations": "The current data-driven stability conditions and iterative design method are primarily applicable to Neural Feedback Loops (NFLs) with linear plants. The fine-tuning algorithm's approximation, which converts the nonconvex optimization problem into a quadratic program, is based on the assumption that the fine-tuning amount for the neural network parameters is small. The stability guarantees provided are for local asymptotic stability around an equilibrium point. Additionally, the data-driven representation of the system requires the collection of data generated by persistently exciting inputs to satisfy a necessary rank condition.",
        "future_research_directions": "Future research directions include extending the data-driven design method to Neural Feedback Loops (NFLs) involving nonlinear plants. Another promising area is to further develop the method to design more robust controllers that can withstand disturbances and noise. Finally, more theoretical work is needed to provide rigorous proof regarding the superiority of data-driven control (DDC) for NN controller design compared to traditional model-based approaches.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Self-Consistency Chain-of-Thought (SC-CoT) still lacks a *causal* notion of “rationale support.” Even PMI-style evidence gain, log p(a|x,r)−log p(a|x), can be inflated by (i) answer leakage (the rationale restates the final answer), (ii) generic verbal filler that increases fluency, or (iii) spurious priors (frequent answers). As a result, SC-style aggregation can over-trust persuasive but non-informative explanations—harmful in tutoring/decision support—and it cannot decide *how much* reasoning is actually necessary per instance. The focused gap: a training-free CoT inference rule that (1) scores rationales by *counterfactual contribution* (what changes if the rationale content is perturbed), (2) extracts a *minimal sufficient* subset of the rationale to avoid verbosity and reduce leakage, and (3) uses these signals to drive instance-adaptive compute (early exit vs. deeper verification).",
    "method": "Counterfactual Evidence & Sufficiency–Weighted Self-Consistency with Adaptive Deliberation (CES-ADSC).\n\nHuman-thinking inspiration: people trust an explanation when it would *change their belief* if removed/altered, and we prefer the *shortest* explanation that still justifies the answer.\n\nGiven question x, we sample CoT traces, but we score only the rationale *excluding* the final “answer sentence” to reduce answer leakage.\n\nFor each sampled trace t_i:\n1) Parse (r_i, ans_i) by splitting at the last “The answer is …”. Use r_i only for scoring.\n2) Compute three teacher-forced log-likelihoods of a canonical answer string y(ans_i) (e.g., “The answer is 17.”):\n   - base: L0 = log p(y|x)\n   - with-rationale: Lr = log p(y|x, r_i)\n   - counterfactual/perturbed: Lcf = log p(y|x, r̃_i)\n     where r̃_i is a *content-destroying perturbation* of r_i (mask all numbers and optionally shuffle/strip math operators). This keeps length/fluency but removes key evidence.\n3) Define two complementary support signals:\n   - Evidence gain (prior-corrected): EG = Lr − L0\n   - Counterfactual contribution (causal/faithfulness proxy): CC = Lr − Lcf\n   EG answers “does any rationale help beyond priors?”; CC answers “does the *specific content* matter, or is it just verbosity?”\n\n4) Minimal-sufficient rationale length (prefix sufficiency): split r_i into <=M chunks (sentences/newlines). Find the smallest prefix r_i[1:k] whose likelihood reaches near-full support:\n      find min k s.t. log p(y|x, r_i[1:k]) ≥ Lr − ε\n   Let ℓ*_i = token length of that minimal prefix. This operationalizes “minimal sufficient reasoning” in a way that is scriptable and model-agnostic.\n\n5) Per-sample score and aggregation:\n      score_i = EG + α·CC − λ·ℓ*_i\n      W(a) = Σ_{i: ans_i=a} exp(score_i/τ)\n   Return argmax_a W(a).\n\n6) Instance-adaptive compute (two-speed deliberation):\n   - Stage 1: sample K_short short traces.\n   - Convert W(a) to posterior P(a).\n   - Early-stop if P(top1) ≥ p_stop or margin ≥ m_stop.\n   - Otherwise Stage 2: sample K_long longer *verification* traces (“check arithmetic, look for contradictions”), rescore and reaggregate.\n\nNovelty over the current Meta-PMI-ADSC:\n- Adds an explicit *counterfactual contribution* term CC computed via rationale perturbation, making the confidence signal harder to game with fluent filler and closer to a causal/faithfulness test.\n- Enforces *answer-leakage control* by scoring only the rationale segment (not the answer sentence) and by using a perturbation that preserves surface form while destroying evidence.\n- Introduces a *minimal-sufficient prefix* estimator ℓ* (a tractable MDL/sufficiency proxy) instead of penalizing raw rationale length, aligning more tightly with human “stop when enough evidence is assembled.”\nAll components are training-free and testable in a single Python script.",
    "experimental_setup": "Datasets (script-feasible subsets; numeric answers):\n- GSM8K test: n=200 random.\n- SVAMP test (fallback: ASDiv): n=200 random.\n\nModel:\n- google/flan-t5-base (HuggingFace, seq2seq; reproducible and fast).\n\nBaselines:\n1) Direct (no CoT): greedy decode with “answer only” prompt.\n2) SC-Majority: K=10, max_new_tokens=256, majority vote.\n3) BoN-AnsLogP: K=10, choose answer with highest log p(y|x,r) (no counterfactual, no sufficiency).\n4) Meta-PMI-ADSC: the user’s current method (EG only + length penalty + adaptive compute), matched sampling budgets.\n5) CES-ADSC (proposed): EG + counterfactual CC + minimal-sufficient prefix length ℓ* + adaptive compute.\n\nKey hyperparameters (tuned on 50 GSM8K val examples):\n- α ∈ {0.0, 0.5, 1.0} (counterfactual weight)\n- λ ∈ {0, 1e-3, 2e-3, 5e-3}\n- ε ∈ {0.2, 0.5} (nats/logp tolerance for sufficiency)\n- p_stop ∈ {0.75, 0.85}, m_stop ∈ {0.25, 0.35}\n\nMetrics:\n- Primary: accuracy (exact match of extracted numeric answer).\n- Secondary: avg generated tokens per question; stage2_rate; and an explanatory safety proxy: mean CC on correct vs incorrect (should separate).\n\nFeasibility notes:\n- Counterfactual and sufficiency scoring adds a few extra forward passes, but K_short/K_long are smaller than fixed SC and early-stop reduces generation, keeping runtime manageable on 200-example subsets.",
    "primary_metric": "accuracy",
    "experimental_code": "import re, math, torch\nfrom collections import defaultdict\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nANS_RE = re.compile(r\"The answer is\\s*(-?\\d+\\.?\\d*)\", re.IGNORECASE)\nNUM_RE = re.compile(r\"-?\\d+\\.?\\d*\")\n\ndef extract_answer(text: str):\n    m = ANS_RE.search(text)\n    if m:\n        return m.group(1)\n    nums = NUM_RE.findall(text.replace(\",\", \"\"))\n    return nums[-1] if nums else None\n\ndef split_rationale_answer(text: str):\n    # returns rationale (excluding answer sentence) and answer clause\n    idx = text.lower().rfind(\"the answer is\")\n    if idx == -1:\n        return text.strip(), \"\"\n    return text[:idx].strip(), text[idx:].strip()\n\ndef chunk_rationale(r: str, max_chunks: int = 6):\n    # lightweight chunking: split by newline; fallback to sentence-ish splitting.\n    parts = [p.strip() for p in r.split(\"\\n\") if p.strip()]\n    if len(parts) <= 1:\n        parts = [p.strip() for p in re.split(r\"(?<=[\\.!\\?])\\s+\", r) if p.strip()]\n    if not parts:\n        return []\n    # merge into <= max_chunks\n    if len(parts) <= max_chunks:\n        return parts\n    # simple merge: concatenate consecutive parts\n    step = math.ceil(len(parts) / max_chunks)\n    merged = []\n    for i in range(0, len(parts), step):\n        merged.append(\" \".join(parts[i:i+step]))\n    return merged[:max_chunks]\n\ndef mask_numbers(r: str):\n    # destroy evidential content while preserving length/fluency-ish\n    return re.sub(r\"-?\\d+\\.?\\d*\", \"X\", r)\n\ndef logp_text(model, tok, encoder_text, decoder_text, device):\n    enc = tok(encoder_text, return_tensors=\"pt\", truncation=True).to(device)\n    lab = tok(decoder_text, return_tensors=\"pt\", truncation=True).input_ids.to(device)\n    out = model(**enc, labels=lab)\n    n = (lab != tok.pad_token_id).sum().item()\n    return float(-out.loss.detach().cpu() * n)\n\ndef sample_cot(model, tok, question, n, max_new_tokens, temperature=0.7, top_p=0.95, device=\"cpu\", verify=False):\n    prefix = f\"Q: {question}\\nA: Let’s think step by step.\"\n    if verify:\n        prefix += \" Check for mistakes and contradictions.\"\n    prefix += \" End with 'The answer is <number>'.\"\n\n    enc = tok(prefix, return_tensors=\"pt\", truncation=True).to(device)\n    gen = model.generate(\n        **enc,\n        do_sample=True,\n        temperature=temperature,\n        top_p=top_p,\n        max_new_tokens=max_new_tokens,\n        num_return_sequences=n,\n    )\n    return prefix, tok.batch_decode(gen, skip_special_tokens=True)\n\ndef minimal_sufficient_prefix_len(model, tok, prefix, rationale, ans_text, Lr_full,\n                                 eps=0.5, max_chunks=6, device=\"cpu\"):\n    chunks = chunk_rationale(rationale, max_chunks=max_chunks)\n    if not chunks:\n        return 0\n    # find minimal k such that logp(ans|x, prefix+chunks[:k]) >= Lr_full - eps\n    for k in range(1, len(chunks)+1):\n        r_pref = \"\\n\".join(chunks[:k])\n        Lk = logp_text(model, tok, encoder_text=prefix + \"\\n\" + r_pref, decoder_text=ans_text, device=device)\n        if Lk >= (Lr_full - eps):\n            return len(tok(r_pref, add_special_tokens=False).input_ids)\n    # if never reaches, use full rationale length\n    return len(tok(rationale, add_special_tokens=False).input_ids)\n\ndef ces_adsc(model, tok, question,\n             K_short=4, K_long=4,\n             alpha=0.5, lam=2e-3, tau=1.0, eps=0.5,\n             p_stop=0.85, m_stop=0.35,\n             device=\"cpu\"):\n    base_cache = {}\n\n    def score_samples(prefix, samples):\n        per_answer = defaultdict(float)\n        out_toks = 0\n        for t in samples:\n            ans = extract_answer(t)\n            if ans is None:\n                continue\n            rationale, _ = split_rationale_answer(t)\n            out_toks += len(tok(t, add_special_tokens=False).input_ids)\n\n            ans_text = f\"The answer is {ans}.\"\n            if ans not in base_cache:\n                base_cache[ans] = logp_text(model, tok, encoder_text=prefix, decoder_text=ans_text, device=device)\n            L0 = base_cache[ans]\n\n            # Score using rationale-only to reduce answer leakage\n            Lr = logp_text(model, tok, encoder_text=prefix + \"\\n\" + rationale, decoder_text=ans_text, device=device)\n            Lcf = logp_text(model, tok, encoder_text=prefix + \"\\n\" + mask_numbers(rationale), decoder_text=ans_text, device=device)\n\n            EG = Lr - L0              # evidence gain beyond priors\n            CC = Lr - Lcf             # counterfactual contribution (content matters)\n\n            l_star = minimal_sufficient_prefix_len(model, tok, prefix, rationale, ans_text, Lr,\n                                                   eps=eps, max_chunks=6, device=device)\n            score = EG + alpha * CC - lam * l_star\n            per_answer[ans] += math.exp(score / max(tau, 1e-6))\n        return per_answer, out_toks\n\n    def posterior(weights):\n        if not weights:\n            return None, None, None\n        Z = sum(weights.values())\n        items = sorted(((a, w/Z) for a, w in weights.items()), key=lambda x: x[1], reverse=True)\n        top1a, top1p = items[0]\n        top2p = items[1][1] if len(items) > 1 else 0.0\n        return top1a, top1p, top1p - top2p\n\n    # Stage 1\n    prefix, s1 = sample_cot(model, tok, question, n=K_short, max_new_tokens=64, device=device, verify=False)\n    w, tok_used = score_samples(prefix, s1)\n    a1, p1, m1 = posterior(w)\n    if a1 is None:\n        return None, tok_used, 1\n    if (p1 >= p_stop) or (m1 >= m_stop):\n        return a1, tok_used, 1\n\n    # Stage 2 (verification)\n    _, s2 = sample_cot(model, tok, question, n=K_long, max_new_tokens=256, device=device, verify=True)\n    w2, tok2 = score_samples(prefix, s2)\n    for a, ww in w2.items():\n        w[a] += ww\n    a2, _, _ = posterior(w)\n    return a2, tok_used + tok2, 2\n\nif __name__ == \"__main__\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    tok = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\").to(device)\n\n    ds = load_dataset(\"gsm8k\", \"main\", split=\"test\").shuffle(seed=0).select(range(200))\n\n    correct = total = 0\n    tot_tokens = stage2 = 0\n    for ex in ds:\n        pred, used_toks, st = ces_adsc(model, tok, ex[\"question\"], device=device)\n        gold = extract_answer(ex[\"answer\"])\n        if pred is not None and gold is not None:\n            correct += int(pred == gold)\n            total += 1\n        tot_tokens += used_toks\n        stage2 += int(st == 2)\n\n    print({\n        \"accuracy\": correct / max(total, 1),\n        \"n\": total,\n        \"avg_gen_tokens\": tot_tokens / len(ds),\n        \"stage2_rate\": stage2 / len(ds),\n    })\n",
    "expected_result": "On GSM8K (n=200) with flan-t5-base (temperature sampling):\n- Direct (no CoT): ~0.18–0.25 accuracy\n- SC-Majority (K=10, 256 tokens): ~0.22–0.30\n- BoN-AnsLogP: ~0.23–0.31\n- Meta-PMI-ADSC: ~0.27–0.36\n- CES-ADSC (proposed): ~0.29–0.38\n\nCompute/efficiency (secondary):\n- CES-ADSC should reduce average generated tokens by ~25–45% vs fixed SC-Majority, with stage2_rate ~30–55% depending on p_stop/m_stop.\n\nSafety/faithfulness proxy (secondary, diagnostic):\n- Mean CC (=Lr−Lcf) should be higher on correct predictions than incorrect ones (expected separation ~0.5–1.5 nats), indicating CC tracks evidential reliance better than EG alone.\n\nOn SVAMP/ASDiv (n=200): expect similar relative ordering, with CES-ADSC +0.01–0.03 absolute over Meta-PMI-ADSC and +0.03–0.06 over SC-Majority.",
    "expected_conclusion": "CES-ADSC strengthens CoT self-consistency by turning “confidence” from a fluency/verbosity proxy into an *evidence-with-counterfactuals* signal: a rationale is trusted only insofar as (i) it raises the answer probability beyond the prior (EG) and (ii) its *specific content* matters (CC), approximated by perturbation-based counterfactual testing. In addition, it estimates a minimal sufficient rationale prefix ℓ* and penalizes that quantity, aligning aggregation with human cognitive economy (“stop once enough evidence is assembled”) rather than blunt length penalties.\n\nAcademic value: the hypothesis reframes SC-CoT aggregation as counterfactual evidence accumulation with an explicit sufficiency test, connecting LLM inference-time reasoning to ideas from causal attribution and minimal explanations—yet remains training-free and broadly applicable.\n\nPractical/social value: by down-weighting rationales that do not survive counterfactual perturbations and by early-stopping when evidence is decisive, the method aims to reduce both (a) verbose hallucinated explanations and (b) inference cost/energy, improving deployability in education and decision-support settings where unfaithful explanations can mislead users."
  },
  "experimental_design": {
    "experiment_summary": "Goal: evaluate inference-time answer selection for grade-school math word problems. Given a word problem x, the system must output the final numeric answer (not a full solution). We compare a new training-free self-consistency aggregation rule (CES-ADSC) against a strong inference-only baseline (Meta-PMI-ADSC) while holding the underlying LM fixed.\n\nTask workflow (per example):\n1) Prompt an LM to generate K chain-of-thought (CoT) traces that end with a canonical answer sentence “The answer is <number>.”\n2) Parse each trace into rationale r_i (everything before the last “The answer is …”) and answer a_i.\n3) For each unique answer a, compute teacher-forced log-likelihoods of the canonical answer string y(a) under (i) the question only, (ii) question + rationale, and (iii) question + a perturbed rationale that preserves surface form but destroys content (e.g., number masking). These scores quantify how much the rationale causally supports the answer vs. merely increasing fluency or leaking the answer.\n4) Aggregate per-sample scores into a weighted self-consistency vote and output the top answer.\n5) Adaptive deliberation: run a short Stage-1 sampling pass; early-exit if posterior mass/margin is high; otherwise run a longer Stage-2 “verification” sampling pass.\n\nScale fit to the provided Runner (H200 140GB VRAM, 240GB RAM):\n- Use a single, small-ish model (Flan-T5-base, ~250M) to make multiple teacher-forced scoring passes cheap.\n- Evaluate on a script-feasible subset of GSM8K test (n=200) to keep total generation + scoring within minutes-hours on one GPU.\n- Hyperparameter selection (optional) is done on a tiny GSM8K validation slice (n=50) via Optuna with limited trials (<=20). No model training.\n- Generation budgets are capped (Stage-1: short traces max_new_tokens=64; Stage-2: long traces max_new_tokens=256) and early-exit reduces average compute.",
    "runner_config": {
      "runner_label": [
        "self-hosted",
        "gpu-runner"
      ],
      "description": "NVIDIA H200, VRAM: 140 GB, RAM: 240 GB"
    },
    "evaluation_metrics": [
      {
        "name": "accuracy",
        "description": "Correctness criteria: A prediction is correct if the extracted numeric value from the model output equals the gold numeric value from the dataset solution. Extract numbers by (a) first matching the explicit pattern /The answer is\\s*(-?\\d+\\.?\\d*)/i; if absent, fall back to the last number-like substring in the output after removing commas. Normalize by stripping whitespace and leading plus signs; treat integers and decimals as exact strings after normalization (GSM8K answers are typically integers).\n\nCalculation method: accuracy = (# correct predictions) / (N), where N is the number of evaluated examples with both a parsed gold answer and a parsed predicted answer.\n\nTask appropriateness: GSM8K is a closed-form numeric QA task; accuracy directly measures whether the final answer is right, regardless of the particular reasoning trace.\n\nRelevant visualizations: (1) Bar chart of accuracy by method; (2) bootstrap 95% CI error bars for accuracy; (3) histogram of per-problem answer frequency for error analysis (optional)."
      },
      {
        "name": "avg_generated_tokens",
        "description": "Correctness criteria: Not applicable (efficiency metric).\n\nCalculation method: For each example, count the number of generated tokens across all sampled traces (Stage-1 plus Stage-2 if triggered) using the model tokenizer with add_special_tokens=False. Report mean across the evaluation set.\n\nTask appropriateness: CES-ADSC claims instance-adaptive compute; average generated tokens is a direct proxy for inference cost/latency/energy.\n\nRelevant visualizations: (1) Boxplot/violin plot of tokens per example; (2) scatter plot of tokens vs. correctness (correct/incorrect colored)."
      },
      {
        "name": "stage2_rate",
        "description": "Correctness criteria: Not applicable (efficiency/control metric).\n\nCalculation method: stage2_rate = (# examples that trigger Stage-2 verification) / N_eval.\n\nTask appropriateness: Directly quantifies how often adaptive deliberation decides that additional reasoning is necessary.\n\nRelevant visualizations: (1) Single bar with confidence interval; (2) histogram of posterior top-1 probability after Stage-1 with the stop threshold marked."
      },
      {
        "name": "cc_separation",
        "description": "Correctness criteria: Not a correctness metric; diagnostic for faithfulness.\n\nCalculation method: For each sampled trace t_i, compute CC_i = Lr − Lcf (counterfactual contribution). Group traces by whether the final aggregated prediction for the example is correct. Report Δ = mean(CC | correct) − mean(CC | incorrect) and/or an effect size (Cohen’s d).\n\nTask appropriateness: The hypothesis is that counterfactual contribution detects content-dependent rationales and correlates with correctness better than evidence-gain alone.\n\nRelevant visualizations: (1) Overlaid density plots of CC for correct vs. incorrect; (2) mean±SE bar chart; (3) ROC curve for using CC to predict correctness (optional)."
      }
    ],
    "models_to_use": [
      "google/flan-t5-base (~250M parameters)"
    ],
    "datasets_to_use": [
      "openai/gsm8k (main)"
    ],
    "proposed_method": {
      "method_name": "CES-ADSC (Counterfactual Evidence & Sufficiency–Weighted Self-Consistency with Adaptive Deliberation)",
      "description": "Training-free CoT aggregation that weights each sampled answer by (i) evidence gain beyond priors (EG = log p(y|x,r) − log p(y|x)), (ii) counterfactual contribution of rationale content (CC = log p(y|x,r) − log p(y|x, r~) where r~ masks numbers/operators to preserve fluency but destroy evidence), and (iii) a minimal-sufficient rationale prefix penalty ℓ* (smallest rationale prefix achieving near-full support within ε of full-rationale logp). Uses two-stage adaptive deliberation: early-exit after short traces if posterior mass/margin is high; otherwise sample longer verification traces and re-aggregate.",
      "optuna_config": {
        "enabled": true,
        "n_trials": 20,
        "search_spaces": [
          {
            "param_name": "alpha",
            "distribution_type": "categorical",
            "choices": [
              0.0,
              0.5,
              1.0
            ]
          },
          {
            "param_name": "lambda_len",
            "distribution_type": "categorical",
            "choices": [
              0.0,
              0.001,
              0.002,
              0.005
            ]
          },
          {
            "param_name": "epsilon",
            "distribution_type": "categorical",
            "choices": [
              0.2,
              0.5
            ]
          },
          {
            "param_name": "p_stop",
            "distribution_type": "categorical",
            "choices": [
              0.75,
              0.85
            ]
          },
          {
            "param_name": "m_stop",
            "distribution_type": "categorical",
            "choices": [
              0.25,
              0.35
            ]
          },
          {
            "param_name": "tau",
            "distribution_type": "categorical",
            "choices": [
              0.7,
              1.0,
              1.3
            ]
          },
          {
            "param_name": "K_short",
            "distribution_type": "categorical",
            "choices": [
              3,
              4,
              5
            ]
          },
          {
            "param_name": "K_long",
            "distribution_type": "categorical",
            "choices": [
              3,
              4,
              6
            ]
          }
        ]
      }
    },
    "comparative_methods": [
      {
        "method_name": "Meta-PMI-ADSC (Evidence-Gain Self-Consistency with Adaptive Deliberation)",
        "description": "Strong inference-only baseline that approximates rationale support using only evidence gain EG = log p(y|x,r) − log p(y|x) (PMI-style) with a simple length penalty on the full rationale length (no counterfactual perturbation, no minimal-sufficient prefix). Uses the same two-stage adaptive sampling/verification schedule and the same parsing rule that excludes the final answer sentence from scoring to control answer leakage.",
        "optuna_config": {
          "enabled": true,
          "n_trials": 20,
          "search_spaces": [
            {
              "param_name": "lambda_len",
              "distribution_type": "categorical",
              "choices": [
                0.0,
                0.001,
                0.002,
                0.005
              ]
            },
            {
              "param_name": "p_stop",
              "distribution_type": "categorical",
              "choices": [
                0.75,
                0.85
              ]
            },
            {
              "param_name": "m_stop",
              "distribution_type": "categorical",
              "choices": [
                0.25,
                0.35
              ]
            },
            {
              "param_name": "tau",
              "distribution_type": "categorical",
              "choices": [
                0.7,
                1.0,
                1.3
              ]
            },
            {
              "param_name": "K_short",
              "distribution_type": "categorical",
              "choices": [
                3,
                4,
                5
              ]
            },
            {
              "param_name": "K_long",
              "distribution_type": "categorical",
              "choices": [
                3,
                4,
                6
              ]
            }
          ]
        }
      }
    ]
  },
  "experiment_code": {
    "train_py": "import math\nfrom copy import deepcopy\nfrom typing import Any, Dict, List, Optional\n\nimport hydra\nimport numpy as np\nimport optuna\nimport torch\nimport wandb\nfrom omegaconf import DictConfig, OmegaConf\nfrom torch.utils.data import DataLoader\nfrom transformers import get_linear_schedule_with_warmup\n\nfrom src.model import build_method, create_model_and_tokenizer, extract_answer\nfrom src.preprocess import QADataset, load_dataset_split, set_seed\n\n\ndef _apply_mode_overrides(cfg: DictConfig) -> DictConfig:\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        if hasattr(cfg, \"run\") and hasattr(cfg.run, \"optuna\"):\n            cfg.run.optuna.n_trials = 0\n        if hasattr(cfg, \"run\") and hasattr(cfg.run, \"dataset\"):\n            subset_size = int(getattr(cfg.run.dataset, \"subset_size\", 2) or 2)\n            cfg.run.dataset.subset_size = min(subset_size, 2)\n        if hasattr(cfg, \"run\") and hasattr(cfg.run, \"training\") and getattr(cfg.run.training, \"enabled\", False):\n            cfg.run.training.epochs = min(int(getattr(cfg.run.training, \"epochs\", 1)), 1)\n            cfg.run.training.batch_size = min(int(getattr(cfg.run.training, \"batch_size\", 1)), 1)\n        if hasattr(cfg, \"run\") and hasattr(cfg.run, \"model\"):\n            cfg.run.model.max_new_tokens_stage1 = min(\n                int(getattr(cfg.run.model, \"max_new_tokens_stage1\", 16)), 16\n            )\n            cfg.run.model.max_new_tokens_stage2 = min(\n                int(getattr(cfg.run.model, \"max_new_tokens_stage2\", 32)), 32\n            )\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(f\"Unsupported mode: {cfg.mode}\")\n    return cfg\n\n\ndef _get_optuna_cfg(run_cfg: DictConfig) -> Optional[DictConfig]:\n    return getattr(run_cfg, \"optuna\", None)\n\n\ndef _space_to_dict(space: Any) -> Dict[str, Any]:\n    if isinstance(space, DictConfig):\n        return OmegaConf.to_container(space, resolve=True)\n    if isinstance(space, dict):\n        return dict(space)\n    return {}\n\n\ndef _resolve_method_params(run_cfg: DictConfig, mode: str) -> Dict[str, Any]:\n    params: Dict[str, Any] = {}\n    for key in (\"method_params\", \"params\"):\n        if hasattr(run_cfg, key):\n            candidate = getattr(run_cfg, key)\n            if isinstance(candidate, DictConfig):\n                params.update(OmegaConf.to_container(candidate, resolve=True))\n            elif isinstance(candidate, dict):\n                params.update(candidate)\n    optuna_cfg = _get_optuna_cfg(run_cfg)\n    if optuna_cfg is not None and getattr(optuna_cfg, \"search_spaces\", None):\n        for space in optuna_cfg.search_spaces:\n            spec = _space_to_dict(space)\n            name = spec.get(\"param_name\") or spec.get(\"name\")\n            if not name or name in params:\n                continue\n            dist = str(spec.get(\"distribution_type\", \"categorical\")).lower()\n            if dist == \"categorical\":\n                choices = list(spec.get(\"choices\") or [])\n                if choices:\n                    params[name] = choices[0]\n            elif dist == \"int\":\n                low = int(spec.get(\"low\", 0))\n                high = int(spec.get(\"high\", low))\n                params[name] = int((low + high) // 2)\n            elif dist == \"float\":\n                low = float(spec.get(\"low\", 0.0))\n                high = float(spec.get(\"high\", low))\n                params[name] = float(low + (high - low) / 2)\n            else:\n                raise ValueError(f\"Unsupported distribution type: {dist}\")\n    params = {k: v for k, v in params.items() if v is not None}\n    if mode == \"trial\":\n        params[\"K_short\"] = min(int(params.get(\"K_short\", 1)), 1)\n        params[\"K_long\"] = min(int(params.get(\"K_long\", 2)), 2)\n    return params\n\n\ndef _sample_hparams(trial: optuna.Trial, search_spaces: List[Dict[str, Any]]) -> Dict[str, Any]:\n    params: Dict[str, Any] = {}\n    for space in search_spaces:\n        name = space[\"param_name\"]\n        dist = space.get(\"distribution_type\", \"categorical\")\n        if dist == \"categorical\":\n            params[name] = trial.suggest_categorical(name, list(space[\"choices\"]))\n        elif dist == \"int\":\n            params[name] = trial.suggest_int(name, int(space[\"low\"]), int(space[\"high\"]))\n        elif dist == \"float\":\n            params[name] = trial.suggest_float(name, float(space[\"low\"]), float(space[\"high\"]))\n        else:\n            raise ValueError(f\"Unsupported distribution type: {dist}\")\n    return params\n\n\ndef _assert_gradients(model: torch.nn.Module) -> None:\n    has_grad = False\n    total_norm = 0.0\n    for param in model.parameters():\n        if not param.requires_grad:\n            continue\n        if param.grad is None:\n            continue\n        has_grad = True\n        total_norm += param.grad.detach().abs().sum().item()\n    assert has_grad, \"No gradients found before optimizer step.\"\n    assert total_norm > 0.0, \"Gradients are zero before optimizer step.\"\n\n\ndef _build_optimizer(model: torch.nn.Module, train_cfg: DictConfig) -> Optional[torch.optim.Optimizer]:\n    opt_name = str(getattr(train_cfg, \"optimizer\", \"none\")).lower()\n    if opt_name in {\"none\", \"null\", \"disabled\"}:\n        return None\n    lr = float(getattr(train_cfg, \"learning_rate\", 0.0))\n    if lr <= 0:\n        raise ValueError(\"learning_rate must be > 0 when optimizer is enabled.\")\n    weight_decay = float(getattr(train_cfg, \"weight_decay\", 0.0))\n    beta1 = float(getattr(train_cfg, \"beta1\", 0.9))\n    beta2 = float(getattr(train_cfg, \"beta2\", 0.999))\n    if opt_name in {\"adamw\", \"adam_w\"}:\n        return torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n    if opt_name in {\"adam\"}:\n        return torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(beta1, beta2))\n    if opt_name in {\"sgd\"}:\n        momentum = float(getattr(train_cfg, \"momentum\", 0.0))\n        return torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n    raise ValueError(f\"Unsupported optimizer: {opt_name}\")\n\n\ndef _train_model(\n    model: torch.nn.Module,\n    tokenizer,\n    train_examples: List[Dict[str, str]],\n    cfg: DictConfig,\n    device: str,\n    wandb_run: Optional[wandb.sdk.wandb_run.Run] = None,\n) -> None:\n    run_cfg = cfg.run\n    train_cfg = run_cfg.training\n    epochs = int(getattr(train_cfg, \"epochs\", 0))\n    if epochs <= 0:\n        return\n    max_length = int(run_cfg.dataset.preprocessing.max_length)\n    dataset = QADataset(train_examples, tokenizer, max_length=max_length)\n    if len(dataset) == 0:\n        return\n    loader = DataLoader(dataset, batch_size=int(train_cfg.batch_size), shuffle=True)\n    optimizer = _build_optimizer(model, train_cfg)\n    if optimizer is None:\n        raise ValueError(\"Training enabled but optimizer is set to none.\")\n    total_steps = max(1, epochs * max(len(loader), 1))\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=int(getattr(train_cfg, \"warmup_steps\", 0)), num_training_steps=total_steps\n    )\n    model.train()\n    step = 0\n    for epoch in range(epochs):\n        for batch in loader:\n            if step == 0:\n                assert batch[\"input_ids\"].shape[0] == batch[\"labels\"].shape[0], \"Batch sizes mismatch.\"\n                assert batch[\"input_ids\"].ndim == 2 and batch[\"labels\"].ndim == 2, \"Unexpected batch dims.\"\n            if cfg.mode == \"trial\" and step >= 2:\n                break\n            batch = {k: v.to(device) for k, v in batch.items()}\n            labels = batch.pop(\"labels\")\n            outputs = model(**batch, labels=labels)\n            loss = outputs.loss\n            assert loss is not None, \"Training loss is None.\"\n            params = [p for p in model.parameters() if p.requires_grad]\n            aux_grads = torch.autograd.grad(\n                loss, params, retain_graph=True, create_graph=False, allow_unused=True\n            )\n            aux_norm = 0.0\n            for grad in aux_grads:\n                if grad is not None:\n                    aux_norm += grad.detach().float().pow(2).sum().item()\n            aux_norm = math.sqrt(aux_norm) if aux_norm > 0 else 0.0\n            loss.backward()\n            _assert_gradients(model)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad(set_to_none=True)\n            if wandb_run is not None:\n                wandb.log(\n                    {\n                        \"train_loss\": float(loss.item()),\n                        \"train_aux_grad_norm\": float(aux_norm),\n                        \"train_epoch\": int(epoch),\n                        \"train_lr\": float(optimizer.param_groups[0][\"lr\"]),\n                    },\n                    step=step,\n                )\n            step += 1\n        if cfg.mode == \"trial\" and step >= 2:\n            break\n    model.eval()\n\n\ndef _run_inference(\n    method,\n    dataset: List[Dict[str, str]],\n    tokenizer,\n    cfg: DictConfig,\n    device: str,\n    wandb_run: Optional[wandb.sdk.wandb_run.Run] = None,\n) -> Dict[str, float]:\n    total = 0\n    correct = 0\n    total_tokens = 0\n    stage2_count = 0\n    cc_correct: List[float] = []\n    cc_incorrect: List[float] = []\n    processed = 0\n    step = 0\n    max_examples = 2 if cfg.mode == \"trial\" else None\n    for idx, ex in enumerate(dataset):\n        if max_examples is not None and idx >= max_examples:\n            break\n        question = ex[\"question\"]\n        gold = extract_answer(ex[\"answer\"])\n        if idx == 0 and gold is not None:\n            prompt = method.build_prompt(question, verify=False) if hasattr(method, \"build_prompt\") else question\n            enc = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n            dec = tokenizer(f\"The answer is {gold}.\", return_tensors=\"pt\", truncation=True)\n            assert enc.input_ids.shape[0] == dec.input_ids.shape[0] == 1, \"Batch size mismatch.\"\n            assert enc.input_ids.ndim == 2 and dec.input_ids.ndim == 2, \"Unexpected input dims.\"\n        result = method.predict(question)\n        pred = result.get(\"pred_answer\")\n        is_correct = 0\n        if pred is not None and gold is not None:\n            total += 1\n            is_correct = int(pred == gold)\n            correct += is_correct\n        total_tokens += int(result.get(\"used_tokens\", 0))\n        stage2_count += int(result.get(\"stage2_used\", 0))\n        cc_vals = result.get(\"cc_values\", [])\n        if pred is not None and gold is not None:\n            if is_correct:\n                cc_correct.extend(cc_vals)\n            else:\n                cc_incorrect.extend(cc_vals)\n        cc_mean = float(np.mean(cc_vals)) if len(cc_vals) > 0 else 0.0\n        processed += 1\n        cumulative_accuracy = correct / max(total, 1)\n        if wandb_run is not None:\n            pred_str = str(pred) if pred is not None else \"\"\n            gold_str = str(gold) if gold is not None else \"\"\n            wandb.log(\n                {\n                    \"example_correct\": int(is_correct),\n                    \"example_tokens\": int(result.get(\"used_tokens\", 0)),\n                    \"stage2_used\": int(result.get(\"stage2_used\", 0)),\n                    \"example_cc_mean\": float(cc_mean),\n                    \"stage1_top1_prob\": float(result.get(\"stage1_top1_prob\", 0.0)),\n                    \"stage1_margin\": float(result.get(\"stage1_margin\", 0.0)),\n                    \"final_top1_prob\": float(result.get(\"final_top1_prob\", 0.0)),\n                    \"example_pred\": pred_str,\n                    \"example_gold\": gold_str,\n                    \"cumulative_accuracy\": float(cumulative_accuracy),\n                    \"example_idx\": int(idx),\n                },\n                step=step,\n            )\n        step += 1\n    accuracy = correct / max(total, 1)\n    avg_tokens = total_tokens / max(processed, 1)\n    stage2_rate = stage2_count / max(processed, 1)\n    cc_correct_mean = float(np.mean(cc_correct)) if len(cc_correct) > 0 else 0.0\n    cc_incorrect_mean = float(np.mean(cc_incorrect)) if len(cc_incorrect) > 0 else 0.0\n    cc_separation = cc_correct_mean - cc_incorrect_mean\n    return {\n        \"accuracy\": float(accuracy),\n        \"avg_generated_tokens\": float(avg_tokens),\n        \"stage2_rate\": float(stage2_rate),\n        \"cc_separation\": float(cc_separation),\n        \"cc_correct_mean\": float(cc_correct_mean),\n        \"cc_incorrect_mean\": float(cc_incorrect_mean),\n        \"n_eval\": int(total),\n    }\n\n\ndef _run_optuna(\n    cfg: DictConfig,\n    run_cfg: DictConfig,\n    model: torch.nn.Module,\n    tokenizer,\n    val_dataset: List[Dict[str, str]],\n    base_params: Dict[str, Any],\n    device: str,\n) -> Dict[str, Any]:\n    optuna_cfg = _get_optuna_cfg(run_cfg)\n    if optuna_cfg is None:\n        return {}\n    if int(getattr(optuna_cfg, \"n_trials\", 0)) <= 0:\n        return {}\n    if len(val_dataset) == 0:\n        return {}\n\n    def objective(trial: optuna.Trial) -> float:\n        set_seed(int(cfg.seed) + int(trial.number))\n        trial_params = deepcopy(base_params)\n        trial_params.update(_sample_hparams(trial, list(optuna_cfg.search_spaces)))\n        method = build_method(run_cfg.method, model, tokenizer, run_cfg, trial_params, device)\n        metrics = _run_inference(method, val_dataset, tokenizer, cfg, device, wandb_run=None)\n        return float(metrics[\"accuracy\"])\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=int(optuna_cfg.n_trials))\n    return study.best_params\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -> None:\n    OmegaConf.set_struct(cfg, False)\n    cfg = _apply_mode_overrides(cfg)\n    set_seed(int(cfg.seed))\n\n    run_cfg = cfg.run\n    run_id = str(getattr(run_cfg, \"run_id\", \"\"))\n    if not run_id:\n        raise ValueError(\"run_id is missing in the run configuration.\")\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, tokenizer = create_model_and_tokenizer(run_cfg.model, device)\n    if tokenizer.pad_token_id is None:\n        raise ValueError(\"Tokenizer pad_token_id is missing after initialization.\")\n    output_embeddings = model.get_output_embeddings()\n    assert output_embeddings is not None, \"Model output embeddings are missing.\"\n    assert output_embeddings.weight.shape[0] == model.config.vocab_size, \"Output dimension mismatch.\"\n\n    dataset = load_dataset_split(run_cfg.dataset)\n    if len(dataset) == 0:\n        raise ValueError(\"Loaded dataset is empty.\")\n\n    method_params = _resolve_method_params(run_cfg, cfg.mode)\n    run_cfg.method_params = method_params\n\n    val_dataset: List[Dict[str, str]] = []\n    if hasattr(run_cfg, \"optuna\") and int(getattr(run_cfg.optuna, \"n_trials\", 0)) > 0:\n        try:\n            val_dataset = load_dataset_split(run_cfg.dataset, split_override=\"train\", subset_size=50, seed=cfg.seed)\n        except Exception:\n            val_dataset = dataset[: min(50, len(dataset))]\n\n    best_params = _run_optuna(cfg, run_cfg, model, tokenizer, val_dataset, method_params, device)\n    if best_params:\n        method_params.update(best_params)\n        run_cfg.method_params = method_params\n\n    wandb_run: Optional[wandb.sdk.wandb_run.Run] = None\n    if cfg.wandb.mode != \"disabled\":\n        wandb_run = wandb.init(\n            entity=cfg.wandb.entity,\n            project=cfg.wandb.project,\n            id=run_id,\n            name=run_id,\n            config=OmegaConf.to_container(cfg, resolve=True),\n            resume=\"allow\",\n        )\n        if method_params:\n            wandb_run.config.update(method_params, allow_val_change=True)\n\n    if getattr(run_cfg.training, \"enabled\", False):\n        try:\n            train_examples = load_dataset_split(run_cfg.dataset, split_override=\"train\")\n        except Exception:\n            train_examples = dataset\n        _train_model(model, tokenizer, train_examples, cfg, device, wandb_run=wandb_run)\n\n    method = build_method(run_cfg.method, model, tokenizer, run_cfg, method_params, device)\n    metrics = _run_inference(method, dataset, tokenizer, cfg, device, wandb_run=wandb_run)\n\n    if wandb_run is not None:\n        for key, value in metrics.items():\n            wandb_run.summary[key] = value\n        print(f\"WandB URL: {wandb_run.url}\")\n        wandb_run.finish()\n    else:\n        print(\"WandB logging disabled; run completed.\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "evaluate_py": "import argparse\nimport json\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, Dict, List, Tuple\n\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport wandb\nfrom omegaconf import OmegaConf\nfrom scipy import stats\n\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\n\nMETRIC_DIRECTIONS = {\n    \"accuracy\": \"max\",\n    \"avg_generated_tokens\": \"min\",\n    \"stage2_rate\": \"min\",\n    \"cc_separation\": \"max\",\n}\n\n\ndef _load_wandb_config() -> Dict[str, str]:\n    cfg_path = Path(__file__).resolve().parents[1] / \"config\" / \"config.yaml\"\n    cfg = OmegaConf.load(cfg_path)\n    return {\n        \"entity\": cfg.wandb.entity,\n        \"project\": cfg.wandb.project,\n    }\n\n\ndef _to_jsonable(obj: Any) -> Any:\n    if obj is None:\n        return None\n    if isinstance(obj, (str, int, float, bool)):\n        return obj\n    if isinstance(obj, (np.integer, np.floating, np.bool_)):\n        return obj.item()\n    if isinstance(obj, np.ndarray):\n        return obj.tolist()\n    if isinstance(obj, pd.Timestamp):\n        return obj.isoformat()\n    if isinstance(obj, pd.Series):\n        return [_to_jsonable(v) for v in obj.tolist()]\n    if isinstance(obj, pd.DataFrame):\n        return [_to_jsonable(rec) for rec in obj.to_dict(orient=\"records\")]\n    if isinstance(obj, dict):\n        return {str(k): _to_jsonable(v) for k, v in obj.items()}\n    if isinstance(obj, (list, tuple)):\n        return [_to_jsonable(v) for v in obj]\n    return str(obj)\n\n\ndef _save_json(path: str, payload: Dict) -> None:\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(_to_jsonable(payload), f, indent=2)\n\n\ndef _bootstrap_ci(data: np.ndarray, n_boot: int = 1000, alpha: float = 0.05) -> Tuple[float, float]:\n    if len(data) == 0:\n        return (0.0, 0.0)\n    rng = np.random.default_rng(0)\n    samples = [rng.choice(data, size=len(data), replace=True).mean() for _ in range(n_boot)]\n    lower = float(np.percentile(samples, 100 * (alpha / 2)))\n    upper = float(np.percentile(samples, 100 * (1 - alpha / 2)))\n    return (lower, upper)\n\n\ndef _metric_direction(metric: str) -> str:\n    if metric in METRIC_DIRECTIONS:\n        return METRIC_DIRECTIONS[metric]\n    lowered = metric.lower()\n    if any(key in lowered for key in [\"loss\", \"perplexity\", \"error\"]):\n        return \"min\"\n    return \"max\"\n\n\ndef _assert_run_is_full(config: Dict, run_id: str) -> None:\n    mode = str(config.get(\"mode\", \"full\")).lower()\n    wandb_cfg = config.get(\"wandb\", {})\n    wandb_mode = None\n    if isinstance(wandb_cfg, dict):\n        wandb_mode = str(wandb_cfg.get(\"mode\", \"\")).lower()\n    if mode == \"trial\" or wandb_mode == \"disabled\":\n        raise RuntimeError(\n            f\"Run {run_id} appears to be a trial run with WandB disabled. \"\n            \"Please evaluate only full runs with WandB logging enabled.\"\n        )\n\n\ndef _plot_learning_curve(history: pd.DataFrame, run_id: str, out_dir: str) -> List[str]:\n    paths: List[str] = []\n    if \"cumulative_accuracy\" in history.columns:\n        y = history[\"cumulative_accuracy\"].dropna().values\n    elif \"example_correct\" in history.columns:\n        y = history[\"example_correct\"].dropna().expanding().mean().values\n    else:\n        return paths\n    x = np.arange(len(y))\n    plt.figure(figsize=(6, 4))\n    plt.plot(x, y, label=\"Cumulative Accuracy\")\n    if len(y) > 0:\n        plt.scatter([x[-1]], [y[-1]], color=\"red\", label=f\"Final={y[-1]:.3f}\")\n    plt.xlabel(\"Step\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(f\"Learning Curve: {run_id}\")\n    plt.legend()\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_learning_curve.pdf\")\n    plt.savefig(path)\n    plt.close()\n    paths.append(path)\n    return paths\n\n\ndef _plot_tokens_distribution(history: pd.DataFrame, run_id: str, out_dir: str) -> List[str]:\n    paths: List[str] = []\n    if \"example_tokens\" not in history.columns:\n        return paths\n    data = history[\"example_tokens\"].dropna()\n    if len(data) == 0:\n        return paths\n    plt.figure(figsize=(6, 4))\n    sns.boxplot(x=data)\n    plt.xlabel(\"Generated Tokens\")\n    plt.title(f\"Tokens per Example: {run_id}\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_tokens_boxplot.pdf\")\n    plt.savefig(path)\n    plt.close()\n    paths.append(path)\n    return paths\n\n\ndef _plot_tokens_scatter(history: pd.DataFrame, run_id: str, out_dir: str) -> List[str]:\n    paths: List[str] = []\n    if \"example_tokens\" not in history.columns or \"example_correct\" not in history.columns:\n        return paths\n    df = history[[\"example_tokens\", \"example_correct\"]].dropna()\n    if len(df) == 0:\n        return paths\n    plt.figure(figsize=(6, 4))\n    sns.stripplot(x=\"example_correct\", y=\"example_tokens\", data=df, jitter=0.2)\n    plt.xlabel(\"Correct (1) vs Incorrect (0)\")\n    plt.ylabel(\"Generated Tokens\")\n    plt.title(f\"Tokens vs Correctness: {run_id}\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_tokens_scatter.pdf\")\n    plt.savefig(path)\n    plt.close()\n    paths.append(path)\n    return paths\n\n\ndef _plot_cc_distribution(history: pd.DataFrame, run_id: str, out_dir: str) -> List[str]:\n    paths: List[str] = []\n    if \"example_cc_mean\" not in history.columns or \"example_correct\" not in history.columns:\n        return paths\n    df = history[[\"example_cc_mean\", \"example_correct\"]].dropna()\n    if len(df) == 0:\n        return paths\n    plt.figure(figsize=(6, 4))\n    sns.histplot(\n        data=df,\n        x=\"example_cc_mean\",\n        hue=\"example_correct\",\n        stat=\"density\",\n        common_norm=False,\n        bins=20,\n    )\n    plt.xlabel(\"CC Mean\")\n    plt.title(f\"CC Distribution (Correct vs Incorrect): {run_id}\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_cc_distribution.pdf\")\n    plt.savefig(path)\n    plt.close()\n    paths.append(path)\n    return paths\n\n\ndef _plot_confusion_matrix(history: pd.DataFrame, run_id: str, out_dir: str, top_k: int = 10) -> List[str]:\n    paths: List[str] = []\n    if \"example_pred\" not in history.columns or \"example_gold\" not in history.columns:\n        return paths\n    df = history[[\"example_pred\", \"example_gold\"]].dropna()\n    if len(df) == 0:\n        return paths\n    df = df.astype(str)\n    top_gold = df[\"example_gold\"].value_counts().head(top_k).index.tolist()\n    top_pred = df[\"example_pred\"].value_counts().head(top_k).index.tolist()\n    labels = sorted(set(top_gold + top_pred))\n\n    def map_label(val: str) -> str:\n        return val if val in labels else \"Other\"\n\n    df_mapped = pd.DataFrame(\n        {\n            \"gold\": df[\"example_gold\"].map(map_label),\n            \"pred\": df[\"example_pred\"].map(map_label),\n        }\n    )\n    matrix = pd.crosstab(df_mapped[\"gold\"], df_mapped[\"pred\"], dropna=False)\n    plt.figure(figsize=(7, 6))\n    sns.heatmap(matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"Gold\")\n    plt.title(f\"Confusion Matrix (Top {top_k}) : {run_id}\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"{run_id}_confusion_matrix_top{top_k}.pdf\")\n    plt.savefig(path)\n    plt.close()\n    paths.append(path)\n    return paths\n\n\ndef _plot_comparison_bars(metric: str, metrics: Dict[str, float], out_dir: str) -> str:\n    plt.figure(figsize=(7, 4))\n    run_ids = list(metrics.keys())\n    values = [metrics[r] for r in run_ids]\n    ax = sns.barplot(x=run_ids, y=values)\n    ax.set_xlabel(\"Run ID\")\n    ax.set_ylabel(metric)\n    ax.set_title(f\"Comparison: {metric}\")\n    for i, v in enumerate(values):\n        ax.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, f\"comparison_{metric}_bar_chart.pdf\")\n    plt.savefig(path)\n    plt.close()\n    return path\n\n\ndef _plot_metrics_table(metrics_by_name: Dict[str, Dict[str, float]], out_dir: str) -> str:\n    df = pd.DataFrame(metrics_by_name).T\n    plt.figure(figsize=(8, 2 + 0.3 * len(df)))\n    plt.axis(\"off\")\n    table = plt.table(\n        cellText=np.round(df.values, 4),\n        rowLabels=df.index,\n        colLabels=df.columns,\n        cellLoc=\"center\",\n        loc=\"center\",\n    )\n    table.scale(1, 1.2)\n    plt.title(\"Performance Metrics Table\", pad=12)\n    plt.tight_layout()\n    path = os.path.join(out_dir, \"comparison_metrics_table.pdf\")\n    plt.savefig(path)\n    plt.close()\n    return path\n\n\ndef _plot_accuracy_ci(metrics_by_name: Dict[str, Dict[str, float]], ci_by_run: Dict[str, Tuple[float, float]], out_dir: str) -> str:\n    run_ids = list(metrics_by_name[\"accuracy\"].keys())\n    values = [metrics_by_name[\"accuracy\"][r] for r in run_ids]\n    ci = [ci_by_run.get(r, (0.0, 0.0)) for r in run_ids]\n    lower_err = [v - c[0] for v, c in zip(values, ci)]\n    upper_err = [c[1] - v for v, c in zip(values, ci)]\n    plt.figure(figsize=(7, 4))\n    plt.bar(run_ids, values, yerr=[lower_err, upper_err], capsize=4)\n    for i, v in enumerate(values):\n        plt.text(i, v, f\"{v:.3f}\", ha=\"center\", va=\"bottom\")\n    plt.ylabel(\"Accuracy\")\n    plt.title(\"Accuracy with 95% CI\")\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, \"comparison_accuracy_ci.pdf\")\n    plt.savefig(path)\n    plt.close()\n    return path\n\n\ndef _plot_tokens_boxplot(histories: Dict[str, pd.DataFrame], out_dir: str) -> str:\n    rows = []\n    for run_id, history in histories.items():\n        if \"example_tokens\" not in history.columns:\n            continue\n        data = history[\"example_tokens\"].dropna()\n        for val in data:\n            rows.append({\"run_id\": run_id, \"example_tokens\": val})\n    df = pd.DataFrame(rows)\n    plt.figure(figsize=(7, 4))\n    if len(df) > 0:\n        sns.boxplot(x=\"run_id\", y=\"example_tokens\", data=df)\n    plt.xticks(rotation=45, ha=\"right\")\n    plt.ylabel(\"Generated Tokens\")\n    plt.title(\"Tokens per Example (Across Runs)\")\n    plt.tight_layout()\n    path = os.path.join(out_dir, \"comparison_tokens_boxplot.pdf\")\n    plt.savefig(path)\n    plt.close()\n    return path\n\n\ndef _parse_cli() -> Tuple[str, str]:\n    parser = argparse.ArgumentParser(add_help=True)\n    parser.add_argument(\"--results_dir\", type=str, default=None)\n    parser.add_argument(\"--run_ids\", type=str, default=None)\n    known, unknown = parser.parse_known_args()\n\n    kv_args: Dict[str, str] = {}\n    for arg in unknown:\n        if \"=\" in arg:\n            key, value = arg.split(\"=\", 1)\n            kv_args[key.lstrip(\"-\")] = value\n\n    results_dir = known.results_dir or kv_args.get(\"results_dir\")\n    run_ids = known.run_ids or kv_args.get(\"run_ids\")\n\n    if results_dir is None or run_ids is None:\n        raise ValueError(\n            \"Missing required arguments. Use: python -m src.evaluate results_dir=... run_ids='[...]'\"\n        )\n    return results_dir, run_ids\n\n\ndef _parse_run_ids(run_ids_str: str) -> List[str]:\n    try:\n        run_ids = json.loads(run_ids_str)\n    except json.JSONDecodeError as exc:\n        raise ValueError(f\"Failed to parse run_ids from: {run_ids_str}\") from exc\n    if not isinstance(run_ids, list):\n        raise ValueError(f\"run_ids must be a JSON list; got: {run_ids_str}\")\n    return [str(r) for r in run_ids]\n\n\ndef main() -> None:\n    results_dir, run_ids_str = _parse_cli()\n\n    cfg = _load_wandb_config()\n    entity = cfg[\"entity\"]\n    project = cfg[\"project\"]\n\n    api = wandb.Api()\n    run_ids = _parse_run_ids(run_ids_str)\n    histories: Dict[str, pd.DataFrame] = {}\n    summaries: Dict[str, Dict] = {}\n    configs: Dict[str, Dict] = {}\n\n    generated_paths: List[str] = []\n\n    for run_id in run_ids:\n        try:\n            run = api.run(f\"{entity}/{project}/{run_id}\")\n        except wandb.errors.CommError as exc:\n            raise RuntimeError(\n                f\"Failed to fetch run {run_id}. Ensure it was logged in full mode with WandB enabled.\"\n            ) from exc\n        history = run.history()\n        history = history.replace({np.nan: None})\n        summary = run.summary._json_dict\n        config = dict(run.config)\n        _assert_run_is_full(config, run_id)\n        histories[run_id] = history\n        summaries[run_id] = summary\n        configs[run_id] = config\n\n        run_dir = os.path.join(results_dir, run_id)\n        os.makedirs(run_dir, exist_ok=True)\n\n        metrics_payload = {\n            \"run_id\": run_id,\n            \"summary\": summary,\n            \"config\": config,\n            \"history\": history.to_dict(orient=\"records\"),\n        }\n        metrics_path = os.path.join(run_dir, \"metrics.json\")\n        _save_json(metrics_path, metrics_payload)\n        generated_paths.append(metrics_path)\n\n        generated_paths.extend(_plot_learning_curve(history, run_id, run_dir))\n        generated_paths.extend(_plot_tokens_distribution(history, run_id, run_dir))\n        generated_paths.extend(_plot_tokens_scatter(history, run_id, run_dir))\n        generated_paths.extend(_plot_cc_distribution(history, run_id, run_dir))\n        generated_paths.extend(_plot_confusion_matrix(history, run_id, run_dir))\n\n    metrics_by_name: Dict[str, Dict[str, float]] = defaultdict(dict)\n    ci_by_run: Dict[str, Tuple[float, float]] = {}\n    for run_id in run_ids:\n        summary = summaries[run_id]\n        history = histories[run_id]\n        accuracy = summary.get(\"accuracy\")\n        if accuracy is None and \"example_correct\" in history.columns:\n            accuracy = float(history[\"example_correct\"].dropna().mean())\n        avg_tokens = summary.get(\"avg_generated_tokens\")\n        if avg_tokens is None and \"example_tokens\" in history.columns:\n            avg_tokens = float(history[\"example_tokens\"].dropna().mean())\n        stage2_rate = summary.get(\"stage2_rate\")\n        if stage2_rate is None and \"stage2_used\" in history.columns:\n            stage2_rate = float(history[\"stage2_used\"].dropna().mean())\n        cc_sep = summary.get(\"cc_separation\")\n        if cc_sep is None and \"example_cc_mean\" in history.columns and \"example_correct\" in history.columns:\n            df = history[[\"example_cc_mean\", \"example_correct\"]].dropna()\n            cc_correct = df[df[\"example_correct\"] > 0][\"example_cc_mean\"].mean()\n            cc_incorrect = df[df[\"example_correct\"] == 0][\"example_cc_mean\"].mean()\n            cc_sep = float(cc_correct - cc_incorrect)\n\n        metrics_by_name[\"accuracy\"][run_id] = float(accuracy) if accuracy is not None else 0.0\n        metrics_by_name[\"avg_generated_tokens\"][run_id] = float(avg_tokens) if avg_tokens is not None else 0.0\n        metrics_by_name[\"stage2_rate\"][run_id] = float(stage2_rate) if stage2_rate is not None else 0.0\n        metrics_by_name[\"cc_separation\"][run_id] = float(cc_sep) if cc_sep is not None else 0.0\n\n        if \"example_correct\" in history.columns:\n            vals = history[\"example_correct\"].dropna().values\n            ci_by_run[run_id] = _bootstrap_ci(vals)\n\n    primary_metric = \"accuracy\"\n    direction = _metric_direction(primary_metric)\n    proposed_runs = {k: v for k, v in metrics_by_name[primary_metric].items() if \"proposed\" in k}\n    baseline_runs = {\n        k: v\n        for k, v in metrics_by_name[primary_metric].items()\n        if \"baseline\" in k or \"comparative\" in k\n    }\n    if direction == \"min\":\n        best_proposed = min(proposed_runs.items(), key=lambda x: x[1]) if proposed_runs else (None, None)\n        best_baseline = min(baseline_runs.items(), key=lambda x: x[1]) if baseline_runs else (None, None)\n    else:\n        best_proposed = max(proposed_runs.items(), key=lambda x: x[1]) if proposed_runs else (None, None)\n        best_baseline = max(baseline_runs.items(), key=lambda x: x[1]) if baseline_runs else (None, None)\n\n    gap = None\n    if best_proposed[0] and best_baseline[0] and best_baseline[1] != 0:\n        if direction == \"min\":\n            gap = (best_baseline[1] - best_proposed[1]) / abs(best_baseline[1]) * 100\n        else:\n            gap = (best_proposed[1] - best_baseline[1]) / abs(best_baseline[1]) * 100\n\n    aggregated_metrics = {\n        \"primary_metric\": primary_metric,\n        \"metrics\": metrics_by_name,\n        \"best_proposed\": {\"run_id\": best_proposed[0], \"value\": best_proposed[1]},\n        \"best_baseline\": {\"run_id\": best_baseline[0], \"value\": best_baseline[1]},\n        \"gap\": gap,\n    }\n\n    if best_proposed[0] and best_baseline[0]:\n        prop_hist = histories[best_proposed[0]]\n        base_hist = histories[best_baseline[0]]\n        stat_tests = {}\n        if \"example_correct\" in prop_hist.columns and \"example_correct\" in base_hist.columns:\n            prop = prop_hist[\"example_correct\"].dropna().values\n            base = base_hist[\"example_correct\"].dropna().values\n            if len(prop) > 1 and len(base) > 1:\n                t_stat, p_val = stats.ttest_ind(prop, base, equal_var=False)\n                stat_tests[\"t_test_accuracy\"] = {\"t_stat\": float(t_stat), \"p_value\": float(p_val)}\n        if \"example_tokens\" in prop_hist.columns and \"example_tokens\" in base_hist.columns:\n            prop = prop_hist[\"example_tokens\"].dropna().values\n            base = base_hist[\"example_tokens\"].dropna().values\n            if len(prop) > 1 and len(base) > 1:\n                t_stat, p_val = stats.ttest_ind(prop, base, equal_var=False)\n                stat_tests[\"t_test_tokens\"] = {\"t_stat\": float(t_stat), \"p_value\": float(p_val)}\n        if stat_tests:\n            aggregated_metrics[\"stat_tests\"] = stat_tests\n\n    comparison_dir = os.path.join(results_dir, \"comparison\")\n    os.makedirs(comparison_dir, exist_ok=True)\n\n    agg_path = os.path.join(comparison_dir, \"aggregated_metrics.json\")\n    _save_json(agg_path, aggregated_metrics)\n    generated_paths.append(agg_path)\n\n    for metric_name, values in metrics_by_name.items():\n        generated_paths.append(_plot_comparison_bars(metric_name, values, comparison_dir))\n\n    if \"accuracy\" in metrics_by_name:\n        generated_paths.append(_plot_accuracy_ci(metrics_by_name, ci_by_run, comparison_dir))\n\n    generated_paths.append(_plot_metrics_table(metrics_by_name, comparison_dir))\n    generated_paths.append(_plot_tokens_boxplot(histories, comparison_dir))\n\n    for path in generated_paths:\n        print(path)\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "preprocess_py": "import random\nfrom typing import Dict, List, Optional\n\nimport numpy as np\nimport torch\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\n\nfrom src.model import extract_answer\n\n\ndef set_seed(seed: int) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef _resolve_dataset_name(name: str) -> str:\n    lname = name.lower()\n    if lname in {\"openai/gsm8k\", \"gsm8k\"}:\n        return \"gsm8k\"\n    if \"svamp\" in lname:\n        return \"svamp\"\n    if \"asdiv\" in lname:\n        return \"asdiv\"\n    return name\n\n\ndef _extract_qa(name: str, ex: Dict) -> Optional[Dict[str, str]]:\n    lname = name.lower()\n    if \"gsm8k\" in lname:\n        q = ex.get(\"question\")\n        a = ex.get(\"answer\")\n        return {\"question\": str(q).strip(), \"answer\": str(a).strip()} if q and a else None\n    if \"svamp\" in lname:\n        body = ex.get(\"Body\") or ex.get(\"body\") or \"\"\n        question = ex.get(\"Question\") or ex.get(\"question\") or \"\"\n        q = f\"{body} {question}\".strip()\n        a = ex.get(\"Answer\") or ex.get(\"answer\")\n        return {\"question\": q, \"answer\": str(a).strip()} if q and a else None\n    if \"asdiv\" in lname:\n        q = ex.get(\"question\") or ex.get(\"problem\") or ex.get(\"Question\") or \"\"\n        a = ex.get(\"answer\") or ex.get(\"Answer\") or ex.get(\"solution\") or \"\"\n        return {\"question\": str(q).strip(), \"answer\": str(a).strip()} if q and a else None\n    q = ex.get(\"question\") or ex.get(\"prompt\") or ex.get(\"input\")\n    a = ex.get(\"answer\") or ex.get(\"output\") or ex.get(\"label\")\n    if q is None or a is None:\n        return None\n    return {\"question\": str(q).strip(), \"answer\": str(a).strip()}\n\n\ndef load_dataset_split(\n    dataset_cfg,\n    split_override: Optional[str] = None,\n    subset_size: Optional[int] = None,\n    seed: Optional[int] = None,\n) -> List[Dict[str, str]]:\n    name = _resolve_dataset_name(dataset_cfg.name)\n    subset = getattr(dataset_cfg, \"subset\", None)\n    split = split_override or dataset_cfg.split\n    try:\n        ds = load_dataset(name, subset, split=split, cache_dir=\".cache/\")\n    except Exception as exc:\n        raise RuntimeError(f\"Failed to load dataset {name} ({subset}) split={split}: {exc}\") from exc\n    if getattr(dataset_cfg, \"shuffle\", False):\n        ds = ds.shuffle(seed=seed if seed is not None else int(getattr(dataset_cfg, \"shuffle_seed\", 0)))\n    if subset_size is None:\n        subset_size = getattr(dataset_cfg, \"subset_size\", None)\n    if subset_size:\n        ds = ds.select(range(min(int(subset_size), len(ds))))\n    examples: List[Dict[str, str]] = []\n    for ex in ds:\n        parsed = _extract_qa(name, ex)\n        if parsed is None:\n            continue\n        examples.append(parsed)\n    return examples\n\n\nclass QADataset(Dataset):\n    def __init__(self, examples: List[Dict[str, str]], tokenizer, max_length: int):\n        self.examples = []\n        for ex in examples:\n            gold = extract_answer(ex[\"answer\"])\n            if gold is None:\n                continue\n            self.examples.append({\"question\": ex[\"question\"], \"answer\": gold})\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self) -> int:\n        return len(self.examples)\n\n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        ex = self.examples[idx]\n        question = ex[\"question\"]\n        gold = ex[\"answer\"]\n        input_text = f\"Q: {question}\\nA:\"\n        target_text = f\"The answer is {gold}.\"\n        model_inputs = self.tokenizer(\n            input_text,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        )\n        labels = self.tokenizer(\n            target_text,\n            truncation=True,\n            max_length=self.max_length,\n            padding=\"max_length\",\n            return_tensors=\"pt\",\n        ).input_ids\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\n            \"input_ids\": model_inputs.input_ids.squeeze(0),\n            \"attention_mask\": model_inputs.attention_mask.squeeze(0),\n            \"labels\": labels.squeeze(0),\n        }\n",
    "model_py": "import math\nimport re\nfrom collections import defaultdict\nfrom typing import Dict, List, Optional, Tuple\n\nimport torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nANS_RE = re.compile(r\"The answer is\\s*(-?\\d+\\.?\\d*)\", re.IGNORECASE)\nNUM_RE = re.compile(r\"-?\\d+\\.?\\d*\")\n\n\ndef normalize_answer(ans: Optional[str]) -> Optional[str]:\n    if ans is None:\n        return None\n    ans = str(ans).strip().replace(\",\", \"\")\n    ans = ans.lstrip(\"+\")\n    if ans.endswith(\".\"):\n        ans = ans[:-1]\n    return ans if ans else None\n\n\ndef extract_answer(text: str) -> Optional[str]:\n    if text is None:\n        return None\n    match = ANS_RE.search(str(text))\n    if match:\n        return normalize_answer(match.group(1))\n    nums = NUM_RE.findall(str(text).replace(\",\", \"\"))\n    if not nums:\n        return None\n    return normalize_answer(nums[-1])\n\n\ndef split_rationale_answer(text: str) -> Tuple[str, str]:\n    if text is None:\n        return \"\", \"\"\n    lower = text.lower()\n    idx = lower.rfind(\"the answer is\")\n    if idx == -1:\n        return text.strip(), \"\"\n    return text[:idx].strip(), text[idx:].strip()\n\n\ndef chunk_rationale(rationale: str, max_chunks: int = 6) -> List[str]:\n    if not rationale:\n        return []\n    parts = [p.strip() for p in rationale.split(\"\\n\") if p.strip()]\n    if len(parts) <= 1:\n        parts = [p.strip() for p in re.split(r\"(?<=[\\.!\\?])\\s+\", rationale) if p.strip()]\n    if not parts:\n        return []\n    if len(parts) <= max_chunks:\n        return parts\n    step = math.ceil(len(parts) / max_chunks)\n    merged = []\n    for i in range(0, len(parts), step):\n        merged.append(\" \".join(parts[i : i + step]))\n    return merged[:max_chunks]\n\n\ndef mask_numbers(rationale: str) -> str:\n    if rationale is None:\n        return \"\"\n    masked = re.sub(r\"-?\\d+\\.?\\d*\", \"X\", rationale)\n    masked = re.sub(r\"[+\\-*/=]\", \"#\", masked)\n    return masked\n\n\ndef logp_text(\n    model: AutoModelForSeq2SeqLM,\n    tokenizer: AutoTokenizer,\n    encoder_text: str,\n    decoder_text: str,\n    device: str,\n    max_length: int = 512,\n) -> float:\n    if tokenizer.pad_token_id is None:\n        raise ValueError(\"Tokenizer pad_token_id is required for logp computation.\")\n    enc = tokenizer(encoder_text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(device)\n    dec = tokenizer(decoder_text, return_tensors=\"pt\", truncation=True, max_length=max_length)\n    labels = dec.input_ids.to(device)\n    labels_mask = labels != tokenizer.pad_token_id\n    labels = labels.masked_fill(~labels_mask, -100)\n    if labels_mask.sum().item() == 0:\n        return 0.0\n    with torch.no_grad():\n        outputs = model(**enc, labels=labels)\n        n_tokens = labels_mask.sum().item()\n        logp = -outputs.loss.detach().float() * max(n_tokens, 1)\n    return float(logp.item())\n\n\ndef minimal_sufficient_prefix_len(\n    model: AutoModelForSeq2SeqLM,\n    tokenizer: AutoTokenizer,\n    prefix: str,\n    rationale: str,\n    ans_text: str,\n    Lr_full: float,\n    eps: float,\n    max_chunks: int,\n    device: str,\n    max_length: int,\n) -> int:\n    chunks = chunk_rationale(rationale, max_chunks=max_chunks)\n    if not chunks:\n        return 0\n    for k in range(1, len(chunks) + 1):\n        r_pref = \"\\n\".join(chunks[:k])\n        Lk = logp_text(\n            model,\n            tokenizer,\n            encoder_text=prefix + \"\\n\" + r_pref,\n            decoder_text=ans_text,\n            device=device,\n            max_length=max_length,\n        )\n        if Lk >= (Lr_full - eps):\n            return len(tokenizer(r_pref, add_special_tokens=False).input_ids)\n    return len(tokenizer(rationale, add_special_tokens=False).input_ids)\n\n\ndef create_model_and_tokenizer(model_cfg, device: str):\n    precision = getattr(model_cfg, \"precision\", \"fp32\")\n    if device == \"cuda\" and precision in {\"fp16\", \"float16\"}:\n        dtype = torch.float16\n    elif device == \"cuda\" and precision in {\"bf16\", \"bfloat16\"}:\n        dtype = torch.bfloat16\n    else:\n        dtype = torch.float32\n    tokenizer = AutoTokenizer.from_pretrained(model_cfg.name, cache_dir=\".cache/\")\n    if tokenizer.pad_token_id is None:\n        if tokenizer.eos_token is not None:\n            tokenizer.pad_token = tokenizer.eos_token\n        elif tokenizer.unk_token is not None:\n            tokenizer.pad_token = tokenizer.unk_token\n    if tokenizer.pad_token_id is None:\n        raise ValueError(\"Tokenizer pad_token_id is missing after initialization.\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        model_cfg.name, cache_dir=\".cache/\", torch_dtype=dtype\n    )\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model.to(device)\n    model.eval()\n    return model, tokenizer\n\n\nclass BaseAggregator:\n    def __init__(\n        self,\n        model: AutoModelForSeq2SeqLM,\n        tokenizer: AutoTokenizer,\n        device: str,\n        model_cfg,\n        params: Dict[str, float],\n        max_length: int,\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.temperature = float(getattr(model_cfg, \"temperature\", 0.7))\n        self.top_p = float(getattr(model_cfg, \"top_p\", 0.95))\n        self.max_new_tokens_stage1 = int(getattr(model_cfg, \"max_new_tokens_stage1\", 64))\n        self.max_new_tokens_stage2 = int(getattr(model_cfg, \"max_new_tokens_stage2\", 256))\n        self.K_short = int(params.get(\"K_short\", 4))\n        self.K_long = int(params.get(\"K_long\", 4))\n        self.p_stop = float(params.get(\"p_stop\", 0.85))\n        self.m_stop = float(params.get(\"m_stop\", 0.35))\n        self.tau = float(params.get(\"tau\", 1.0))\n        self.lambda_len = float(params.get(\"lambda_len\", 0.0))\n        self.max_length = max_length\n        self.max_chunks = int(params.get(\"max_chunks\", 6))\n\n    def build_prompt(self, question: str, verify: bool = False) -> str:\n        prompt = f\"Q: {question}\\nA: Let’s think step by step.\".replace(\"\\u2019\", \"'\")\n        if verify:\n            prompt += \" Check for mistakes and contradictions.\"\n        prompt += \" End with 'The answer is <number>'.\"\n        return prompt\n\n    def _sample_traces(self, question: str, n: int, max_new_tokens: int, verify: bool) -> Tuple[str, List[str]]:\n        prompt = self.build_prompt(question, verify=verify)\n        enc = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=self.max_length,\n        ).to(self.device)\n        temperature = max(self.temperature, 1e-5)\n        with torch.no_grad():\n            gen = self.model.generate(\n                **enc,\n                do_sample=True,\n                temperature=temperature,\n                top_p=self.top_p,\n                max_new_tokens=max_new_tokens,\n                num_return_sequences=n,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        decoded = self.tokenizer.batch_decode(gen, skip_special_tokens=True)\n        return prompt, decoded\n\n    def posterior(self, weights: Dict[str, float]) -> Tuple[Optional[str], Optional[float], Optional[float]]:\n        if not weights:\n            return None, None, None\n        Z = sum(weights.values())\n        items = sorted(((a, w / Z) for a, w in weights.items()), key=lambda x: x[1], reverse=True)\n        top1a, top1p = items[0]\n        top2p = items[1][1] if len(items) > 1 else 0.0\n        return top1a, top1p, top1p - top2p\n\n    def score_samples(self, prefix: str, samples: List[str]) -> Tuple[Dict[str, float], int, List[float]]:\n        weights: Dict[str, float] = defaultdict(float)\n        token_count = 0\n        cc_values: List[float] = []\n        base_cache: Dict[str, float] = {}\n        for text in samples:\n            ans = extract_answer(text)\n            if ans is None:\n                continue\n            rationale, _ = split_rationale_answer(text)\n            token_count += len(self.tokenizer(text, add_special_tokens=False).input_ids)\n            ans_text = f\"The answer is {ans}.\"\n            if ans not in base_cache:\n                base_cache[ans] = logp_text(\n                    self.model,\n                    self.tokenizer,\n                    encoder_text=prefix,\n                    decoder_text=ans_text,\n                    device=self.device,\n                    max_length=self.max_length,\n                )\n            L0 = base_cache[ans]\n            Lr = logp_text(\n                self.model,\n                self.tokenizer,\n                encoder_text=prefix + \"\\n\" + rationale,\n                decoder_text=ans_text,\n                device=self.device,\n                max_length=self.max_length,\n            )\n            EG = Lr - L0\n            rationale_len = len(self.tokenizer(rationale, add_special_tokens=False).input_ids)\n            score = EG - self.lambda_len * rationale_len\n            weights[ans] += math.exp(score / max(self.tau, 1e-6))\n            Lcf = logp_text(\n                self.model,\n                self.tokenizer,\n                encoder_text=prefix + \"\\n\" + mask_numbers(rationale),\n                decoder_text=ans_text,\n                device=self.device,\n                max_length=self.max_length,\n            )\n            cc_values.append(Lr - Lcf)\n        return weights, token_count, cc_values\n\n    def predict(self, question: str) -> Dict[str, float]:\n        prefix1, samples1 = self._sample_traces(\n            question, n=self.K_short, max_new_tokens=self.max_new_tokens_stage1, verify=False\n        )\n        weights1, tok1, cc1 = self.score_samples(prefix1, samples1)\n        a1, p1, m1 = self.posterior(weights1)\n        if a1 is None:\n            return {\n                \"pred_answer\": None,\n                \"used_tokens\": tok1,\n                \"stage2_used\": 0,\n                \"cc_values\": cc1,\n                \"stage1_top1_prob\": 0.0,\n                \"stage1_margin\": 0.0,\n                \"final_top1_prob\": 0.0,\n            }\n        if (p1 >= self.p_stop) or (m1 >= self.m_stop):\n            final_answer = a1\n            final_prob = p1\n            total_tokens = tok1\n            cc_vals = cc1\n            stage2_used = 0\n        else:\n            prefix2, samples2 = self._sample_traces(\n                question, n=self.K_long, max_new_tokens=self.max_new_tokens_stage2, verify=True\n            )\n            weights2, tok2, cc2 = self.score_samples(prefix2, samples2)\n            for ans, w in weights2.items():\n                weights1[ans] = weights1.get(ans, 0.0) + w\n            final_answer, final_prob, _ = self.posterior(weights1)\n            total_tokens = tok1 + tok2\n            cc_vals = cc1 + cc2\n            stage2_used = 1\n        return {\n            \"pred_answer\": final_answer,\n            \"used_tokens\": total_tokens,\n            \"stage2_used\": stage2_used,\n            \"cc_values\": cc_vals,\n            \"stage1_top1_prob\": float(p1) if p1 is not None else 0.0,\n            \"stage1_margin\": float(m1) if m1 is not None else 0.0,\n            \"final_top1_prob\": float(final_prob) if final_prob is not None else 0.0,\n        }\n\n\nclass DirectAnswer:\n    def __init__(\n        self,\n        model: AutoModelForSeq2SeqLM,\n        tokenizer: AutoTokenizer,\n        device: str,\n        model_cfg,\n        params: Dict[str, float],\n        max_length: int,\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.max_new_tokens = int(getattr(model_cfg, \"max_new_tokens_stage1\", 64))\n        self.max_length = max_length\n\n    def build_prompt(self, question: str, verify: bool = False) -> str:\n        return f\"Q: {question}\\nA: The answer is\"\n\n    def predict(self, question: str) -> Dict[str, float]:\n        prompt = self.build_prompt(question)\n        enc = self.tokenizer(\n            prompt,\n            return_tensors=\"pt\",\n            truncation=True,\n            max_length=self.max_length,\n        ).to(self.device)\n        with torch.no_grad():\n            gen = self.model.generate(\n                **enc,\n                do_sample=False,\n                max_new_tokens=self.max_new_tokens,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        decoded = self.tokenizer.batch_decode(gen, skip_special_tokens=True)\n        output = decoded[0] if decoded else \"\"\n        pred = extract_answer(output)\n        used_tokens = len(self.tokenizer(output, add_special_tokens=False).input_ids)\n        return {\n            \"pred_answer\": pred,\n            \"used_tokens\": used_tokens,\n            \"stage2_used\": 0,\n            \"cc_values\": [],\n            \"stage1_top1_prob\": 1.0,\n            \"stage1_margin\": 1.0,\n            \"final_top1_prob\": 1.0,\n        }\n\n\nclass SCMajority:\n    def __init__(\n        self,\n        model: AutoModelForSeq2SeqLM,\n        tokenizer: AutoTokenizer,\n        device: str,\n        model_cfg,\n        params: Dict[str, float],\n        max_length: int,\n        compute_cc: bool = True,\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.temperature = float(getattr(model_cfg, \"temperature\", 0.7))\n        self.top_p = float(getattr(model_cfg, \"top_p\", 0.95))\n        self.max_new_tokens = int(getattr(model_cfg, \"max_new_tokens_stage2\", 256))\n        self.K = int(params.get(\"K_long\", 10))\n        self.max_length = max_length\n        self.compute_cc = compute_cc\n\n    def build_prompt(self, question: str, verify: bool = False) -> str:\n        prompt = f\"Q: {question}\\nA: Let’s think step by step.\".replace(\"\\u2019\", \"'\")\n        prompt += \" End with 'The answer is <number>'.\"\n        return prompt\n\n    def predict(self, question: str) -> Dict[str, float]:\n        prompt = self.build_prompt(question)\n        enc = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_length).to(\n            self.device\n        )\n        with torch.no_grad():\n            gen = self.model.generate(\n                **enc,\n                do_sample=True,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                max_new_tokens=self.max_new_tokens,\n                num_return_sequences=self.K,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        decoded = self.tokenizer.batch_decode(gen, skip_special_tokens=True)\n        counts: Dict[str, int] = defaultdict(int)\n        total_tokens = 0\n        cc_values: List[float] = []\n        for text in decoded:\n            total_tokens += len(self.tokenizer(text, add_special_tokens=False).input_ids)\n            ans = extract_answer(text)\n            if ans is None:\n                continue\n            counts[ans] += 1\n            if self.compute_cc:\n                rationale, _ = split_rationale_answer(text)\n                ans_text = f\"The answer is {ans}.\"\n                Lr = logp_text(\n                    self.model,\n                    self.tokenizer,\n                    encoder_text=prompt + \"\\n\" + rationale,\n                    decoder_text=ans_text,\n                    device=self.device,\n                    max_length=self.max_length,\n                )\n                Lcf = logp_text(\n                    self.model,\n                    self.tokenizer,\n                    encoder_text=prompt + \"\\n\" + mask_numbers(rationale),\n                    decoder_text=ans_text,\n                    device=self.device,\n                    max_length=self.max_length,\n                )\n                cc_values.append(Lr - Lcf)\n        if not counts:\n            return {\n                \"pred_answer\": None,\n                \"used_tokens\": total_tokens,\n                \"stage2_used\": 0,\n                \"cc_values\": cc_values,\n                \"stage1_top1_prob\": 0.0,\n                \"stage1_margin\": 0.0,\n                \"final_top1_prob\": 0.0,\n            }\n        total = sum(counts.values())\n        sorted_counts = sorted(counts.items(), key=lambda x: x[1], reverse=True)\n        top1a, top1c = sorted_counts[0]\n        top2c = sorted_counts[1][1] if len(sorted_counts) > 1 else 0\n        top1p = top1c / max(total, 1)\n        margin = (top1c - top2c) / max(total, 1)\n        return {\n            \"pred_answer\": top1a,\n            \"used_tokens\": total_tokens,\n            \"stage2_used\": 0,\n            \"cc_values\": cc_values,\n            \"stage1_top1_prob\": float(top1p),\n            \"stage1_margin\": float(margin),\n            \"final_top1_prob\": float(top1p),\n        }\n\n\nclass BoNAnsLogP:\n    def __init__(\n        self,\n        model: AutoModelForSeq2SeqLM,\n        tokenizer: AutoTokenizer,\n        device: str,\n        model_cfg,\n        params: Dict[str, float],\n        max_length: int,\n        compute_cc: bool = True,\n    ):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.temperature = float(getattr(model_cfg, \"temperature\", 0.7))\n        self.top_p = float(getattr(model_cfg, \"top_p\", 0.95))\n        self.max_new_tokens = int(getattr(model_cfg, \"max_new_tokens_stage2\", 256))\n        self.K = int(params.get(\"K_long\", 10))\n        self.tau = float(params.get(\"tau\", 1.0))\n        self.max_length = max_length\n        self.compute_cc = compute_cc\n\n    def build_prompt(self, question: str, verify: bool = False) -> str:\n        prompt = f\"Q: {question}\\nA: Let’s think step by step.\".replace(\"\\u2019\", \"'\")\n        prompt += \" End with 'The answer is <number>'.\"\n        return prompt\n\n    def predict(self, question: str) -> Dict[str, float]:\n        prompt = self.build_prompt(question)\n        enc = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.max_length).to(\n            self.device\n        )\n        with torch.no_grad():\n            gen = self.model.generate(\n                **enc,\n                do_sample=True,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                max_new_tokens=self.max_new_tokens,\n                num_return_sequences=self.K,\n                pad_token_id=self.tokenizer.pad_token_id,\n            )\n        decoded = self.tokenizer.batch_decode(gen, skip_special_tokens=True)\n        total_tokens = 0\n        best_score = -float(\"inf\")\n        best_answer = None\n        weights: Dict[str, float] = defaultdict(float)\n        cc_values: List[float] = []\n        for text in decoded:\n            total_tokens += len(self.tokenizer(text, add_special_tokens=False).input_ids)\n            ans = extract_answer(text)\n            if ans is None:\n                continue\n            rationale, _ = split_rationale_answer(text)\n            ans_text = f\"The answer is {ans}.\"\n            Lr = logp_text(\n                self.model,\n                self.tokenizer,\n                encoder_text=prompt + \"\\n\" + rationale,\n                decoder_text=ans_text,\n                device=self.device,\n                max_length=self.max_length,\n            )\n            if Lr > best_score:\n                best_score = Lr\n                best_answer = ans\n            weights[ans] += math.exp(Lr / max(self.tau, 1e-6))\n            if self.compute_cc:\n                Lcf = logp_text(\n                    self.model,\n                    self.tokenizer,\n                    encoder_text=prompt + \"\\n\" + mask_numbers(rationale),\n                    decoder_text=ans_text,\n                    device=self.device,\n                    max_length=self.max_length,\n                )\n                cc_values.append(Lr - Lcf)\n        if not weights:\n            return {\n                \"pred_answer\": None,\n                \"used_tokens\": total_tokens,\n                \"stage2_used\": 0,\n                \"cc_values\": cc_values,\n                \"stage1_top1_prob\": 0.0,\n                \"stage1_margin\": 0.0,\n                \"final_top1_prob\": 0.0,\n            }\n        Z = sum(weights.values())\n        top1p = weights.get(best_answer, 0.0) / max(Z, 1e-6)\n        return {\n            \"pred_answer\": best_answer,\n            \"used_tokens\": total_tokens,\n            \"stage2_used\": 0,\n            \"cc_values\": cc_values,\n            \"stage1_top1_prob\": float(top1p),\n            \"stage1_margin\": float(top1p),\n            \"final_top1_prob\": float(top1p),\n        }\n\n\nclass MetaPMIADSC(BaseAggregator):\n    def __init__(\n        self,\n        model: AutoModelForSeq2SeqLM,\n        tokenizer: AutoTokenizer,\n        device: str,\n        model_cfg,\n        params: Dict[str, float],\n        max_length: int,\n        compute_cc: bool = True,\n    ):\n        super().__init__(model, tokenizer, device, model_cfg, params, max_length)\n        self.lambda_len = float(params.get(\"lambda_len\", 0.002))\n        self.compute_cc = compute_cc\n\n    def score_samples(self, prefix: str, samples: List[str]) -> Tuple[Dict[str, float], int, List[float]]:\n        weights: Dict[str, float] = defaultdict(float)\n        token_count = 0\n        cc_values: List[float] = []\n        base_cache: Dict[str, float] = {}\n        for text in samples:\n            ans = extract_answer(text)\n            if ans is None:\n                continue\n            rationale, _ = split_rationale_answer(text)\n            token_count += len(self.tokenizer(text, add_special_tokens=False).input_ids)\n            ans_text = f\"The answer is {ans}.\"\n            if ans not in base_cache:\n                base_cache[ans] = logp_text(\n                    self.model,\n                    self.tokenizer,\n                    encoder_text=prefix,\n                    decoder_text=ans_text,\n                    device=self.device,\n                    max_length=self.max_length,\n                )\n            L0 = base_cache[ans]\n            Lr = logp_text(\n                self.model,\n                self.tokenizer,\n                encoder_text=prefix + \"\\n\" + rationale,\n                decoder_text=ans_text,\n                device=self.device,\n                max_length=self.max_length,\n            )\n            EG = Lr - L0\n            rationale_len = len(self.tokenizer(rationale, add_special_tokens=False).input_ids)\n            score = EG - self.lambda_len * rationale_len\n            weights[ans] += math.exp(score / max(self.tau, 1e-6))\n            if self.compute_cc:\n                Lcf = logp_text(\n                    self.model,\n                    self.tokenizer,\n                    encoder_text=prefix + \"\\n\" + mask_numbers(rationale),\n                    decoder_text=ans_text,\n                    device=self.device,\n                    max_length=self.max_length,\n                )\n                cc_values.append(Lr - Lcf)\n        return weights, token_count, cc_values\n\n\nclass CESADSC(BaseAggregator):\n    def __init__(\n        self,\n        model: AutoModelForSeq2SeqLM,\n        tokenizer: AutoTokenizer,\n        device: str,\n        model_cfg,\n        params: Dict[str, float],\n        max_length: int,\n    ):\n        super().__init__(model, tokenizer, device, model_cfg, params, max_length)\n        self.alpha = float(params.get(\"alpha\", 0.5))\n        self.lambda_len = float(params.get(\"lambda_len\", 0.002))\n        self.epsilon = float(params.get(\"epsilon\", 0.5))\n\n    def score_samples(self, prefix: str, samples: List[str]) -> Tuple[Dict[str, float], int, List[float]]:\n        weights: Dict[str, float] = defaultdict(float)\n        token_count = 0\n        cc_values: List[float] = []\n        base_cache: Dict[str, float] = {}\n        for text in samples:\n            ans = extract_answer(text)\n            if ans is None:\n                continue\n            rationale, _ = split_rationale_answer(text)\n            token_count += len(self.tokenizer(text, add_special_tokens=False).input_ids)\n            ans_text = f\"The answer is {ans}.\"\n            if ans not in base_cache:\n                base_cache[ans] = logp_text(\n                    self.model,\n                    self.tokenizer,\n                    encoder_text=prefix,\n                    decoder_text=ans_text,\n                    device=self.device,\n                    max_length=self.max_length,\n                )\n            L0 = base_cache[ans]\n            Lr = logp_text(\n                self.model,\n                self.tokenizer,\n                encoder_text=prefix + \"\\n\" + rationale,\n                decoder_text=ans_text,\n                device=self.device,\n                max_length=self.max_length,\n            )\n            Lcf = logp_text(\n                self.model,\n                self.tokenizer,\n                encoder_text=prefix + \"\\n\" + mask_numbers(rationale),\n                decoder_text=ans_text,\n                device=self.device,\n                max_length=self.max_length,\n            )\n            EG = Lr - L0\n            CC = Lr - Lcf\n            l_star = minimal_sufficient_prefix_len(\n                self.model,\n                self.tokenizer,\n                prefix=prefix,\n                rationale=rationale,\n                ans_text=ans_text,\n                Lr_full=Lr,\n                eps=self.epsilon,\n                max_chunks=self.max_chunks,\n                device=self.device,\n                max_length=self.max_length,\n            )\n            score = EG + self.alpha * CC - self.lambda_len * l_star\n            weights[ans] += math.exp(score / max(self.tau, 1e-6))\n            cc_values.append(CC)\n        return weights, token_count, cc_values\n\n\ndef build_method(method_name: str, model, tokenizer, run_cfg, params: Dict[str, float], device: str):\n    name = method_name.lower()\n    max_length = int(run_cfg.dataset.preprocessing.max_length)\n    if \"ces-adsc\" in name:\n        return CESADSC(model, tokenizer, device, run_cfg.model, params, max_length=max_length)\n    if \"meta-pmi-adsc\" in name or \"meta\" in name:\n        return MetaPMIADSC(model, tokenizer, device, run_cfg.model, params, max_length=max_length)\n    if \"bon\" in name or \"best-of-n\" in name or \"anslogp\" in name:\n        return BoNAnsLogP(model, tokenizer, device, run_cfg.model, params, max_length=max_length)\n    if \"sc-majority\" in name or \"self-consistency\" in name or (\"sc\" in name and \"majority\" in name):\n        return SCMajority(model, tokenizer, device, run_cfg.model, params, max_length=max_length)\n    if \"direct\" in name:\n        return DirectAnswer(model, tokenizer, device, run_cfg.model, params, max_length=max_length)\n    raise ValueError(f\"Unsupported method name: {method_name}\")\n",
    "main_py": "import os\nimport subprocess\nimport sys\nfrom pathlib import Path\n\nimport hydra\nfrom hydra.utils import get_original_cwd\nfrom omegaconf import DictConfig, OmegaConf\n\n\ndef _repo_root() -> Path:\n    try:\n        return Path(get_original_cwd())\n    except Exception:\n        return Path(__file__).resolve().parents[1]\n\n\ndef _apply_mode_overrides(cfg: DictConfig) -> DictConfig:\n    if cfg.mode == \"trial\":\n        cfg.wandb.mode = \"disabled\"\n        if hasattr(cfg, \"run\") and hasattr(cfg.run, \"optuna\"):\n            cfg.run.optuna.n_trials = 0\n    elif cfg.mode == \"full\":\n        cfg.wandb.mode = \"online\"\n    else:\n        raise ValueError(f\"Unsupported mode: {cfg.mode}\")\n    return cfg\n\n\n@hydra.main(config_path=\"../config\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -> None:\n    OmegaConf.set_struct(cfg, False)\n    cfg = _apply_mode_overrides(cfg)\n    run_id = str(getattr(cfg.run, \"run_id\", \"\"))\n    if not run_id:\n        raise ValueError(\"run_id missing in cfg.run. Ensure run configuration is loaded.\")\n\n    repo_root = _repo_root()\n\n    overrides = [\n        f\"run={run_id}\",\n        f\"results_dir={cfg.results_dir}\",\n        f\"mode={cfg.mode}\",\n        f\"wandb.mode={cfg.wandb.mode}\",\n        f\"seed={cfg.seed}\",\n    ]\n    if cfg.mode == \"trial\":\n        overrides.append(\"run.optuna.n_trials=0\")\n\n    cmd = [sys.executable, \"-m\", \"src.train\"] + overrides\n    env = os.environ.copy()\n    env[\"HYDRA_FULL_ERROR\"] = \"1\"\n    env[\"PYTHONPATH\"] = f\"{repo_root}{os.pathsep}{env.get('PYTHONPATH', '')}\"\n    subprocess.run(cmd, check=True, env=env, cwd=str(repo_root))\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "pyproject_toml": "[project]\nname = \"ces-adsc-experiments\"\nversion = \"0.1.0\"\ndescription = \"CES-ADSC inference experiments with Hydra\"\nrequires-python = \">=3.10\"\ndependencies = [\n  \"torch>=2.1.0\",\n  \"transformers>=4.38.0\",\n  \"datasets>=2.17.0\",\n  \"hydra-core>=1.3.2\",\n  \"wandb>=0.16.0\",\n  \"optuna>=3.5.0\",\n  \"numpy>=1.24.0\",\n  \"pandas>=2.0.0\",\n  \"scipy>=1.10.0\",\n  \"matplotlib>=3.7.0\",\n  \"seaborn>=0.13.0\",\n  \"tqdm>=4.66.0\",\n  \"sentencepiece>=0.1.99\"\n]\n\n[build-system]\nrequires = [\"setuptools>=65.0\"]\nbuild-backend = \"setuptools.build_meta\"\n",
    "config_yaml": "defaults:\n  - run: proposed-flan-t5-base-gsm8k\n  - _self_\n\nmode: full\nresults_dir: ./results\nseed: 42\n\nwandb:\n  entity: airas\n  project: 2026-02-06\n  mode: online\n\nhydra:\n  run:\n    dir: ${results_dir}/${run.run_id}\n  sweep:\n    dir: ${results_dir}/multirun\n  job:\n    chdir: false\n  output_subdir: null\n",
    "run_configs": {
      "proposed-flan-t5-base-gsm8k": "run_id: proposed-flan-t5-base-gsm8k\nmethod: CES-ADSC (Counterfactual Evidence & Sufficiency–Weighted Self-Consistency with Adaptive Deliberation)\nmodel:\n  name: google/flan-t5-base\n  precision: fp16\n  max_new_tokens_stage1: 64\n  max_new_tokens_stage2: 256\n  temperature: 0.7\n  top_p: 0.95\ndataset:\n  name: openai/gsm8k\n  subset: main\n  split: test\n  subset_size: 200\n  shuffle: true\n  shuffle_seed: 0\n  preprocessing:\n    max_length: 512\ntraining:\n  enabled: false\n  learning_rate: 0.0\n  batch_size: 1\n  epochs: 0\n  optimizer: none\n  warmup_steps: 0\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: alpha\n      distribution_type: categorical\n      choices: [0.0, 0.5, 1.0]\n    - param_name: lambda_len\n      distribution_type: categorical\n      choices: [0.0, 0.001, 0.002, 0.005]\n    - param_name: epsilon\n      distribution_type: categorical\n      choices: [0.2, 0.5]\n    - param_name: p_stop\n      distribution_type: categorical\n      choices: [0.75, 0.85]\n    - param_name: m_stop\n      distribution_type: categorical\n      choices: [0.25, 0.35]\n    - param_name: tau\n      distribution_type: categorical\n      choices: [0.7, 1.0, 1.3]\n    - param_name: K_short\n      distribution_type: categorical\n      choices: [3, 4, 5]\n    - param_name: K_long\n      distribution_type: categorical\n      choices: [3, 4, 6]\n",
      "comparative-1-flan-t5-base-gsm8k": "run_id: comparative-1-flan-t5-base-gsm8k\nmethod: Meta-PMI-ADSC (Evidence-Gain Self-Consistency with Adaptive Deliberation)\nmodel:\n  name: google/flan-t5-base\n  precision: fp16\n  max_new_tokens_stage1: 64\n  max_new_tokens_stage2: 256\n  temperature: 0.7\n  top_p: 0.95\ndataset:\n  name: openai/gsm8k\n  subset: main\n  split: test\n  subset_size: 200\n  shuffle: true\n  shuffle_seed: 0\n  preprocessing:\n    max_length: 512\ntraining:\n  enabled: false\n  learning_rate: 0.0\n  batch_size: 1\n  epochs: 0\n  optimizer: none\n  warmup_steps: 0\noptuna:\n  n_trials: 20\n  search_spaces:\n    - param_name: lambda_len\n      distribution_type: categorical\n      choices: [0.0, 0.001, 0.002, 0.005]\n    - param_name: p_stop\n      distribution_type: categorical\n      choices: [0.75, 0.85]\n    - param_name: m_stop\n      distribution_type: categorical\n      choices: [0.25, 0.35]\n    - param_name: tau\n      distribution_type: categorical\n      choices: [0.7, 1.0, 1.3]\n    - param_name: K_short\n      distribution_type: categorical\n      choices: [3, 4, 5]\n    - param_name: K_long\n      distribution_type: categorical\n      choices: [3, 4, 6]\n"
    }
  }
}